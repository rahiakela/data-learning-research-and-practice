{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-neural-network-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQ1JpTtqHix00HkGW2zl6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/2_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPJWYkEzn0-x"
      },
      "source": [
        "##Neural Network from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jge4t8T3oHfA"
      },
      "source": [
        "You should now have a general understanding\n",
        "of what’s going on behind the scenes in a neural network. What was a magical\n",
        "black box at the start, has turned into a clearer picture.\n",
        "\n",
        "<img src='https://github.com/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "- the model, composed of layers that are chained together, maps the input data to predictions. \n",
        "- The loss function then compares these predictions\n",
        "to the targets, producing a loss value: a measure of how well the model’s predictions match what was expected. \n",
        "- The optimizer uses this loss value to update the model’s weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlW4h5vdpa3N"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0IXiaYkrCla"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAud-_puuj1p"
      },
      "source": [
        "##NN using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSL_MlYwu3VQ"
      },
      "source": [
        "Now you understand that the input images are stored in NumPy tensors, which are\n",
        "here formatted as float32 tensors of shape (60000, 784) (training data) and (10000,\n",
        "784) (test data) respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZL4iPADuq_W",
        "outputId": "48e65cb3-c3f8-4163-9432-41036481c56f"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg5JqcZsu4R2"
      },
      "source": [
        "Now you understand that this model consists of a chain of two Dense layers, that each\n",
        "layer applies a few simple tensor operations to the input data, and that these operations\n",
        "involve weight tensors. Weight tensors, which are attributes of the layers, are\n",
        "where the knowledge of the model persists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rlG8gpgu71Q"
      },
      "source": [
        "model = keras.Sequential([\n",
        "   layers.Dense(512, activation=\"relu\"),\n",
        "   layers.Dense(10, activation=\"softmax\")                      \n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdutgLbxvUgH"
      },
      "source": [
        "Now you understand that sparse_categorical_crossentropy is the loss function\n",
        "that’s used as a feedback signal for learning the weight tensors, and which the training\n",
        "phase will attempt to minimize. \n",
        "\n",
        "You also know that this reduction of the loss\n",
        "happens via mini-batch stochastic gradient descent. The exact rules governing a specific\n",
        "use of gradient descent are defined by the rmsprop optimizer passed as the first\n",
        "argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZctRRGBvc8q"
      },
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weUDRAeMvtn7"
      },
      "source": [
        "Now you understand what happens when you call fit: the model will start to iterate\n",
        "on the training data in mini-batches of 128 samples, 5 times over (each iteration over\n",
        "all the training data is called an epoch).\n",
        "\n",
        "For each batch, the model will compute the\n",
        "gradient of the loss with regard to the weights (using the Backpropagation algorithm,\n",
        "which derives from the chain rule in calculus) and move the weights in the direction\n",
        "that will reduce the value of the loss for this batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-tSBvyDvoLA",
        "outputId": "bb1804fb-4438-4a7b-c8b9-1b8eaff8ea0d"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.2560 - accuracy: 0.9260\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1036 - accuracy: 0.9694\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0680 - accuracy: 0.9797\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0499 - accuracy: 0.9852\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0383 - accuracy: 0.9886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6f7144490>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTntBbZwv8_Z"
      },
      "source": [
        "After these 5 epochs, the model will have performed 2,345 gradient updates (469\n",
        "per epoch), and the loss of the model will be sufficiently low that the model will be\n",
        "capable of classifying handwritten digits with high accuracy.\n",
        "\n",
        "At this point, you already know most of what there is to know about neural networks.\n",
        "Let’s prove it by reimplementing a simplified version of that first example\n",
        "“from scratch” in TensorFlow, step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBUzy-DIwEJX"
      },
      "source": [
        "##NN from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEdUocYXwGYh"
      },
      "source": [
        "What better demonstrates full, unambiguous understanding than implementing everything\n",
        "from scratch? Of course, what “from scratch” means here is relative: we won’t\n",
        "reimplement basic tensor operations, and we won’t implement backpropagation. But\n",
        "we’ll go to such a low level that we will barely use any Keras functionality at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIcxzi5vLURe"
      },
      "source": [
        "###A simple Dense class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugDuoqsxLVKt"
      },
      "source": [
        "You’ve learned earlier that the Dense layer implements the following input transformation,\n",
        "where W and b are model parameters, and activation is an element-wise\n",
        "function (usually relu, but it would be softmax for the last layer):\n",
        "\n",
        "```python\n",
        "output = activation(dot(W, input) + b)\n",
        "```\n",
        "\n",
        "Let’s implement a simple Python class, NaiveDense, that creates two TensorFlow\n",
        "variables, W and b, and exposes a `__call__()` method that applies the preceding\n",
        "transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWacHeTsLe1h"
      },
      "source": [
        "class NaiveDense:\n",
        "\n",
        "  def __init__(self, input_size, output_size, activation):\n",
        "    self.activation = activation\n",
        "\n",
        "    # Create a matrix, W, of shape (input_size, output_size), initialized with random values\n",
        "    w_shape = (input_size, output_size)\n",
        "    w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "    self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "    # Create a vector, b, of shape (output_size,), initialized with zeros\n",
        "    b_shape = (output_size, )\n",
        "    b_initial_value = tf.zeros(b_shape)\n",
        "    self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "  # Convenience method for retrieving the layer’s weights\n",
        "  @property\n",
        "  def weights(self):\n",
        "    return [self.W, self.b]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJQkPSMXPTQe"
      },
      "source": [
        "###A simple Sequential class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKrQl30cPUKk"
      },
      "source": [
        "Now, let’s create a NaiveSequential class to chain these layers. It wraps a list of layers\n",
        "and exposes a `__call__()` method that simply calls the underlying layers on the\n",
        "inputs, in order. \n",
        "\n",
        "It also features a weights property to easily keep track of the layers’\n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGMMumwfPctB"
      },
      "source": [
        "class NaiveSequential:\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    weights = []\n",
        "    for layer in self.layers:\n",
        "      weights += layer.weights\n",
        "    return weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5AY46DYQbfZ"
      },
      "source": [
        "Using this NaiveDense class and this NaiveSequential class, we can create a mock\n",
        "Keras model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hre_5n22QcP6"
      },
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)                     \n",
        "])\n",
        "\n",
        "assert len(model.weights) == 4"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aS4RJ9WRKOh"
      },
      "source": [
        "###A batch generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCSXfHcdRLUv"
      },
      "source": [
        "Next, we need a way to iterate over the MNIST data in mini-batches. This is easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ4JD7kwROWm"
      },
      "source": [
        "class BatchGenerator:\n",
        "  def __init__(self, images, labels, batch_size=128):\n",
        "    assert len(images) == len(labels)\n",
        "\n",
        "    self.index = 0\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "  def next(self):\n",
        "    images = self.images[self.index: self.index + self.batch_size]\n",
        "    labels = self.labels[self.index: self.index + self.batch_size]\n",
        "    self.index += self.batch_size\n",
        "    return images, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upyji865SWWY"
      },
      "source": [
        "##Running one training step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuEullaSSYbk"
      },
      "source": [
        ""
      ]
    }
  ]
}