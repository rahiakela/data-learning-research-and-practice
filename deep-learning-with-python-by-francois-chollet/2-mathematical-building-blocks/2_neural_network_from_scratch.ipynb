{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-neural-network-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMFZ/rEBJtALAyRXN9rWCRi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/2_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPJWYkEzn0-x"
      },
      "source": [
        "##Neural Network from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jge4t8T3oHfA"
      },
      "source": [
        "You should now have a general understanding\n",
        "of what’s going on behind the scenes in a neural network. What was a magical\n",
        "black box at the start, has turned into a clearer picture.\n",
        "\n",
        "<img src='https://github.com/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "- the model, composed of layers that are chained together, maps the input data to predictions. \n",
        "- The loss function then compares these predictions\n",
        "to the targets, producing a loss value: a measure of how well the model’s predictions match what was expected. \n",
        "- The optimizer uses this loss value to update the model’s weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlW4h5vdpa3N"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0IXiaYkrCla"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAud-_puuj1p"
      },
      "source": [
        "##NN using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSL_MlYwu3VQ"
      },
      "source": [
        "Now you understand that the input images are stored in NumPy tensors, which are\n",
        "here formatted as float32 tensors of shape (60000, 784) (training data) and (10000,\n",
        "784) (test data) respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZL4iPADuq_W"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg5JqcZsu4R2"
      },
      "source": [
        "Now you understand that this model consists of a chain of two Dense layers, that each\n",
        "layer applies a few simple tensor operations to the input data, and that these operations\n",
        "involve weight tensors. Weight tensors, which are attributes of the layers, are\n",
        "where the knowledge of the model persists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rlG8gpgu71Q"
      },
      "source": [
        "model = keras.Sequential([\n",
        "   layers.Dense(512, activation=\"relu\"),\n",
        "   layers.Dense(10, activation=\"softmax\")                      \n",
        "])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdutgLbxvUgH"
      },
      "source": [
        "Now you understand that sparse_categorical_crossentropy is the loss function\n",
        "that’s used as a feedback signal for learning the weight tensors, and which the training\n",
        "phase will attempt to minimize. \n",
        "\n",
        "You also know that this reduction of the loss\n",
        "happens via mini-batch stochastic gradient descent. The exact rules governing a specific\n",
        "use of gradient descent are defined by the rmsprop optimizer passed as the first\n",
        "argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZctRRGBvc8q"
      },
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weUDRAeMvtn7"
      },
      "source": [
        "Now you understand what happens when you call fit: the model will start to iterate\n",
        "on the training data in mini-batches of 128 samples, 5 times over (each iteration over\n",
        "all the training data is called an epoch).\n",
        "\n",
        "For each batch, the model will compute the\n",
        "gradient of the loss with regard to the weights (using the Backpropagation algorithm,\n",
        "which derives from the chain rule in calculus) and move the weights in the direction\n",
        "that will reduce the value of the loss for this batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-tSBvyDvoLA",
        "outputId": "9af7d2c2-1cd2-4876-f83f-497426e21455"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2553 - accuracy: 0.9258\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1027 - accuracy: 0.9699\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0671 - accuracy: 0.9801\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0493 - accuracy: 0.9855\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0366 - accuracy: 0.9892\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7d97bfc610>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTntBbZwv8_Z"
      },
      "source": [
        "After these 5 epochs, the model will have performed 2,345 gradient updates (469\n",
        "per epoch), and the loss of the model will be sufficiently low that the model will be\n",
        "capable of classifying handwritten digits with high accuracy.\n",
        "\n",
        "At this point, you already know most of what there is to know about neural networks.\n",
        "Let’s prove it by reimplementing a simplified version of that first example\n",
        "“from scratch” in TensorFlow, step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBUzy-DIwEJX"
      },
      "source": [
        "##NN from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEdUocYXwGYh"
      },
      "source": [
        "What better demonstrates full, unambiguous understanding than implementing everything\n",
        "from scratch? Of course, what “from scratch” means here is relative: we won’t\n",
        "reimplement basic tensor operations, and we won’t implement backpropagation. But\n",
        "we’ll go to such a low level that we will barely use any Keras functionality at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIcxzi5vLURe"
      },
      "source": [
        "###A simple Dense class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugDuoqsxLVKt"
      },
      "source": [
        "You’ve learned earlier that the Dense layer implements the following input transformation,\n",
        "where W and b are model parameters, and activation is an element-wise\n",
        "function (usually relu, but it would be softmax for the last layer):\n",
        "\n",
        "```python\n",
        "output = activation(dot(W, input) + b)\n",
        "```\n",
        "\n",
        "Let’s implement a simple Python class, NaiveDense, that creates two TensorFlow\n",
        "variables, W and b, and exposes a `__call__()` method that applies the preceding\n",
        "transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWacHeTsLe1h"
      },
      "source": [
        "class NaiveDense:\n",
        "\n",
        "  def __init__(self, input_size, output_size, activation):\n",
        "    self.activation = activation\n",
        "\n",
        "    # Create a matrix, W, of shape (input_size, output_size), initialized with random values\n",
        "    w_shape = (input_size, output_size)\n",
        "    w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "    self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "    # Create a vector, b, of shape (output_size,), initialized with zeros\n",
        "    b_shape = (output_size, )\n",
        "    b_initial_value = tf.zeros(b_shape)\n",
        "    self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "  # Convenience method for retrieving the layer’s weights\n",
        "  @property\n",
        "  def weights(self):\n",
        "    return [self.W, self.b]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJQkPSMXPTQe"
      },
      "source": [
        "###A simple Sequential class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKrQl30cPUKk"
      },
      "source": [
        "Now, let’s create a NaiveSequential class to chain these layers. It wraps a list of layers\n",
        "and exposes a `__call__()` method that simply calls the underlying layers on the\n",
        "inputs, in order. \n",
        "\n",
        "It also features a weights property to easily keep track of the layers’\n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGMMumwfPctB"
      },
      "source": [
        "class NaiveSequential:\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    weights = []\n",
        "    for layer in self.layers:\n",
        "      weights += layer.weights\n",
        "    return weights"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5AY46DYQbfZ"
      },
      "source": [
        "Using this NaiveDense class and this NaiveSequential class, we can create a mock\n",
        "Keras model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hre_5n22QcP6"
      },
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)                     \n",
        "])\n",
        "\n",
        "assert len(model.weights) == 4"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aS4RJ9WRKOh"
      },
      "source": [
        "###A batch generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCSXfHcdRLUv"
      },
      "source": [
        "Next, we need a way to iterate over the MNIST data in mini-batches. This is easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ4JD7kwROWm"
      },
      "source": [
        "class BatchGenerator:\n",
        "  def __init__(self, images, labels, batch_size=128):\n",
        "    assert len(images) == len(labels)\n",
        "\n",
        "    self.index = 0\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "  def next(self):\n",
        "    images = self.images[self.index: self.index + self.batch_size]\n",
        "    labels = self.labels[self.index: self.index + self.batch_size]\n",
        "    self.index += self.batch_size\n",
        "    return images, labels"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upyji865SWWY"
      },
      "source": [
        "##Running one training step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuEullaSSYbk"
      },
      "source": [
        "The most difficult part of the process is the “training step”: updating the weights of\n",
        "the model after running it on one batch of data. We need to\n",
        "\n",
        "1. Compute the predictions of the model for the images in the batch.\n",
        "2. Compute the loss value for these predictions, given the actual labels.\n",
        "3. Compute the gradient of the loss with regard to the model’s weights.\n",
        "4. Move the weights by a small amount in the direction opposite to the gradient.\n",
        "\n",
        "To compute the gradient, we will use the TensorFlow GradientTape object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXiPWQioyHj"
      },
      "source": [
        "def one_training_step(model, images_path, labels_path):\n",
        "  # Run the “forward pass” (compute the model’s predictions under a GradientTape scope).\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images_path)\n",
        "    per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_path, predictions)\n",
        "    average_loss = tf.reduce_mean(per_sample_losses)\n",
        "  \n",
        "  # Compute the gradient of the loss with regard to the weights. The output gradients is a list where each entry corresponds to a weight from the model.weights list.\n",
        "  gradients = tape.gradient(average_loss, model.weights)\n",
        "  # Update the weights using the gradients\n",
        "  update_weights(gradients, model.weights)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83prsh_up53S"
      },
      "source": [
        "As you already know, the purpose of the “weight update” step (represented by the preceding\n",
        "update_weights function) is to move the weights by “a bit” in a direction that\n",
        "will reduce the loss on this batch. \n",
        "\n",
        "The magnitude of the move is determined by the\n",
        "“learning rate,” typically a small quantity. The simplest way to implement this\n",
        "`update_weights` function is to subtract `gradient * learning_rate` from each weight:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZGORPMzqHQW"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "  for g, w in zip(gradients, weights):\n",
        "    # assign_sub is the equivalent of -= for TensorFlow variables\n",
        "    w.assign_sub(g * learning_rate)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttlQkrtOqrjd"
      },
      "source": [
        "In practice, you would almost never implement a weight update step like this by hand.\n",
        "\n",
        "Instead, you would use an Optimizer instance from Keras, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87dAkKgLqsfS"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def update_weights2(gradients, weights):\n",
        "  optimizers.apply_gradients(zip(gradients, weights))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rrgZhnrH9p"
      },
      "source": [
        "Now that our per-batch training step is ready, we can move on to implementing an\n",
        "entire epoch of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTweV6IhrIaU"
      },
      "source": [
        "##The full training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALwhgRfyrKNA"
      },
      "source": [
        "An epoch of training simply consists of repeating the training step for each batch in\n",
        "the training data, and the full training loop is simply the repetition of one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg5BVRZwrR05"
      },
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "  for epoch_counter in range(epochs):\n",
        "    print(f\"Epoch {epoch_counter}\")\n",
        "    batch_generator = BatchGenerator(images, labels)\n",
        "    for batch_counter in range(batch_generator.num_batches):\n",
        "      images_batch, labels_batch = batch_generator.next()\n",
        "      loss = one_training_step(model, images_batch, labels_batch)\n",
        "      if batch_counter % 100 == 0:\n",
        "        print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtG66svwtQCk"
      },
      "source": [
        "Let’s test drive it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-B7veNXtQcB"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTY-uk0btXgY",
        "outputId": "f7e9e16d-e0f3-45ff-fd39-3553f7ee9cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-95d2297f2eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-039de3d81b17>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, images, labels, epochs, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss at batch {batch_counter}: {loss:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knQSqSyHu6Mm"
      },
      "source": [
        "##Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqvFAcNbu7Dy"
      },
      "source": [
        "We can evaluate the model by taking the argmax of its predictions over the test images,\n",
        "and comparing it to the expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmwMLsaZu-Am"
      },
      "source": [
        "predictions = model(test_images)\n",
        "predictions = predictions.numpy()\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matches = predicted_labels == test_labels\n",
        "\n",
        "print(f\"accuracy: {matches.mean(): .2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ZtpsBbxpMQ"
      },
      "source": [
        "All done! As you can see, it’s quite a bit of work to do “by hand” what you can do in a\n",
        "few lines of Keras code. But because you’ve gone through these steps, you should now\n",
        "have a crystal clear understanding of what goes on inside a neural network when you\n",
        "call fit(). \n",
        "\n",
        "Having this low-level mental model of what your code is doing behind the\n",
        "scenes will make you better able to leverage the high-level features of the Keras API."
      ]
    }
  ]
}