{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-gears-of-neural-networks--tensor-operations.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPt7ybyMen7vJfcJ1vEzUuB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/1_gears_of_neural_networks_tensor_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsRP3eB2CM-h"
      },
      "source": [
        "##The gears of neural networks: Tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-hxOYQDCPwY"
      },
      "source": [
        "Much as any computer program can be ultimately reduced to a small set of binary\n",
        "operations on binary inputs (AND, OR, NOR, and so on), all transformations learned\n",
        "by deep neural networks can be reduced to a handful of tensor operations (or tensor functions)\n",
        "applied to tensors of numeric data. For instance, it’s possible to add tensors,\n",
        "multiply tensors, and so on.\n",
        "\n",
        "A Keras layer instance looks like this:\n",
        "\n",
        "```python\n",
        "keras.layers.Dense(512, activation=\"relu\")\n",
        "```\n",
        "\n",
        "This layer can be interpreted as a function, which takes as input a matrix and returns\n",
        "another matrix—a new representation for the input tensor. \n",
        "\n",
        "Specifically, the function\n",
        "is as follows (where W is a matrix and b is a vector, both attributes of the layer):\n",
        "\n",
        "```python\n",
        "output = relu(dot(input, W) + b)\n",
        "```\n",
        "\n",
        "Let’s unpack this. We have three tensor operations here:\n",
        "\n",
        "- A dot product (dot) between the input tensor and a tensor named W\n",
        "- An addition (+) between the resulting matrix and a vector b\n",
        "- A relu operation: relu(x) is max(x, 0); “relu” stands for “rectified linear unit”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVmTe862DdX3"
      },
      "source": [
        "##Element-wise operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pufUfYjgDeFS"
      },
      "source": [
        "The relu operation and addition are element-wise operations: operations that are\n",
        "applied independently to each entry in the tensors being considered. This means\n",
        "these operations are highly amenable to massively parallel implementations.\n",
        "\n",
        "If you want to write a naive Python implementation of\n",
        "an element-wise operation, you use a for loop, as in this naive implementation of an\n",
        "element-wise relu operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcnhKcAeDxEH"
      },
      "source": [
        "def naive_relu(x):\n",
        "  # x is a rank-2 NumPy tensor\n",
        "  assert len(x.shape) == 2\n",
        "\n",
        "  # Avoid overwriting the input tensor\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] = max(x[i, j], 0)\n",
        "  return x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j0R9_MkEdnk"
      },
      "source": [
        "You could do the same for addition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y18TXyIEfR5"
      },
      "source": [
        "def naive_add(x, y):\n",
        "  # x and y are rank-2 NumPy tensors\n",
        "  assert len(x.shape) == 2\n",
        "  assert x.shape == y.shape\n",
        "  \n",
        "  # Avoid overwriting the input tensor\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(y.shape[1]):\n",
        "      x[i, j] += y[i, j]\n",
        "  return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cEsCXXbGSE5"
      },
      "source": [
        "On the same principle, you can do element-wise multiplication, subtraction, and so on.\n",
        "\n",
        "In practice, when dealing with NumPy arrays, these operations are available as welloptimized\n",
        "built-in NumPy functions, which themselves delegate the heavy lifting to a\n",
        "Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are low-level,\n",
        "highly parallel, efficient tensor-manipulation routines that are typically implemented\n",
        "in Fortran or C.\n",
        "\n",
        "So, in NumPy, you can do the following element-wise operation, and it will be blazing\n",
        "fast:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LosHpiUvGhJM"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ct9Cqv5GmV_"
      },
      "source": [
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoOGO1GmGnpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8764b107-285c-40a1-8655-8b207be95ede"
      },
      "source": [
        "z = x + y\n",
        "z"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.14745926, 1.23403304, 1.07145376, ..., 0.69298026, 0.97656831,\n",
              "        0.91711773],\n",
              "       [1.00544481, 1.33681472, 0.49318897, ..., 0.48758283, 0.46381589,\n",
              "        0.75134547],\n",
              "       [0.58401856, 0.84126636, 1.53273405, ..., 1.22265976, 0.87427216,\n",
              "        0.80675042],\n",
              "       ...,\n",
              "       [0.69448852, 1.619114  , 0.5624287 , ..., 1.54145763, 1.70469161,\n",
              "        1.20280307],\n",
              "       [1.01686823, 0.18868716, 1.2857621 , ..., 0.59698089, 1.49297035,\n",
              "        0.94123989],\n",
              "       [0.80501938, 0.87875109, 1.20982281, ..., 1.07455795, 1.06342163,\n",
              "        1.07836931]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk4i44IDGsMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4861e3-d85d-4c19-b58d-e1e73fe50aff"
      },
      "source": [
        "z = np.maximum(z, 0)\n",
        "z"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.14745926, 1.23403304, 1.07145376, ..., 0.69298026, 0.97656831,\n",
              "        0.91711773],\n",
              "       [1.00544481, 1.33681472, 0.49318897, ..., 0.48758283, 0.46381589,\n",
              "        0.75134547],\n",
              "       [0.58401856, 0.84126636, 1.53273405, ..., 1.22265976, 0.87427216,\n",
              "        0.80675042],\n",
              "       ...,\n",
              "       [0.69448852, 1.619114  , 0.5624287 , ..., 1.54145763, 1.70469161,\n",
              "        1.20280307],\n",
              "       [1.01686823, 0.18868716, 1.2857621 , ..., 0.59698089, 1.49297035,\n",
              "        0.94123989],\n",
              "       [0.80501938, 0.87875109, 1.20982281, ..., 1.07455795, 1.06342163,\n",
              "        1.07836931]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrxJfhHrG5xt"
      },
      "source": [
        "Let’s actually time the difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9sk9K4KG6PE"
      },
      "source": [
        "import time"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaXyRKlJG8tO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b85777a-2422-4d53-d291-9dd3b6392668"
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "for _ in range(1000):\n",
        "  z = x + y\n",
        "  z = np.maximum(z, 0.0)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBrjciNnHWy2"
      },
      "source": [
        "This takes 0.01 s. Meanwhile, the naive version takes a stunning 2.73 s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz4eHTimHWSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44b3cec-19fe-4eba-b8d3-1292862e9d7a"
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "for _ in range(1000):\n",
        "  z = naive_add(x, y)\n",
        "  z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 2.79 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf873Hk4Hp1h"
      },
      "source": [
        "Likewise, when running TensorFlow code on a GPU, element-wise operations are executed\n",
        "via fully vectorized CUDA implementations that can best utilize the highly parallel\n",
        "GPU chip architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--BCAvkkHqSo"
      },
      "source": [
        "##Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mn3DqznHum6"
      },
      "source": [
        "When possible, and if there’s no ambiguity, the smaller tensor will be broadcast to match the shape of the larger tensor. \n",
        "\n",
        "Broadcasting consists of two steps:\n",
        "1. Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.\n",
        "2. The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\n",
        "\n",
        "Let’s look at a concrete example. Consider X with shape `(32, 10)` and y with shape `(10,)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f58IbUoqDvJ-"
      },
      "source": [
        "X = np.random.random((32, 10))    # X is a random matrix with shape (32, 10).\n",
        "y = np.random.random((10, ))      # y is a random vector with shape (10,)."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNYJWUJEKbv"
      },
      "source": [
        "First, we add an empty first axis to y, whose shape becomes `(1, 10)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdDFMjfzEGoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17689763-4b00-48fe-a15a-43b3b266573c"
      },
      "source": [
        "y = np.expand_dims(y, axis=0)    # The shape of y is now (1, 10).\n",
        "y.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3-HwHCUEc_R"
      },
      "source": [
        "Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape `(32, 10)`, where `Y[i, :] == y` for i in range`(0, 32)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JopbQXUJEV9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936c5d0e-fb5a-47e1-ef68-de426c7ba7bb"
      },
      "source": [
        "# Repeat y 32 times along axis 0 to obtain Y, which has shape (32, 10).\n",
        "Y = np.concatenate([y] * 32, axis=0)\n",
        "Y.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EbVPTnNG0M3"
      },
      "source": [
        "At this point, we can proceed to add X and Y, because they have the same shape.\n",
        "\n",
        "The repetition operation is entirely virtual: it happens at the\n",
        "algorithmic level rather than at the memory level. But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t4nYLgfHIso"
      },
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "  assert len(x.shape) == 2   # x is a rank-2 NumPy tensor\n",
        "  assert len(y.shape) == 1   # y is a NumPy vector\n",
        "  assert len(x.shape[1]) == len(y.shape[0])\n",
        "\n",
        "  # Avoid overwriting the input tensor\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] += y[j]\n",
        "  return x            "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF7AVcuPJjpy"
      },
      "source": [
        "With broadcasting, you can generally perform element-wise operations that take two inputs tensors if one tensor has shape `(a, b, … n, n + 1, … m)` and the other has shape `(n, n + 1, … m)`. The broadcasting will then automatically happen for axes a through `n - 1`.\n",
        "\n",
        "The following example applies the element-wise maximum operation to two tensors\n",
        "of different shapes via broadcasting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jux8gzwKJwyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330cc806-552b-4356-93cd-26557dd34675"
      },
      "source": [
        "x = np.random.random((64, 3, 32, 10))   # x is a random tensor with shape (64, 3, 32, 10).\n",
        "y = np.random.random((32, 10))          # y is a random tensor with shape (32, 10).\n",
        "\n",
        "z = np.maximum(x, y)                    # The output z has shape (64, 3, 32, 10) like x\n",
        "z.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 3, 32, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fveF8cJNKfeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9b49b2-772e-4797-8354-ea38baafa352"
      },
      "source": [
        "x = np.random.random((64, 3, 32, 10))   # x is a random tensor with shape (64, 3, 32, 10).\n",
        "y = np.random.random((3, 32, 10))       # y is a random tensor with shape (3, 32, 10).\n",
        "\n",
        "z = np.maximum(x, y)                    # The output z has shape (64, 3, 32, 10) like x\n",
        "z.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 3, 32, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifvnemdHKcpe"
      },
      "source": [
        "##Tensor product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhD9CzmeKdhy"
      },
      "source": [
        "The tensor product, or dot product (not to be confused with an element-wise product, the * operator), is one of the most common, most useful tensor operations.\n",
        "\n",
        "In NumPy, a tensor product is done using the np.dot function (because the mathematical notation for tensor product is usually a dot):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXiR3GBIDJap",
        "outputId": "19fc442f-5ee7-42eb-c875-871c58e1cc0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = np.random.random((32, ))\n",
        "y = np.random.random((32, ))\n",
        "\n",
        "z = np.dot(x, y)\n",
        "z"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.890335521359871"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JX3IbgxDcwM"
      },
      "source": [
        "In mathematical notation, you’d note the operation with a dot (•):\n",
        "\n",
        "```python\n",
        "z = x • y\n",
        "```\n",
        "\n",
        "Mathematically, what does the dot operation do? \n",
        "\n",
        "Let’s start with the dot product of\n",
        "two vectors, x and y. It’s computed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFedHV0pDk-P"
      },
      "source": [
        "def naive_vector_dot(x, y):\n",
        "  # x and y are NumPy vectors\n",
        "  assert len(x.shape) == 1\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[0] == y.shape[0]\n",
        "\n",
        "  z = 0.0\n",
        "  for i in range(x.shape[0]):\n",
        "    z += x[i] * y[i]\n",
        "  return z"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGpdxG1SERzQ"
      },
      "source": [
        "You’ll have noticed that the dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product.\n",
        "\n",
        "You can also take the dot product between a matrix x and a vector y, which returns a vector where the coefficients are the dot products between y and the rows of x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAP0iszEEj1C"
      },
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "  # x is a NumPy matrix\n",
        "  assert len(x.shape) == 2\n",
        "  # y is a NumPy vector\n",
        "  assert len(y.shape) == 1\n",
        "  # The first dimension of x must be the same as the 0th dimension of y!\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "\n",
        "  # This operation returns a vector of 0s with the same shape as y.\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      z[i] += x[i, j] * y[j]\n",
        "  return z"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvuIjsyEHJBn"
      },
      "source": [
        "You could also reuse the code we wrote previously, which highlights the relationship between a matrix-vector product and a vector product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjvJjmBKHKjB"
      },
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "  # x is a NumPy matrix\n",
        "  assert len(x.shape) == 2\n",
        "  # y is a NumPy vector\n",
        "  assert len(y.shape) == 1\n",
        "  # The first dimension of x must be the same as the 0th dimension of y!\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "\n",
        "  # This operation returns a vector of 0s with the same shape as y.\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    z[i] = naive_matrix_vector_dot(x[i, :], y)\n",
        "  return z"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Appp3A6hL1la"
      },
      "source": [
        "Note that as soon as one of the two tensors has an ndim greater than 1, dot is no longer symmetric, which is to say that `dot(x, y)` isn’t the same as `dot(y, x)`.\n",
        "\n",
        "Of course, a dot product generalizes to tensors with an arbitrary number of axes.\n",
        "\n",
        "The most common applications may be the dot product between two matrices. You can take the dot product of two matrices `x` and `y (dot(x, y))` if and only if `x.shape[1] == y.shape[0]`. The result is a matrix with shape `(x.shape[0], y.shape[1])`, where the coefficients are the vector products between the rows of x and the columns of y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJhv7sqFMusc"
      },
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "  # x and y are NumPy matrices\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 2\n",
        "  # The first dimension of x must be the same as the 0th dimension of y!\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "\n",
        "  # This operation returns a matrix of 0s with a specific shape.\n",
        "  z = np.zeros((x.shape[0], y.shape[1]))\n",
        "  for i in range(x.shape[0]):     # Iterates over the rows of x . . .\n",
        "    for j in range(y.shape[1]):    # . . . and over the columns of y\n",
        "      row_x = x[i, :]\n",
        "      column_y = y[:, j]\n",
        "      z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "  return z"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l0Aqc3zOa_E"
      },
      "source": [
        "To understand dot-product shape compatibility, it helps to visualize the input and output tensors by aligning them as shown.\n",
        "\n",
        "<img src='https://github.com/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "In the figure, x, y, and z are pictured as rectangles (literal boxes of coefficients). Because the rows of x and the columns of y must have the same size, it follows that the width of x must match the height of y. \n",
        "\n",
        "If you go on to develop new machine learning algorithms, you’ll likely be drawing such diagrams often.\n",
        "\n",
        "More generally, you can take the dot product between higher-dimensional tensors,\n",
        "following the same rules for shape compatibility as outlined earlier for the 2D case:\n",
        "\n",
        "```python\n",
        "(a, b, c, d) • (d,) → (a, b, c)\n",
        "(a, b, c, d) • (d, e) → (a, b, c, e)\n",
        "```\n",
        "\n",
        "And so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBP2ZSG5QPpr"
      },
      "source": [
        "##Tensor reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLmjsg1UQzYu"
      },
      "source": [
        ""
      ]
    }
  ]
}