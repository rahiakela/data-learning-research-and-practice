{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-gears-of-neural-networks--tensor-operations.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxtocVoLi6hp5EzLtfR+9f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/2-mathematical-building-blocks/1_gears_of_neural_networks_tensor_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsRP3eB2CM-h"
      },
      "source": [
        "##The gears of neural networks: Tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-hxOYQDCPwY"
      },
      "source": [
        "Much as any computer program can be ultimately reduced to a small set of binary\n",
        "operations on binary inputs (AND, OR, NOR, and so on), all transformations learned\n",
        "by deep neural networks can be reduced to a handful of tensor operations (or tensor functions)\n",
        "applied to tensors of numeric data. For instance, it’s possible to add tensors,\n",
        "multiply tensors, and so on.\n",
        "\n",
        "A Keras layer instance looks like this:\n",
        "\n",
        "```python\n",
        "keras.layers.Dense(512, activation=\"relu\")\n",
        "```\n",
        "\n",
        "This layer can be interpreted as a function, which takes as input a matrix and returns\n",
        "another matrix—a new representation for the input tensor. \n",
        "\n",
        "Specifically, the function\n",
        "is as follows (where W is a matrix and b is a vector, both attributes of the layer):\n",
        "\n",
        "```python\n",
        "output = relu(dot(input, W) + b)\n",
        "```\n",
        "\n",
        "Let’s unpack this. We have three tensor operations here:\n",
        "\n",
        "- A dot product (dot) between the input tensor and a tensor named W\n",
        "- An addition (+) between the resulting matrix and a vector b\n",
        "- A relu operation: relu(x) is max(x, 0); “relu” stands for “rectified linear unit”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVmTe862DdX3"
      },
      "source": [
        "##Element-wise operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pufUfYjgDeFS"
      },
      "source": [
        "The relu operation and addition are element-wise operations: operations that are\n",
        "applied independently to each entry in the tensors being considered. This means\n",
        "these operations are highly amenable to massively parallel implementations.\n",
        "\n",
        "If you want to write a naive Python implementation of\n",
        "an element-wise operation, you use a for loop, as in this naive implementation of an\n",
        "element-wise relu operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcnhKcAeDxEH"
      },
      "source": [
        "def naive_relu(x):\n",
        "  # x is a rank-2 NumPy tensor\n",
        "  assert len(x.shape) == 2\n",
        "\n",
        "  # Avoid overwriting the input tensor\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] = max(x[i, j], 0)\n",
        "  return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j0R9_MkEdnk"
      },
      "source": [
        "You could do the same for addition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y18TXyIEfR5"
      },
      "source": [
        "def naive_add(x, y):\n",
        "  # x and y are rank-2 NumPy tensors\n",
        "  assert len(x.shape) == 2\n",
        "  assert x.shape == y.shape\n",
        "  \n",
        "  # Avoid overwriting the input tensor\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(y.shape[1]):\n",
        "      x[i, j] += y[i, j]\n",
        "  return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cEsCXXbGSE5"
      },
      "source": [
        "On the same principle, you can do element-wise multiplication, subtraction, and so on.\n",
        "\n",
        "In practice, when dealing with NumPy arrays, these operations are available as welloptimized\n",
        "built-in NumPy functions, which themselves delegate the heavy lifting to a\n",
        "Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are low-level,\n",
        "highly parallel, efficient tensor-manipulation routines that are typically implemented\n",
        "in Fortran or C.\n",
        "\n",
        "So, in NumPy, you can do the following element-wise operation, and it will be blazing\n",
        "fast:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LosHpiUvGhJM"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ct9Cqv5GmV_"
      },
      "source": [
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoOGO1GmGnpr",
        "outputId": "2c3a386e-f4bb-4178-bfdd-2cccb07f598b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "z = x + y\n",
        "z"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.01274148, 1.61155269, 1.53795073, ..., 0.77090228, 1.54068284,\n",
              "        1.09296087],\n",
              "       [1.73714791, 0.40342784, 1.33469845, ..., 0.54809805, 0.34937233,\n",
              "        1.55131   ],\n",
              "       [0.39046875, 0.80679872, 0.99828881, ..., 1.04383479, 0.90647764,\n",
              "        1.37171019],\n",
              "       ...,\n",
              "       [1.25587013, 1.55416512, 0.8519494 , ..., 0.59000461, 1.33213851,\n",
              "        1.63504904],\n",
              "       [1.41566618, 1.70147724, 1.67182684, ..., 0.27070605, 0.34975674,\n",
              "        0.86700909],\n",
              "       [0.60044965, 0.19018441, 1.08574581, ..., 0.43884102, 0.47323858,\n",
              "        1.10698798]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk4i44IDGsMg",
        "outputId": "4854b175-efb5-40e5-c4c2-a350c756f508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "z = np.maximum(z, 0)\n",
        "z"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.01274148, 1.61155269, 1.53795073, ..., 0.77090228, 1.54068284,\n",
              "        1.09296087],\n",
              "       [1.73714791, 0.40342784, 1.33469845, ..., 0.54809805, 0.34937233,\n",
              "        1.55131   ],\n",
              "       [0.39046875, 0.80679872, 0.99828881, ..., 1.04383479, 0.90647764,\n",
              "        1.37171019],\n",
              "       ...,\n",
              "       [1.25587013, 1.55416512, 0.8519494 , ..., 0.59000461, 1.33213851,\n",
              "        1.63504904],\n",
              "       [1.41566618, 1.70147724, 1.67182684, ..., 0.27070605, 0.34975674,\n",
              "        0.86700909],\n",
              "       [0.60044965, 0.19018441, 1.08574581, ..., 0.43884102, 0.47323858,\n",
              "        1.10698798]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrxJfhHrG5xt"
      },
      "source": [
        "Let’s actually time the difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9sk9K4KG6PE"
      },
      "source": [
        "import time"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaXyRKlJG8tO",
        "outputId": "4d383354-d1ce-4a3d-805b-9662a65e0c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "for _ in range(1000):\n",
        "  z = x + y\n",
        "  z = np.maximum(z, 0.0)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBrjciNnHWy2"
      },
      "source": [
        "This takes 0.01 s. Meanwhile, the naive version takes a stunning 2.73 s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz4eHTimHWSF",
        "outputId": "27820724-38d6-491a-fe6e-194673184f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "for _ in range(1000):\n",
        "  z = naive_add(x, y)\n",
        "  z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 2.73 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf873Hk4Hp1h"
      },
      "source": [
        "Likewise, when running TensorFlow code on a GPU, element-wise operations are executed\n",
        "via fully vectorized CUDA implementations that can best utilize the highly parallel\n",
        "GPU chip architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--BCAvkkHqSo"
      },
      "source": [
        "##Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mn3DqznHum6"
      },
      "source": [
        ""
      ]
    }
  ]
}