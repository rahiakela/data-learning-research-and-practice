{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-keras-custom-training-and-evaluation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM1C6VXO93NC1od8kLT+23I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/7-deep-dive-into-keras/03_keras_custom_training_and_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keras custom training and evaluation fundamentals"
      ],
      "metadata": {
        "id": "luPerGH_0G31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three APIs for building models in Keras:\n",
        "\n",
        "* The Sequential model, the most approachable API—it’s basically a Python list. As such, it’s limited to simple stacks of layers.\n",
        "* The Functional API, which focuses on graph-like model architectures. It represents\n",
        "a nice mid-point between usability and flexibility, and as such, it’s the\n",
        "most commonly used model-building API.\n",
        "* Model subclassing, a low-level option where you write everything yourself from\n",
        "scratch. This is ideal if you want full control over every little thing. However, you\n",
        "won’t get access to many built-in Keras features, and you will be more at risk of\n",
        "making mistakes.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/7-deep-dive-into-keras/images/1.png?raw=1' width='600'/>"
      ],
      "metadata": {
        "id": "TdZ1Nkni0Pga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "USIIvxaF9LXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "t_xajM529MdN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "id": "lsEhxzMfFG_L",
        "outputId": "b8087195-8ec1-402d-b1f3-47c99da55e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():\n",
        "  inputs = keras.Input(shape=(28 * 28, ))\n",
        "  features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "  features = layers.Dropout(0.5)(features)\n",
        "  outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "HApNgYLcFHd3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using built-in training and evaluation loops"
      ],
      "metadata": {
        "id": "WyN_Mwlm9SSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After all, the built-in `fit()` workflow is solely focused on supervised learning: a setup\n",
        "where there are known targets (also called labels or annotations) associated with your\n",
        "input data, and where you compute your loss as a function of these targets and the\n",
        "model’s predictions. \n",
        "\n",
        "\n",
        "However, not every form of machine learning falls into this category. There are other setups where no explicit targets are present, such as generative\n",
        "learning self-supervised learning (where targets\n",
        "are obtained from the inputs), and reinforcement learning (where learning is driven by\n",
        "occasional “rewards,” much like training a dog).\n",
        "\n",
        "Whenever you find yourself in a situation where the built-in fit() is not enough,\n",
        "you will need to write your own custom training logic.\n",
        "\n",
        "As a reminder, the contents of a\n",
        "typical training loop look like this:\n",
        "\n",
        "* Run the forward pass (compute the model’s output) inside a gradient tape to\n",
        "obtain a loss value for the current batch of data.\n",
        "* Retrieve the gradients of the loss with regard to the model’s weights.\n",
        "* Update the model’s weights so as to lower the loss value on the current batch of data.\n",
        "\n",
        "These steps are repeated for as many batches as necessary. This is essentially what\n",
        "`fit()` does under the hood."
      ],
      "metadata": {
        "id": "-7MIA3a99S8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training versus inference"
      ],
      "metadata": {
        "id": "h8MWD0tkAvrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the low-level training loop examples you’ve seen so far:\n",
        "\n",
        "* step 1 (the forward pass) was done via `predictions = model(inputs)`\n",
        "* step 2 (retrieving the gradients computed by the gradient tape) was done via `gradients = tape.gradient(loss,model.weights)`\n",
        "\n",
        "Some Keras layers, such as the `Dropout` layer, have different behaviors during training and during inference. Such layers expose\n",
        "a training Boolean argument in their `call()` method. \n",
        "\n",
        "Calling `dropout(inputs,\n",
        "training=True)` will drop some activation entries, while calling `dropout(inputs, training=False)` does nothing.\n",
        "\n",
        "In addition, note that when you retrieve the gradients of the weights of your\n",
        "model, you should not use `tape.gradients(loss, model.weights)`, but rather `tape.gradients(loss, model.trainable_weights)`. Indeed, layers and models own two kinds of weights:\n",
        "\n",
        "* **Trainable weights**—These are meant to be updated via backpropagation to minimize the loss of the model, such as the kernel and bias of a Dense layer.\n",
        "* **Non-trainable weights**—These are meant to be updated during the forward pass by the layers that own them.\n",
        "\n",
        "Among Keras built-in layers, the only layer that features non-trainable weights is the `BatchNormalization` layer.The `BatchNormalization`\n",
        "layer needs non-trainable weights in order to track information about the mean and\n",
        "standard deviation of the data that passes through it, so as to perform an online\n",
        "approximation of feature normalization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2BmgTQUrJW9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "  gradients = tape.gradients(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(model.trainable_weights, gradients))"
      ],
      "metadata": {
        "id": "6x6fmKh1JU-f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Low-level usage of metrics"
      ],
      "metadata": {
        "id": "LJ951pStxXT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ve already learned about the metrics API: simply\n",
        "call `update_state(y_true, y_pred)` for each batch of targets and predictions, and\n",
        "then use `result()` to query the current metric value:"
      ],
      "metadata": {
        "id": "up57ax70F9l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result: {current_result:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FtKi4uGxZtZ",
        "outputId": "30a3ca2f-04ff-47e1-e897-91adb585a9c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may also need to track the average of a scalar value, such as the model’s loss. You\n",
        "can do this via the `keras.metrics.Mean metric`:"
      ],
      "metadata": {
        "id": "-7PnJOt7vG8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for val in values:\n",
        "  mean_tracker.update_state(val)\n",
        "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THoc3w3dxtDz",
        "outputId": "1cbc1871-bbbf-43a8-e7db-e053d4a0f26c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values: 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to use `metric.reset_state()` when you want to reset the current results."
      ],
      "metadata": {
        "id": "60kAwh-xvgr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A complete training and evaluation loop"
      ],
      "metadata": {
        "id": "2LZ856EkBaDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s combine the forward pass, backward pass, and metrics tracking into a `fit()`-like training step function that takes a batch of data and targets and returns the logs that\n",
        "would get displayed by the `fit()` progress bar."
      ],
      "metadata": {
        "id": "lb5R-ufpBfks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mnist_model()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "loss_tracking_metric = keras.metrics.Mean()\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Run the forward pass, Note that we pass training=True\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  # Run the backward pass. Note that we use model.trainable_weights\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  # Keep track of metrics\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[metric.name] = metric.result()\n",
        "\n",
        "  # Keep track of the loss average\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs[\"loss\"] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "R1He4KzAvAlp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to reset the state of our metrics at the start of each epoch and before running evaluation."
      ],
      "metadata": {
        "id": "pyUMADTF-5N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "Ip5zzaoy-6dA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now lay out our complete training loop. \n",
        "\n",
        "Note that we use a `tf.data.Dataset` object to turn our NumPy data into an iterator that iterates over the data in batches of size 32."
      ],
      "metadata": {
        "id": "RuyiHsr4_QwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)\n",
        "  print(f\"Results at the end of epoch {epoch}\")\n",
        "  for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "fntIACzX_Upr",
        "outputId": "09a26b30-21dc-4a17-b0bc-ae1acd94a341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results at the end of epoch 0\n",
            "...sparse_categorical_accuracy: 0.9154\n",
            "...loss: 0.2884\n",
            "Results at the end of epoch 1\n",
            "...sparse_categorical_accuracy: 0.9531\n",
            "...loss: 0.1670\n",
            "Results at the end of epoch 2\n",
            "...sparse_categorical_accuracy: 0.9623\n",
            "...loss: 0.1377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here’s the evaluation loop: a simple for loop that repeatedly calls a `test_step()` function, which processes a single batch of data. The `test_step()` function is just a subset of the logic of `train_step()`. \n",
        "\n",
        "It omits the code that deals with updating the weights\n",
        "of the model—that is to say, everything involving the `GradientTape` and the optimizer."
      ],
      "metadata": {
        "id": "fZ1u5Bd1BKEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(inputs, targets):\n",
        "  # Make prediction, Note that we pass training=False\n",
        "  predictions = model(inputs, training=False)\n",
        "  loss = loss_fn(targets, predictions)\n",
        "\n",
        "  # Keep track of metrics\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[f\"val_{metric.name}\"] = metric.result()\n",
        "\n",
        "  # Keep track of the loss average\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "grE0SWRIBZKW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "  print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "XwdS365HDOa3",
        "outputId": "5ce0ad95-0d0a-491d-bbfd-8f0034001bcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9690\n",
            "...val_loss: 0.1255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats—you’ve just reimplemented `fit()` and `evaluate()`!"
      ],
      "metadata": {
        "id": "B_fwP8m1D_q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make it fast with tf.function"
      ],
      "metadata": {
        "id": "DzSbaACtECgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed that your custom loops are running significantly slower than the\n",
        "built-in `fit()` and `evaluate()`, despite implementing essentially the same logic.\n",
        "That’s because, by default, TensorFlow code is executed line by line, eagerly, much like\n",
        "NumPy code or regular Python code. Eager execution makes it easier to debug your\n",
        "code, but it is far from optimal from a performance standpoint.\n",
        "\n",
        "It’s more performant to compile your TensorFlow code into a computation graph that\n",
        "can be globally optimized in a way that code interpreted line by line cannot. The syntax\n",
        "to do this is very simple: just add a `@tf.function` to any function you want to compile\n",
        "before executing."
      ],
      "metadata": {
        "id": "lAddPcxoEFsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(inputs, targets):\n",
        "  # Make prediction, Note that we pass training=False\n",
        "  predictions = model(inputs, training=False)\n",
        "  loss = loss_fn(targets, predictions)\n",
        "\n",
        "  # Keep track of metrics\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[f\"val_{metric.name}\"] = metric.result()\n",
        "\n",
        "  # Keep track of the loss average\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "aqIQ3cgEHPSQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "  print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "yxJmAswyHY9W",
        "outputId": "5ef273e9-e472-420f-8921-fe87e9bd406d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9690\n",
            "...val_loss: 0.1255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the Colab CPU, we go from taking `1.80s` to run the evaluation loop to only `0.8s`. Much faster!\n",
        "\n",
        "Remember, while you are debugging your code, prefer running it eagerly, without\n",
        "any `@tf.function` decorator. It’s easier to track bugs this way. Once your code is working\n",
        "and you want to make it fast, add a `@tf.function` decorator to your training step\n",
        "and your evaluation step—or any other performance-critical function."
      ],
      "metadata": {
        "id": "8_ico5emHieM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom training loop"
      ],
      "metadata": {
        "id": "68t0sq2-H_oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T48KItvCIAXL"
      }
    }
  ]
}