{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-machine-translation--sequence-to-sequence-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcquHEW+sGQgci3OzPzESZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/04_machine_translation_sequence_to_sequence_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Machine translation: A sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "Pka5_bIxkQdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you’ll deepen your expertise by learning about\n",
        "sequence-to-sequence models.\n",
        "\n",
        "A sequence-to-sequence model takes a sequence as input (often a sentence or\n",
        "paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n",
        "- **Machine translation**—Convert a paragraph in a source language to its equivalent in a target language.\n",
        "- **Text summarization**—Convert a long document to a shorter version that retains the most important information.\n",
        "- **Question answering**—Convert an input question into its answer.\n",
        "- **Chatbots**—Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
        "- **Text generation**—Convert a text prompt into a paragraph that completes the prompt.\n",
        "\n",
        "The general template behind sequence-to-sequence models is described in figure.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/3.png?raw=1' width='600'/>\n",
        "\n",
        "During training:-\n",
        "- An `encoder` model turns the source sequence into an intermediate representation.\n",
        "- A `decoder` is trained to predict the next token i in the target sequence by looking at both previous tokens `(0 to i - 1)` and the encoded source sequence.\n",
        "\n",
        "**During inference, we don’t have access to the target sequence**—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
        "\n",
        "- We obtain the encoded source sequence from the encoder.\n",
        "- The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string `[start]`), and uses them to predict the first real token in the sequence.\n",
        "- The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string\n",
        "`[end]`).\n",
        "\n",
        "Everything you’ve learned so far can be repurposed to build this new kind of model.\n",
        "\n",
        "Let’s dive in.\n"
      ],
      "metadata": {
        "id": "BHePHVWuk-6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "KkxNMWG1mnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XUDQfXo9mpJp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll be working with an English-to-Spanish translation dataset available at\n",
        "www.manythings.org/anki/. \n",
        "\n",
        "Let’s download it:"
      ],
      "metadata": {
        "id": "NGB_nEcMnipW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "MjRO78CmnnH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "DeMzuQbmo4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. \n",
        "\n",
        "Let’s parse this file."
      ],
      "metadata": {
        "id": "qWjNE2oLo7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "  # Each line contains an English phrase and its Spanish translation, tab-separated.\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  # We prepend \"[start]\" and append \"[end]\" to the Spanish sentence, to match the template\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "QcKV-E2rpKfx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `text_pairs` look like this:"
      ],
      "metadata": {
        "id": "6PYcBUf5rBzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ts-AXBMzrEeq",
        "outputId": "0bc215d2-cefa-42f5-9704-69dc58442dcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I feel more comfortable behind the wheel.', '[start]Me siento más cómodo conduciendo.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ZKHJ29-JrVaI",
        "outputId": "2c079997-2ba8-4571-94df-1614dc19656b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('He lives across the street from us.', '[start]Vive al otro lado de nuestra calle.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s shuffle them and split them into the usual training, validation, and test sets:"
      ],
      "metadata": {
        "id": "TgG4WIZ1rTIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "_xCJvrN7rT0k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s prepare two separate TextVectorization layers: one for English and one for Spanish. \n",
        "\n",
        "We’re going to need to customize the way strings are preprocessed:"
      ],
      "metadata": {
        "id": "ZScoIPTZuKY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a custom string standardization function for the Spanish TextVectorization layer: it preserves [ and ] \n",
        "# but strips ¿ (as well as all other characters from strings.punctuation).\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "# Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.\n",
        "target_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", \n",
        "                                                output_sequence_length=sequence_length + 1,\n",
        "                                                standardize=custom_standardization)\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# Learn the vocabulary of each language\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "Wh7IlyfVuNSl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can turn our data into a tf.data pipeline. \n",
        "\n",
        "We want it to return a tuple `(inputs, target)` where `inputs` is a dict with two keys,`encoder_inputs` (the English sentence) and `decoder_inputs` (the Spanish sentence), and `target` is the Spanish sentence offset by one step ahead."
      ],
      "metadata": {
        "id": "flQpu1pcwxpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return (\n",
        "      {\n",
        "      \"english\": eng,\n",
        "      \"spanish\": spa[:, :-1],  # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length\n",
        "      },\n",
        "      spa[:, 1:]   # The target Spanish sentence is one step ahead. Both are still the same length (20 words)\n",
        "  )"
      ],
      "metadata": {
        "id": "lRd8xb_uw68P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  # Use in-memory caching to speed up preprocessing\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "metadata": {
        "id": "mQNGKIVKzAF7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "FatVJpipzx8c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what our dataset outputs look like:"
      ],
      "metadata": {
        "id": "B_Bd8zTL0EpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "TkPwTeeH0Gnu",
        "outputId": "0a698f06-7545-4ef6-e84f-4a527615c890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now ready—time to build some models. We’ll start with a recurrent\n",
        "sequence-to-sequence model before moving on to a Transformer."
      ],
      "metadata": {
        "id": "3sz5nJvQ0sBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "wc2FZshD0spZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step. \n",
        "\n",
        "In Keras, it would look like this:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "However, there are two major issues with this approach:\n",
        "\n",
        "* The target sequence must always be the same length as the source sequence.\n",
        "* Due to the step-by-step nature of RNNs, the model will only be looking at\n",
        "tokens `0…N` in the source sequence in order to predict token N in the target\n",
        "sequence. This constraint makes this setup unsuitable for most tasks, and\n",
        "particularly translation.\n",
        "\n",
        "If you’re a human translator, you’d start by reading the entire source sentence before\n",
        "starting to translate it. This is especially important if you’re dealing with languages\n",
        "that have wildly different word ordering, like English and Japanese. And that’s exactly\n",
        "what standard sequence-to-sequence models do.\n",
        "\n",
        "In a proper sequence-to-sequence setup, you would first use an\n",
        "RNN (the encoder) to turn the entire source sequence into a single vector (or set of\n",
        "vectors). \n",
        "\n",
        "This could be the last output of the RNN, or alternatively, its final internal\n",
        "state vectors. \n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/4.png?raw=1' width='600'/>\n",
        "\n",
        "Then you would use this vector (or vectors) as the `initial state` of another RNN (the decoder), which would look at elements `0…N` in the target sequence, and\n",
        "try to predict step `N+1` in the target sequence.\n",
        "\n",
        "Let’s implement this in Keras with GRU-based encoders and decoders. The choice\n",
        "of GRU rather than LSTM makes things a bit simpler, since GRU only has a single\n",
        "state vector, whereas LSTM has multiple. \n",
        "\n",
        "Let’s start with the encoder."
      ],
      "metadata": {
        "id": "64zlIq2T0wkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024"
      ],
      "metadata": {
        "id": "kfQrePqY6SEx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The English source sentence goes here.\n",
        "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\n",
        "# Don’t forget masking: it’s critical in this setup\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "RYOcpMqK4Fpj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s add the decoder—a simple GRU layer that takes as its initial state the encoded source sentence. \n",
        "\n",
        "On top of it, we add a Dense layer that produces for each\n",
        "output step a probability distribution over the Spanish vocabulary."
      ],
      "metadata": {
        "id": "6yPp5_kW-TFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Spanish target sentence goes here\n",
        "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "# The encoded source sentence serves as the initial state of the decoder GRU\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predicts the next token\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future\n",
        "seq2seq_rnn = keras.Model(inputs=[source, past_target], outputs=target_next_step)"
      ],
      "metadata": {
        "id": "kaP45JT8-bFT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, the decoder takes as input the entire target sequence, but thanks to\n",
        "the step-by-step nature of RNNs, it only looks at tokens `0…N` in the input to predict token N in the output (which corresponds to the next token in the sequence, since\n",
        "the output is intended to be offset by one step). \n",
        "\n",
        "This means we only use information\n",
        "from the past to predict the future, as we should; otherwise we’d be cheating, and our\n",
        "model would not work at inference time.\n",
        "\n",
        "Let’s start training."
      ],
      "metadata": {
        "id": "8Hhq-ul-Bk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "XxMp-usaB2WW",
        "outputId": "44dcfedb-93b6-45de-eb90-7f6fe7c95324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 129s 89ms/step - loss: 1.4788 - accuracy: 0.3245 - val_loss: 1.2061 - val_accuracy: 0.4267\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 1.2140 - accuracy: 0.4477 - val_loss: 1.0664 - val_accuracy: 0.4965\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 1.0930 - accuracy: 0.5039 - val_loss: 0.9931 - val_accuracy: 0.5337\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 1.0115 - accuracy: 0.5398 - val_loss: 0.9639 - val_accuracy: 0.5540\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.9690 - accuracy: 0.5660 - val_loss: 0.9531 - val_accuracy: 0.5631\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.9426 - accuracy: 0.5881 - val_loss: 0.9505 - val_accuracy: 0.5697\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.9256 - accuracy: 0.6049 - val_loss: 0.9529 - val_accuracy: 0.5731\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.9143 - accuracy: 0.6165 - val_loss: 0.9565 - val_accuracy: 0.5742\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.9060 - accuracy: 0.6266 - val_loss: 0.9619 - val_accuracy: 0.5749\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.9000 - accuracy: 0.6337 - val_loss: 0.9641 - val_accuracy: 0.5785\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8962 - accuracy: 0.6388 - val_loss: 0.9671 - val_accuracy: 0.5781\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8919 - accuracy: 0.6429 - val_loss: 0.9731 - val_accuracy: 0.5770\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8907 - accuracy: 0.6454 - val_loss: 0.9752 - val_accuracy: 0.5774\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8894 - accuracy: 0.6470 - val_loss: 0.9790 - val_accuracy: 0.5779\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8887 - accuracy: 0.6487 - val_loss: 0.9801 - val_accuracy: 0.5771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f80220e93d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We picked accuracy as a crude way to monitor validation-set performance during\n",
        "training. We get to 64% accuracy: on average, the model predicts the next word in the\n",
        "Spanish sentence correctly 64% of the time. However, in practice, next-token accuracy\n",
        "isn’t a great metric for machine translation models.\n",
        "\n",
        "If you work on a real-world machine translation system, you will likely use `BLEU scores` to evaluate your models—a metric that looks at entire generated sequences\n",
        "and that seems to correlate well with human perception of translation quality.\n",
        "\n",
        "At last, let’s use our model for inference.\n",
        "\n",
        "We’ll pick a few sentences in the test set\n",
        "and check how our model translates them. We’ll start from the seed token, `[start]`,\n",
        "and feed it into the decoder model, together with the encoded English source sentence.\n",
        "\n",
        "We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoder\n",
        "repeatedly, sampling one new target token at each iteration, until we get to `[end]`\n",
        "or reach the maximum sentence length."
      ],
      "metadata": {
        "id": "4-_XLXPLC40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GP4YWv07DkXJ"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}