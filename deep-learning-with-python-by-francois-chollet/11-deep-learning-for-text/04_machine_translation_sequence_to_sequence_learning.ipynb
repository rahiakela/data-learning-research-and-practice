{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-machine-translation--sequence-to-sequence-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcmROuCB3dQ6ld66hwMCpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/04_machine_translation_sequence_to_sequence_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Machine translation: A sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "Pka5_bIxkQdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you’ll deepen your expertise by learning about\n",
        "sequence-to-sequence models.\n",
        "\n",
        "A sequence-to-sequence model takes a sequence as input (often a sentence or\n",
        "paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n",
        "- **Machine translation**—Convert a paragraph in a source language to its equivalent in a target language.\n",
        "- **Text summarization**—Convert a long document to a shorter version that retains the most important information.\n",
        "- **Question answering**—Convert an input question into its answer.\n",
        "- **Chatbots**—Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
        "- **Text generation**—Convert a text prompt into a paragraph that completes the prompt.\n",
        "\n",
        "The general template behind sequence-to-sequence models is described in figure.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/3.png?raw=1' width='600'/>\n",
        "\n",
        "During training:-\n",
        "- An `encoder` model turns the source sequence into an intermediate representation.\n",
        "- A `decoder` is trained to predict the next token i in the target sequence by looking at both previous tokens `(0 to i - 1)` and the encoded source sequence.\n",
        "\n",
        "**During inference, we don’t have access to the target sequence**—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
        "\n",
        "- We obtain the encoded source sequence from the encoder.\n",
        "- The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string `[start]`), and uses them to predict the first real token in the sequence.\n",
        "- The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string\n",
        "`[end]`).\n",
        "\n",
        "Everything you’ve learned so far can be repurposed to build this new kind of model.\n",
        "\n",
        "Let’s dive in.\n"
      ],
      "metadata": {
        "id": "BHePHVWuk-6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "KkxNMWG1mnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XUDQfXo9mpJp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll be working with an English-to-Spanish translation dataset available at\n",
        "www.manythings.org/anki/. \n",
        "\n",
        "Let’s download it:"
      ],
      "metadata": {
        "id": "NGB_nEcMnipW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "MjRO78CmnnH7",
        "outputId": "9ec10cc6-dbeb-4ad4-8522-4a552c61d0b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-01 04:24:24--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.6.128, 74.125.126.128, 74.125.132.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.6.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-01 04:24:24 (149 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "DeMzuQbmo4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. \n",
        "\n",
        "Let’s parse this file."
      ],
      "metadata": {
        "id": "qWjNE2oLo7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "  # Each line contains an English phrase and its Spanish translation, tab-separated.\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  # We prepend \"[start]\" and append \"[end]\" to the Spanish sentence, to match the template\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "QcKV-E2rpKfx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `text_pairs` look like this:"
      ],
      "metadata": {
        "id": "6PYcBUf5rBzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ts-AXBMzrEeq",
        "outputId": "6b8299e4-f173-4c87-8f29-e79fe85a163c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Is there something that you want?', '[start]¿Hay algo que quieras?[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ZKHJ29-JrVaI",
        "outputId": "ece8d731-90ca-4b11-8e35-99e9a263ad04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Mary was arrested for shoplifting.', '[start]Mary fue arrestada por ratera.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s shuffle them and split them into the usual training, validation, and test sets:"
      ],
      "metadata": {
        "id": "TgG4WIZ1rTIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "_xCJvrN7rT0k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s prepare two separate TextVectorization layers: one for English and one for Spanish. \n",
        "\n",
        "We’re going to need to customize the way strings are preprocessed:"
      ],
      "metadata": {
        "id": "ZScoIPTZuKY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a custom string standardization function for the Spanish TextVectorization layer: it preserves [ and ] \n",
        "# but strips ¿ (as well as all other characters from strings.punctuation).\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "# Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.\n",
        "target_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", \n",
        "                                                output_sequence_length=sequence_length + 1,\n",
        "                                                standardize=custom_standardization)\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# Learn the vocabulary of each language\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "Wh7IlyfVuNSl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can turn our data into a tf.data pipeline. \n",
        "\n",
        "We want it to return a tuple `(inputs, target)` where `inputs` is a dict with two keys,`encoder_inputs` (the English sentence) and `decoder_inputs` (the Spanish sentence), and `target` is the Spanish sentence offset by one step ahead."
      ],
      "metadata": {
        "id": "flQpu1pcwxpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return (\n",
        "      {\n",
        "      \"english\": eng,\n",
        "      \"spanish\": spa[:, :-1],  # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length\n",
        "      },\n",
        "      spa[:, 1:]   # The target Spanish sentence is one step ahead. Both are still the same length (20 words)\n",
        "  )"
      ],
      "metadata": {
        "id": "lRd8xb_uw68P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  # Use in-memory caching to speed up preprocessing\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "metadata": {
        "id": "mQNGKIVKzAF7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "FatVJpipzx8c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what our dataset outputs look like:"
      ],
      "metadata": {
        "id": "B_Bd8zTL0EpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "TkPwTeeH0Gnu",
        "outputId": "225beb8a-0d40-4f10-d47c-b9753b00378d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now ready—time to build some models. We’ll start with a recurrent\n",
        "sequence-to-sequence model before moving on to a Transformer."
      ],
      "metadata": {
        "id": "3sz5nJvQ0sBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "wc2FZshD0spZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step. \n",
        "\n",
        "In Keras, it would look like this:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "However, there are two major issues with this approach:\n",
        "\n",
        "* The target sequence must always be the same length as the source sequence.\n",
        "* Due to the step-by-step nature of RNNs, the model will only be looking at\n",
        "tokens `0…N` in the source sequence in order to predict token N in the target\n",
        "sequence. This constraint makes this setup unsuitable for most tasks, and\n",
        "particularly translation.\n",
        "\n",
        "If you’re a human translator, you’d start by reading the entire source sentence before\n",
        "starting to translate it. This is especially important if you’re dealing with languages\n",
        "that have wildly different word ordering, like English and Japanese. And that’s exactly\n",
        "what standard sequence-to-sequence models do.\n",
        "\n",
        "In a proper sequence-to-sequence setup, you would first use an\n",
        "RNN (the encoder) to turn the entire source sequence into a single vector (or set of\n",
        "vectors). \n",
        "\n",
        "This could be the last output of the RNN, or alternatively, its final internal\n",
        "state vectors. \n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/4.png?raw=1' width='600'/>\n",
        "\n",
        "Then you would use this vector (or vectors) as the `initial state` of another RNN (the decoder), which would look at elements `0…N` in the target sequence, and\n",
        "try to predict step `N+1` in the target sequence.\n",
        "\n",
        "Let’s implement this in Keras with GRU-based encoders and decoders. The choice\n",
        "of GRU rather than LSTM makes things a bit simpler, since GRU only has a single\n",
        "state vector, whereas LSTM has multiple. \n",
        "\n",
        "Let’s start with the encoder."
      ],
      "metadata": {
        "id": "64zlIq2T0wkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024"
      ],
      "metadata": {
        "id": "kfQrePqY6SEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The English source sentence goes here.\n",
        "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\n",
        "# Don’t forget masking: it’s critical in this setup\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "RYOcpMqK4Fpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s add the decoder—a simple GRU layer that takes as its initial state the encoded source sentence. \n",
        "\n",
        "On top of it, we add a Dense layer that produces for each\n",
        "output step a probability distribution over the Spanish vocabulary."
      ],
      "metadata": {
        "id": "6yPp5_kW-TFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Spanish target sentence goes here\n",
        "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "# The encoded source sentence serves as the initial state of the decoder GRU\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predicts the next token\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future\n",
        "seq2seq_rnn = keras.Model(inputs=[source, past_target], outputs=target_next_step)"
      ],
      "metadata": {
        "id": "kaP45JT8-bFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, the decoder takes as input the entire target sequence, but thanks to\n",
        "the step-by-step nature of RNNs, it only looks at tokens `0…N` in the input to predict token N in the output (which corresponds to the next token in the sequence, since\n",
        "the output is intended to be offset by one step). \n",
        "\n",
        "This means we only use information\n",
        "from the past to predict the future, as we should; otherwise we’d be cheating, and our\n",
        "model would not work at inference time.\n",
        "\n",
        "Let’s start training."
      ],
      "metadata": {
        "id": "8Hhq-ul-Bk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "XxMp-usaB2WW",
        "outputId": "82f5133b-41de-44ae-e6ee-5e6b9d4cf0d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 132s 91ms/step - loss: 1.4797 - accuracy: 0.3238 - val_loss: 1.2083 - val_accuracy: 0.4242\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 1.2150 - accuracy: 0.4467 - val_loss: 1.0688 - val_accuracy: 0.4953\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 121s 93ms/step - loss: 1.0937 - accuracy: 0.5035 - val_loss: 0.9998 - val_accuracy: 0.5309\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 115s 89ms/step - loss: 1.0118 - accuracy: 0.5391 - val_loss: 0.9691 - val_accuracy: 0.5504\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 117s 90ms/step - loss: 0.9689 - accuracy: 0.5658 - val_loss: 0.9580 - val_accuracy: 0.5601\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 121s 93ms/step - loss: 0.9418 - accuracy: 0.5889 - val_loss: 0.9576 - val_accuracy: 0.5663\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 123s 94ms/step - loss: 0.9254 - accuracy: 0.6050 - val_loss: 0.9611 - val_accuracy: 0.5696\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 122s 94ms/step - loss: 0.9133 - accuracy: 0.6179 - val_loss: 0.9655 - val_accuracy: 0.5706\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 118s 91ms/step - loss: 0.9049 - accuracy: 0.6275 - val_loss: 0.9687 - val_accuracy: 0.5731\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8990 - accuracy: 0.6346 - val_loss: 0.9723 - val_accuracy: 0.5741\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 116s 89ms/step - loss: 0.8948 - accuracy: 0.6397 - val_loss: 0.9763 - val_accuracy: 0.5748\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8915 - accuracy: 0.6437 - val_loss: 0.9810 - val_accuracy: 0.5734\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8899 - accuracy: 0.6462 - val_loss: 0.9836 - val_accuracy: 0.5744\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8885 - accuracy: 0.6480 - val_loss: 0.9847 - val_accuracy: 0.5745\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8881 - accuracy: 0.6491 - val_loss: 0.9870 - val_accuracy: 0.5750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4cd5975d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We picked accuracy as a crude way to monitor validation-set performance during\n",
        "training. We get to 64% accuracy: on average, the model predicts the next word in the\n",
        "Spanish sentence correctly 64% of the time. However, in practice, next-token accuracy\n",
        "isn’t a great metric for machine translation models.\n",
        "\n",
        "If you work on a real-world machine translation system, you will likely use `BLEU scores` to evaluate your models—a metric that looks at entire generated sequences\n",
        "and that seems to correlate well with human perception of translation quality.\n",
        "\n",
        "At last, let’s use our model for inference.\n",
        "\n",
        "We’ll pick a few sentences in the test set\n",
        "and check how our model translates them. We’ll start from the seed token, `[start]`,\n",
        "and feed it into the decoder model, together with the encoded English source sentence.\n",
        "\n",
        "We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoder\n",
        "repeatedly, sampling one new target token at each iteration, until we get to `[end]`\n",
        "or reach the maximum sentence length."
      ],
      "metadata": {
        "id": "4-_XLXPLC40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a dict to convert token index predictions to string tokens\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  # seed token\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    # sample the next token\n",
        "    next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    # Convert the next token prediction to a string and append it to the generated sentence.\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "    # Exit condition: either hit max length or sample a stop character\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "GP4YWv07DkXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "M-zD2DzwDyhs",
        "outputId": "a0143ba0-1d86-48bf-daec-ad765cd0fa4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "How big you are!\n",
            "[start]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Why don't you leave, Tom?\n",
            "[start] no te por tom[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom wants to stay here.\n",
            "[start] tom quiere aquí[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We love you.\n",
            "[start]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "There were a lot of boats on the lake.\n",
            "[start] muchos en el mary del error[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Let's play tennis in the afternoon.\n",
            "[start] al tenis por la tarde[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "This is the worst of all.\n",
            "[start] es el mejor de la para nada[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I'll be in trouble if the story gets out.\n",
            "[start] en problemas si la historia se [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I jumped up and down.\n",
            "[start] y [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I need you here in case something else happens.\n",
            "[start] que estás aquí en otra [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom winked.\n",
            "[start] un [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "She sent her children off to school.\n",
            "[start] a sus niños a la escuela[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Here's some water.\n",
            "[start] algo de agua[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "That short woman over there is my mother.\n",
            "[start] de allí es [UNK] de mi madre[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "My father has lived in Nagoya for 30 years.\n",
            "[start] ha padre ha [UNK] en hace tres años[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom's response was immediate.\n",
            "[start] la fue de tom fue de gran no al muy [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "When does the performance begin?\n",
            "[start] la [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We're out of time.\n",
            "[start] de tiempo[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We went to the beach.\n",
            "[start] a la playa[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I wrote a letter last night.\n",
            "[start] una carta anoche[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this inference setup, while very simple, is rather inefficient, since we reprocess\n",
        "the entire source sentence and the entire generated target sentence every time\n",
        "we sample a new word.\n",
        "\n",
        "In a practical application, you’d factor the encoder and the\n",
        "decoder as two separate models, and your decoder would only run a single step at\n",
        "each token-sampling iteration, reusing its previous internal state.\n",
        "\n",
        "There are many ways this toy model could be improved: \n",
        "\n",
        "* We could use a deep stack of\n",
        "recurrent layers for both the encoder and the decoder (note that for the decoder, this makes state management a bit more involved). \n",
        "* We could use an LSTM instead of a GRU. And so on. \n",
        "\n",
        "Beyond such tweaks, however, the RNN approach to sequence-to-sequence\n",
        "learning has a few fundamental limitations:\n",
        "\n",
        "* The source sequence representation has to be held entirely in the encoder state\n",
        "vector(s), which puts significant limitations on the size and complexity of the\n",
        "sentences you can translate. It’s a bit as if a human were translating a sentence\n",
        "entirely from memory, without looking twice at the source sentence while producing\n",
        "the translation.\n",
        "\n",
        "* RNNs have trouble dealing with very long sequences, since they tend to progressively\n",
        "forget about the past—by the time you’ve reached the 100th token in\n",
        "either sequence, little information remains about the start of the sequence.That means RNN-based models can’t hold onto long-term context, which can\n",
        "be essential for translating long documents.\n",
        "\n",
        "These limitations are what has led the machine learning community to embrace the\n",
        "Transformer architecture for sequence-to-sequence problems."
      ],
      "metadata": {
        "id": "508No_6uEMBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-to-sequence learning with Transformer"
      ],
      "metadata": {
        "id": "EGRK2tdiFJWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence-to-sequence learning is the task where Transformer really shines. Neural\n",
        "attention enables Transformer models to successfully process sequences that are con\n",
        "siderably\n",
        "longer and more complex than those RNNs can handle.\n",
        "\n",
        "Look at the decoder\n",
        "internals: you’ll recognize that it looks very similar to the Transformer encoder, except\n",
        "that an extra attention block is inserted between the self-attention block applied to\n",
        "the target sequence and the dense layers of the exit block.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/5.png?raw=1' width='600'/>\n",
        "\n",
        "\n",
        "Let’s implement it. Like for the TransformerEncoder, we’ll use a Layer subclass."
      ],
      "metadata": {
        "id": "4XZJdFnQFLUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "\n",
        "      self.multi_head_attention_layer_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.multi_head_attention_layer_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "      \n",
        "      self.dense_projection = keras.Sequential([\n",
        "           layers.Dense(dense_dim, activation=\"relu\"),\n",
        "           layers.Dense(embed_dim)                                     \n",
        "      ])\n",
        "\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "      self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "      # This attribute ensures that the layer will propagate its input mask to its outputs\n",
        "      self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          \"embed_dim\": self.embed_dim,\n",
        "          \"num_heads\": self.num_heads,\n",
        "          \"dense_dim\": self.dense_dim\n",
        "      })\n",
        "      return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    \"\"\"\n",
        "    Causal padding is absolutely critical to successfully training a sequence-to-sequence Transformer.\n",
        "    we’ll mask the upper half of the pairwise attention matrix to prevent the model from paying any attention \n",
        "    to information from the future only information from tokens 0...N in the target sequence should be used \n",
        "    when generating target token N+1.\n",
        "    \"\"\"\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    # Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    # Replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length)\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    "
      ],
      "metadata": {
        "id": "72mYZvEnihqh"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}