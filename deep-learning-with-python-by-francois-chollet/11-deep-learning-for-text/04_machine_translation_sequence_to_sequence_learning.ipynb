{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-machine-translation--sequence-to-sequence-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+yFzm9yEM6X0YZyRjLbKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/04_machine_translation_sequence_to_sequence_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Machine translation: A sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "Pka5_bIxkQdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you’ll deepen your expertise by learning about\n",
        "sequence-to-sequence models.\n",
        "\n",
        "A sequence-to-sequence model takes a sequence as input (often a sentence or\n",
        "paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n",
        "- **Machine translation**—Convert a paragraph in a source language to its equivalent in a target language.\n",
        "- **Text summarization**—Convert a long document to a shorter version that retains the most important information.\n",
        "- **Question answering**—Convert an input question into its answer.\n",
        "- **Chatbots**—Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
        "- **Text generation**—Convert a text prompt into a paragraph that completes the prompt.\n",
        "\n",
        "The general template behind sequence-to-sequence models is described in figure.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "During training:-\n",
        "- An `encoder` model turns the source sequence into an intermediate representation.\n",
        "- A `decoder` is trained to predict the next token i in the target sequence by looking at both previous tokens `(0 to i - 1)` and the encoded source sequence.\n",
        "\n",
        "**During inference, we don’t have access to the target sequence**—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
        "\n",
        "- We obtain the encoded source sequence from the encoder.\n",
        "- The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string `[start]`), and uses them to predict the first real token in the sequence.\n",
        "- The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string\n",
        "`[end]`).\n",
        "\n",
        "Everything you’ve learned so far can be repurposed to build this new kind of model.\n",
        "\n",
        "Let’s dive in.\n"
      ],
      "metadata": {
        "id": "BHePHVWuk-6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "KkxNMWG1mnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XUDQfXo9mpJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll be working with an English-to-Spanish translation dataset available at\n",
        "www.manythings.org/anki/. \n",
        "\n",
        "Let’s download it:"
      ],
      "metadata": {
        "id": "NGB_nEcMnipW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "MjRO78CmnnH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "DeMzuQbmo4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. \n",
        "\n",
        "Let’s parse this file."
      ],
      "metadata": {
        "id": "qWjNE2oLo7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "  # Each line contains an English phrase and its Spanish translation, tab-separated.\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  # We prepend \"[start]\" and append \"[end]\" to the Spanish sentence, to match the template\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "QcKV-E2rpKfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `text_pairs` look like this:"
      ],
      "metadata": {
        "id": "6PYcBUf5rBzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ts-AXBMzrEeq",
        "outputId": "a758241b-910f-4c6b-c9b0-11baa2bcabd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I helped my mother with the cooking.', '[start]Ayudé a cocinar a mi mamá.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ZKHJ29-JrVaI",
        "outputId": "9e8c5a34-f5d0-43b6-e718-cc7b554d3d96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Tom saw it.', '[start]Tom lo vio.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s shuffle them and split them into the usual training, validation, and test sets:"
      ],
      "metadata": {
        "id": "TgG4WIZ1rTIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "_xCJvrN7rT0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s prepare two separate TextVectorization layers: one for English and one for Spanish. \n",
        "\n",
        "We’re going to need to customize the way strings are preprocessed:"
      ],
      "metadata": {
        "id": "ZScoIPTZuKY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a custom string standardization function for the Spanish TextVectorization layer: it preserves [ and ] \n",
        "# but strips ¿ (as well as all other characters from strings.punctuation).\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "# Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.\n",
        "target_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", \n",
        "                                                output_sequence_length=sequence_length + 1,\n",
        "                                                standardize=custom_standardization)\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# Learn the vocabulary of each language\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "Wh7IlyfVuNSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can turn our data into a tf.data pipeline. \n",
        "\n",
        "We want it to return a tuple `(inputs, target)` where `inputs` is a dict with two keys,`encoder_inputs` (the English sentence) and `decoder_inputs` (the Spanish sentence), and `target` is the Spanish sentence offset by one step ahead."
      ],
      "metadata": {
        "id": "flQpu1pcwxpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return (\n",
        "      {\n",
        "      \"english\": eng,\n",
        "      \"spanish\": spa[:, :-1],  # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length\n",
        "      },\n",
        "      spa[:, 1:]   # The target Spanish sentence is one step ahead. Both are still the same length (20 words)\n",
        "  )"
      ],
      "metadata": {
        "id": "lRd8xb_uw68P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  # Use in-memory caching to speed up preprocessing\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "metadata": {
        "id": "mQNGKIVKzAF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "FatVJpipzx8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what our dataset outputs look like:"
      ],
      "metadata": {
        "id": "B_Bd8zTL0EpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "TkPwTeeH0Gnu",
        "outputId": "c4e1afff-240d-4cee-f848-bbe85bb3271f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now ready—time to build some models. We’ll start with a recurrent\n",
        "sequence-to-sequence model before moving on to a Transformer."
      ],
      "metadata": {
        "id": "3sz5nJvQ0sBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "wc2FZshD0spZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "64zlIq2T0wkm"
      }
    }
  ]
}