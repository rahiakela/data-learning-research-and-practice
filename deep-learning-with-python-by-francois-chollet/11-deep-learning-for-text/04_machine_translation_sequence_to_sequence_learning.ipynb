{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-machine-translation--sequence-to-sequence-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNjengWbq6vifxU5udPkRaU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/04_machine_translation_sequence_to_sequence_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Machine translation: sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "Pka5_bIxkQdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you’ll deepen your expertise by learning about\n",
        "sequence-to-sequence models.\n",
        "\n",
        "A sequence-to-sequence model takes a sequence as input (often a sentence or\n",
        "paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n",
        "- **Machine translation**—Convert a paragraph in a source language to its equivalent in a target language.\n",
        "- **Text summarization**—Convert a long document to a shorter version that retains the most important information.\n",
        "- **Question answering**—Convert an input question into its answer.\n",
        "- **Chatbots**—Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
        "- **Text generation**—Convert a text prompt into a paragraph that completes the prompt.\n",
        "\n",
        "The general template behind sequence-to-sequence models is described in figure.\n",
        "\n",
        "<img src='https://github.com/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "During training:-\n",
        "- An `encoder` model turns the source sequence into an intermediate representation.\n",
        "- A `decoder` is trained to predict the next token i in the target sequence by looking at both previous tokens `(0 to i - 1)` and the encoded source sequence.\n",
        "\n",
        "**During inference, we don’t have access to the target sequence**—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
        "\n",
        "- We obtain the encoded source sequence from the encoder.\n",
        "- The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string `[start]`), and uses them to predict the first real token in the sequence.\n",
        "- The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string\n",
        "`[end]`).\n",
        "\n",
        "Everything you’ve learned so far can be repurposed to build this new kind of model.\n",
        "\n",
        "Let’s dive in.\n"
      ],
      "metadata": {
        "id": "BHePHVWuk-6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "KkxNMWG1mnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XUDQfXo9mpJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll be working with an English-to-Spanish translation dataset available at\n",
        "www.manythings.org/anki/. \n",
        "\n",
        "Let’s download it:"
      ],
      "metadata": {
        "id": "NGB_nEcMnipW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "MjRO78CmnnH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "DeMzuQbmo4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. \n",
        "\n",
        "Let’s parse this file."
      ],
      "metadata": {
        "id": "qWjNE2oLo7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QcKV-E2rpKfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}