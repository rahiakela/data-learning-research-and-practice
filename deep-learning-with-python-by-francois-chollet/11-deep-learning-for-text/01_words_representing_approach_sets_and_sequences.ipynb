{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-words-representing-approach--sets-and-sequences.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvTgRxxN6SBQCSF+Cm6w3w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/01_words_representing_approach_sets_and_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words representing approache: Sets and sequences"
      ],
      "metadata": {
        "id": "bVyFMTsGCS2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A much more problematic question,\n",
        "however, is how to encode the way words are woven into sentences: word order.\n",
        "\n",
        "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don’t have a natural, canonical order.\n",
        "\n",
        "The simplest thing you could do is just discard order and\n",
        "treat text as an unordered set of words—this gives you **bag-of-words models**.\n",
        "\n",
        "You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the **recurrent models**.\n",
        "\n",
        "Finally, a hybrid approach is also possible: the **Transformer architecture** is technically order-agnostic, yet it injects word-position information into\n",
        "the representations it processes, which enables it to simultaneously look at different parts of a sentence, while still being order-aware. \n",
        "\n",
        "Because they take into account word order, **both RNNs and Transformers are called sequence models.**\n",
        "\n",
        "We’ll demonstrate each approach on a well-known text classification benchmark:\n",
        "the IMDB movie review sentiment-classification dataset.\n",
        "\n",
        "Let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwj5k3eqCizD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "8av_x5IrLvDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import os, pathlib, shutil, random"
      ],
      "metadata": {
        "id": "qqyixaVcLwJ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by downloading the dataset from the Stanford page and uncompressing it:"
      ],
      "metadata": {
        "id": "uD5ryFCIMTIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "# delete unwanted file and subdirectory\n",
        "rm -rf aclImdb/train/unsup\n",
        "rm -rf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "qikpFafHMZRp",
        "outputId": "017712f7-b950-4585-c4fd-a759e72f2d86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  16.7M      0  0:00:04  0:00:04 --:--:-- 17.9M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the content of a few of these text files\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "1P--z8OhM6ub",
        "outputId": "d0335ec0-4b80-4d03-f7a1-8872bb9eb08d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let’s download the GloVe word embeddings."
      ],
      "metadata": {
        "id": "u3jPbA4vb8Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "mik4B6eFb9d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the IMDB movie reviews data"
      ],
      "metadata": {
        "id": "KFSx_y3EN9zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, the `train/pos/` directory contains a set of `12,500` text files, each of which\n",
        "contains the text body of a positive-sentiment movie review to be used as training data.\n",
        "The negative-sentiment reviews live in the “neg” directories. \n",
        "\n",
        "In total, there are `25,000`\n",
        "text files for training and another 25,000 for testing.\n",
        "\n",
        "Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`:"
      ],
      "metadata": {
        "id": "qlF10e0JN-gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir/\"val\"\n",
        "train_dir = base_dir/\"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir/category)\n",
        "  files = os.listdir(train_dir/category)\n",
        "  # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code\n",
        "  random.Random(1337).shuffle(files)\n",
        "  # Take 20% of the training files to use for validation\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
        "    shutil.move(train_dir/category/fname, val_dir/category/fname)"
      ],
      "metadata": {
        "id": "YMuEAvy2OQRY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember how, we used the `image_dataset_from_directory` utility to\n",
        "create a batched Dataset of images and their labels for a directory structure? You can do the exact same thing for text files using the `text_dataset_from_directory` utility.\n",
        "\n",
        "Let’s create three Dataset objects for training, validation, and testing:"
      ],
      "metadata": {
        "id": "TIAswQR1SUUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "V6vA9z3ASbd3",
        "outputId": "743d7443-98b0-441b-ac6e-a471f7b4865a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”"
      ],
      "metadata": {
        "id": "lMPppHlCTFC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "nwyYSSmCTHml",
        "outputId": "74a047b7-be19-4b61-dba9-ff0b153d7dbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"Not an altogether bad start for the program -- but what a slap in the face to real law enforcement. The worst part of the series is that it attempts to bill itself as reality fare -- and is anything but. Men and women that dedicate their lives to the enforcement of laws deserve better than this. What is next, medical school in a minute? Charo performing lipo? Charles Grodin assisting on a hip replacement? C'mon...show a little respect. Even the citizens of Muncie are outing the program as staged. Police Academy = High School Gym? Poor editing (how many times can they use the car-to-car shot of the Taco Bell in the background?), cheesy siren effects (the same loop added ad nauseum to every 'call' whether rolling code or not), and last, but not least -- more officer safety issues than you could shake a stick at.<br /><br />If I want to see manufactured police work and wise-ass fake cops, I would watch RENO 911.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All set. Now let’s try learning something from this data."
      ],
      "metadata": {
        "id": "zCfTCoZSTro0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing words as a set: The bag-of-words approach"
      ],
      "metadata": {
        "id": "kyo9giBDTsIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to encode a piece of text for processing by a machine learning\n",
        "model is to discard order and treat it as a set (a “bag”) of tokens. \n",
        "\n",
        "You could either look at individual words (unigrams), or try to recover some local order information by looking at groups of consecutive token (N-grams)."
      ],
      "metadata": {
        "id": "lY-0GpLxTv8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Single words with binary encoding"
      ],
      "metadata": {
        "id": "_3pyKsnQS-wX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use a bag of single words, the sentence “the cat sat on the mat” becomes-\n",
        "\n",
        "```python\n",
        "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n",
        "```\n",
        "\n",
        "The main advantage of this encoding is that you can represent an entire text as a single\n",
        "vector, where each entry is a presence indicator for a given word. \n",
        "\n",
        "For instance,\n",
        "using binary encoding (multi-hot), you’d encode a text as a vector with as many\n",
        "dimensions as there are words in your vocabulary—with 0s almost everywhere and\n",
        "some 1s for dimensions that encode words present in the text.\n",
        "\n",
        "First, let’s process our raw text datasets with a TextVectorization layer so that they yield multi-hot encoded binary word vectors."
      ],
      "metadata": {
        "id": "UVs_HWi8TCEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the vocabulary to the 20,000 most frequent words.In general, 20,000 is the right vocabulary size for text classification.\n",
        "text_vectorization = layers.TextVectorization(max_tokens=20000, output_mode=\"multi_hot\") # Encode the output tokens as multi-hot binary vectors\n",
        "\n",
        "# Prepare a dataset that only yields raw text inputs (no labels).\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "u72_BiqgTjIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare processed versions of our training, validation, and test dataset.\n",
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "3a29FT2wUsMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try to inspect the output of one of these datasets."
      ],
      "metadata": {
        "id": "eI1_gNfuVOst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "JwH8YzizVPLE",
        "outputId": "24f13cae-2eb2-4e96-ca0b-9227899fd238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s write a reusable model-building function that we’ll use in all of our experiments."
      ],
      "metadata": {
        "id": "DZAZyiSdWgGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens, ))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "CTrGV_KEWjKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let’s train and test our model."
      ],
      "metadata": {
        "id": "Ghqvx59tWgzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1TCKuhVzXfBK",
        "outputId": "a3179a3d-e10a-4ec8-a3c0-071d9097012e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)]"
      ],
      "metadata": {
        "id": "sxWAQKTrXlYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call `cache()` on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory."
      ],
      "metadata": {
        "id": "j-3ER_JvX4Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "id": "N78xCH3bX9wj",
        "outputId": "47474cc9-7ca5-49fa-bdfe-b183990a31ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 11s 17ms/step - loss: 0.4184 - accuracy: 0.8244 - val_loss: 0.3023 - val_accuracy: 0.8854\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2846 - accuracy: 0.8974 - val_loss: 0.2879 - val_accuracy: 0.8882\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2563 - accuracy: 0.9118 - val_loss: 0.3041 - val_accuracy: 0.8836\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2457 - accuracy: 0.9158 - val_loss: 0.3158 - val_accuracy: 0.8884\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2378 - accuracy: 0.9172 - val_loss: 0.3307 - val_accuracy: 0.8836\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2355 - accuracy: 0.9215 - val_loss: 0.3406 - val_accuracy: 0.8860\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2320 - accuracy: 0.9239 - val_loss: 0.3551 - val_accuracy: 0.8838\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2144 - accuracy: 0.9297 - val_loss: 0.3735 - val_accuracy: 0.8848\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2163 - accuracy: 0.9319 - val_loss: 0.3773 - val_accuracy: 0.8794\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2150 - accuracy: 0.9347 - val_loss: 0.3783 - val_accuracy: 0.8810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbefc443c90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "WcyU9pgcYVuU",
        "outputId": "cb36ebb9-5beb-41bf-da3f-ef13a14bde0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 8s 10ms/step - loss: 0.2874 - accuracy: 0.8876\n",
            "Test acc: 0.888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gets us to a test accuracy of 88.8%: not bad! Note that in this case, since the dataset\n",
        "is a balanced two-class classification dataset (there are as many positive samples as\n",
        "negative samples), the “naive baseline” we could reach without training an actual model\n",
        "would only be 50%. \n",
        "\n",
        "Meanwhile, the best score that can be achieved on this dataset\n",
        "without leveraging external data is around 95% test accuracy."
      ],
      "metadata": {
        "id": "IWLplN1yYoRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bigrams with binary encoding"
      ],
      "metadata": {
        "id": "pglJ08BIYx2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, discarding word order is very reductive, because even atomic concepts can\n",
        "be expressed via multiple words: the term `United States` conveys a concept that is\n",
        "quite distinct from the meaning of the words `states` and `united` taken separately.\n",
        "\n",
        "For this reason, you will usually end up re-injecting local order information into your\n",
        "bag-of-words representation by looking at N-grams rather than single words (most\n",
        "commonly, bigrams).\n",
        "\n",
        "With bigrams, our sentence becomes:\n",
        "\n",
        "```python\n",
        "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
        "```\n",
        "\n",
        "The TextVectorization layer can be configured to return arbitrary N-grams: bigrams,\n",
        "trigrams, etc. Just pass an `ngrams=N` argument as in the following listing."
      ],
      "metadata": {
        "id": "-nbJNwsuY166"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the vocabulary to the 20,000 most frequent words.In general, 20,000 is the right vocabulary size for text classification.\n",
        "text_vectorization = layers.TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"multi_hot\") # Encode the output tokens as multi-hot binary vectors\n",
        "\n",
        "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "40n99NGIWHlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare processed versions of our training, validation, and test dataset.\n",
        "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "OQWSvXRFWdfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = get_model()\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "qmXeFKn1Wp0Z",
        "outputId": "ed2fd1aa-f546-4dfe-d166-a522b63149b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)]\n",
        "\n",
        "model2.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "id": "v8ytN348WysC",
        "outputId": "fb504a99-611c-45b8-b2ba-5d1c7c397feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 12s 17ms/step - loss: 0.3722 - accuracy: 0.8447 - val_loss: 0.2814 - val_accuracy: 0.8944\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2436 - accuracy: 0.9141 - val_loss: 0.2837 - val_accuracy: 0.8962\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2132 - accuracy: 0.9306 - val_loss: 0.3032 - val_accuracy: 0.8948\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2114 - accuracy: 0.9356 - val_loss: 0.3177 - val_accuracy: 0.8942\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1958 - accuracy: 0.9431 - val_loss: 0.3396 - val_accuracy: 0.8880\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1942 - accuracy: 0.9453 - val_loss: 0.3556 - val_accuracy: 0.8920\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1779 - accuracy: 0.9471 - val_loss: 0.3602 - val_accuracy: 0.8888\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1909 - accuracy: 0.9464 - val_loss: 0.3674 - val_accuracy: 0.8848\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1831 - accuracy: 0.9513 - val_loss: 0.3730 - val_accuracy: 0.8866\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1778 - accuracy: 0.9475 - val_loss: 0.3921 - val_accuracy: 0.8878\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbefa128890>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "fT5W_JDRWzQv",
        "outputId": "f69f066a-c0d6-4ab6-fb30-87ac200e02fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 8s 10ms/step - loss: 0.2760 - accuracy: 0.8966\n",
            "Test acc: 0.897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re now getting 89.5% test accuracy, a marked improvement! Turns out local order is pretty important."
      ],
      "metadata": {
        "id": "FDIk63_6XSf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bigrams with TF-IDF encoding"
      ],
      "metadata": {
        "id": "DY9FktUfXoTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also add a bit more information to this representation by counting how many\n",
        "times each word or N-gram occurs, that is to say, by taking the histogram of the words\n",
        "over the text:\n",
        "\n",
        "```python\n",
        "{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
        "\"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n",
        "```\n",
        "\n",
        "If you’re doing text classification, knowing how many times a word occurs in a sample\n",
        "is critical: any sufficiently long movie review may contain the word `terrible` regardless\n",
        "of sentiment, but a review that contains many instances of the word `terrible` is\n",
        "likely a negative one.\n",
        "\n",
        "Here’s how you’d count bigram occurrences with the TextVectorization layer.\n",
        "\n",
        "```python\n",
        "text_vectorization = TextVectorization(\n",
        "ngrams=2,\n",
        "max_tokens=20000,\n",
        "output_mode=\"count\"\n",
        ")\n",
        "```\n",
        "\n",
        "Now, of course, some words are bound to occur more often than others no matter\n",
        "what the text is about. The words `the,` `a,` `is,` and `are` will always dominate your\n",
        "word count histograms, drowning out other words—despite being pretty much useless\n",
        "features in a classification context. How could we address this?\n",
        "\n",
        "The best practice is to go with something called **TF-IDF normalization—TF-IDF** stands for `term frequency,\n",
        "inverse document frequency.`\n",
        "\n",
        "TF-IDF is so common that it’s built into the TextVectorization layer. All you need to do to start using it is to switch the output_mode argument to `tf_idf`."
      ],
      "metadata": {
        "id": "Mau-yTqyYId9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the vocabulary to the 20,000 most frequent words.In general, 20,000 is the right vocabulary size for text classification.\n",
        "text_vectorization = layers.TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"tf_idf\") # Encode the output tokens as tf-idf binary vectors\n",
        "\n",
        "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Prepare processed versions of our training, validation, and test dataset.\n",
        "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "model3 = get_model()\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "syj-ag-tXR2e",
        "outputId": "50dd788f-91aa-4543-e9cd-152edce82fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)]\n",
        "\n",
        "model3.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "id": "ERG3BJBObdJv",
        "outputId": "5b3ea6a9-9873-4a64-9746-0f8df2df8087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.4843 - accuracy: 0.7954 - val_loss: 0.2975 - val_accuracy: 0.8934\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3128 - accuracy: 0.8729 - val_loss: 0.3250 - val_accuracy: 0.8928\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2815 - accuracy: 0.8868 - val_loss: 0.3327 - val_accuracy: 0.8912\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.2590 - accuracy: 0.8946 - val_loss: 0.3439 - val_accuracy: 0.8830\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2425 - accuracy: 0.8985 - val_loss: 0.3750 - val_accuracy: 0.8878\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2359 - accuracy: 0.9003 - val_loss: 0.3813 - val_accuracy: 0.8774\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2325 - accuracy: 0.9030 - val_loss: 0.3670 - val_accuracy: 0.8762\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2300 - accuracy: 0.9011 - val_loss: 0.3530 - val_accuracy: 0.8812\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2181 - accuracy: 0.9021 - val_loss: 0.3740 - val_accuracy: 0.8778\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2129 - accuracy: 0.9054 - val_loss: 0.3959 - val_accuracy: 0.8728\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbef9f70910>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "IbKWeRxpbdp-",
        "outputId": "96580963-9cb5-42ad-c62d-a4e74e437915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2989 - accuracy: 0.8894\n",
            "Test acc: 0.889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gets us an `88.9%` test accuracy on the IMDB classification task: it doesn’t seem to\n",
        "be particularly helpful in this case. However, for many text-classification datasets, it\n",
        "would be typical to see a one-percentage-point increase when using `TF-IDF` compared\n",
        "to plain binary encoding.\n",
        "\n",
        "**Exporting a model that processes raw strings**\n",
        "\n",
        "Just create a new model that reuses your TextVectorization layer and adds to it\n",
        "the model you just trained:"
      ],
      "metadata": {
        "id": "GjJpKDi3bvHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One input sample would be one string\n",
        "inputs = keras.Input(shape=(1, ), dtype=\"string\")\n",
        "# Apply text preprocessing\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "# Apply the previously trained model\n",
        "outputs = model(processed_inputs)\n",
        "# Instantiate the end-to-end model\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "je-t-x9PcZaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting model can process batches of raw strings:"
      ],
      "metadata": {
        "id": "LkrNcDHec9nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_data = tf.convert_to_tensor([[\"That was an excellent movie, I loved it.\"]])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "id": "njsyMpElc7qv",
        "outputId": "6bf409fb-2ecc-4922-b96a-306ff46bbbec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.86 percent positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing words as a sequence: The sequence model approach"
      ],
      "metadata": {
        "id": "ceDW_Q5ndkMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know that word order matters: **manual engineering of order-based features, such as bigrams, yields a nice accuracy boost**. \n",
        "\n",
        "Now remember: **the history of deep learning is that of a move away from manual feature engineering, toward letting models learn their own features from exposure to data alone.**\n",
        "\n",
        "What if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? \n",
        "\n",
        "**This is what sequence models are about.**\n",
        "\n",
        "To implement a sequence model, you’d start by representing your input samples as\n",
        "sequences of integer indices (one integer standing for one word). Then, you’d map each integer to a vector to obtain vector sequences. \n",
        "\n",
        "Finally, you’d feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent\n",
        "vectors, such as a 1D convnet, a RNN, or a Transformer.\n",
        "\n",
        "For some time around 2016–2017, bidirectional RNNs (in particular, bidirectional\n",
        "LSTMs) were considered to be the state of the art for sequence modeling.\n",
        "\n",
        "However, nowadays sequence modeling is almost universally done with Transformers.\n",
        "\n",
        "Oddly, one-dimensional convnets were never very popular in NLP, even though, a residual stack of depthwise-separable 1D convolutions can often achieve comparable performance to a bidirectional\n",
        "LSTM, at a greatly reduced computational cost."
      ],
      "metadata": {
        "id": "LIwdLUjNdlQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preparing datasets for sequences"
      ],
      "metadata": {
        "id": "s4JUN4qkQY9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let’s prepare datasets that return integer sequences."
      ],
      "metadata": {
        "id": "h7T3QwANQfoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "# Prepare a dataset that only yields raw text inputs (no labels).\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words.\n",
        "text_vectorization = layers.TextVectorization(max_tokens=max_tokens,\n",
        "                                              output_mode=\"int\",\n",
        "                                              output_sequence_length=max_length)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "eKQRxZI9Qmk2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A simple bidirectional LSTM"
      ],
      "metadata": {
        "id": "4S4HkAW2aue8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to convert our integer sequences to vector\n",
        "sequences is to one-hot encode the integers (each dimension would represent one\n",
        "possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple bidirectional LSTM."
      ],
      "metadata": {
        "id": "EFPpHO9zR6_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One input is a sequence of integers\n",
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "# Encode the integers into binary 20,000-dimensional vectors\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "# add a bidirectional LSTM\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "prcYoNX_R8gz",
        "outputId": "312bbd77-af7e-49a9-b0e3-86f738133987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s train our model."
      ],
      "metadata": {
        "id": "BWjx88yOUA_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.model.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "s-tQ3kFlUBjO",
        "outputId": "6c9d3ac1-7489-4398-f482-37cdf4351329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 46/625 [=>............................] - ETA: 3:32:57 - loss: 0.6931 - accuracy: 0.5088"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A first observation: this model trains very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size `(600, 20000) (600 words per sample, 20,000 possible words)`. That’s `12,000,000` floats for a single movie review. Our bidirectional LSTM has a lot of work to do.\n",
        "\n",
        "Second, the model only gets to `87%` test accuracy—it doesn’t perform nearly as well as our (very fast) binary unigram model.\n",
        "\n",
        "Clearly, using one-hot encoding to turn words into vectors, which was the simplest thing we could do, wasn’t a great idea. There’s a better way: **word embeddings**."
      ],
      "metadata": {
        "id": "U-hyjFg1XdEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word embeddings"
      ],
      "metadata": {
        "id": "eWCQTO1UDlRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embeddings are vector representations of words that map human language into a structured geometric space.\n",
        "\n",
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "- Learn word embeddings jointly with the main task using embeddings layer\n",
        "- Load pretrained word embeddings into your model"
      ],
      "metadata": {
        "id": "7EoCwzVsD_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Learn word embeddings with embedding layer"
      ],
      "metadata": {
        "id": "DkjHH29gEndf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It’s thus reasonable to learn a new embedding space with every new task. Fortunately,\n",
        "backpropagation makes this easy, and Keras makes it even easier. It’s about\n",
        "learning the weights of a layer: the Embedding layer."
      ],
      "metadata": {
        "id": "vahJgczVc-L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One input is a sequence of integers\n",
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "# The Embedding layer takes at least two arguments: the number of possible tokens and the dimensionality of the embeddings (here, 256).\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "# add a bidirectional LSTM\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nLtl5qpwDvor",
        "outputId": "d435a2f4-3ceb-46a3-bed5-231daca8cecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               73984     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\", save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.model.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "e9Az8iMbdk5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It trains much faster than the one-hot model (since the LSTM only has to process\n",
        "256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable\n",
        "(87%)."
      ],
      "metadata": {
        "id": "Ni3AokZvd2ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Understading padding and masking"
      ],
      "metadata": {
        "id": "mU2yaDRyd3E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need some way to tell the RNN that it should skip the iterations for padded value(mostly zeros). There’s an API for that: masking.\n",
        "\n",
        "The Embedding layer is capable of generating a “mask” that corresponds to its\n",
        "input data. This mask is a tensor of ones and zeros (or True/False booleans), of shape\n",
        "(`batch_size`, `sequence_length`), where the entry `mask[i, t]` indicates where timestep\n",
        "t of sample i should be skipped or not (the timestep will be skipped if `mask[i, t]`\n",
        "is 0 or False, and processed otherwise).\n",
        "\n",
        "By default, this option isn’t active—you can turn it on by passing mask_zero=True\n",
        "to your Embedding layer. You can retrieve the mask with the compute_mask() method:"
      ],
      "metadata": {
        "id": "Yrk--UfIeG3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)"
      ],
      "metadata": {
        "id": "RW3kzSybffDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_input = [\n",
        "   [4, 3, 2, 1, 0, 0, 0],\n",
        "   [5, 4, 3, 2, 1, 0, 0],\n",
        "   [2, 1, 0, 0, 0, 0, 0]           \n",
        "]"
      ],
      "metadata": {
        "id": "jZlH-b8chsPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = embedding_layer.compute_mask(some_input)\n",
        "mask"
      ],
      "metadata": {
        "id": "-268znIUh0nY",
        "outputId": "cc4a1085-98ab-4d12-ad77-1ff5c0e3c2e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
              "array([[ True,  True,  True,  True, False, False, False],\n",
              "       [ True,  True,  True,  True,  True, False, False],\n",
              "       [ True,  True, False, False, False, False, False]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, you will almost never have to manage masks by hand. Instead, Keras will\n",
        "automatically pass on the mask to every layer that is able to process it (as a piece of\n",
        "metadata attached to the sequence it represents). \n",
        "\n",
        "This mask will be used by RNN layers\n",
        "to skip masked steps. If your model returns an entire sequence, the mask will also\n",
        "be used by the loss function to skip masked steps in the output sequence."
      ],
      "metadata": {
        "id": "tonR0N5BiUQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One input is a sequence of integers\n",
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "# add a bidirectional LSTM\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mrxAX9XviU5s",
        "outputId": "8dedc6af-b7e1-4dc5-9183-161d6975aa04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\", save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.model.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "JbS_FVcfinWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we get to 88% test accuracy—a small but noticeable improvement."
      ],
      "metadata": {
        "id": "pgSObbNLitu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using pretrained word-embeddings"
      ],
      "metadata": {
        "id": "0cLL97BgiuKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes you have so little training data available that you can’t use your data alone\n",
        "to learn an appropriate task-specific embedding of your vocabulary. \n",
        "\n",
        "In such cases,\n",
        "instead of learning word embeddings jointly with the problem you want to solve, you\n",
        "can load embedding vectors from a precomputed embedding space that you know is\n",
        "highly structured and exhibits useful properties—one that captures generic aspects of\n",
        "language structure.\n",
        "\n",
        "Let’s use GloVe word embedding and parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation."
      ],
      "metadata": {
        "id": "yzgkRUEpi2lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "id": "BmcM3Y12crZD",
        "outputId": "bfa569b5-9f21-4087-9147-cb12c9a9f18f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s build an embedding matrix that you can load into an Embedding layer. \n",
        "\n",
        "It must be a matrix of shape `(max_words, embedding_dim)`, where each entry $i$ contains the `embedding_dim` - dimensional vector for the word of index $i$ in the reference word index (built during tokenization)."
      ],
      "metadata": {
        "id": "F020b46bd13J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# Retrieve the vocabulary indexed by our previous TextVectorization layer\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "# Use it to create a mapping from words to their index in the vocabulary.\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))"
      ],
      "metadata": {
        "id": "XcmggiT3eAz0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a matrix that we’ll fill with the GloVe vectors.\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  # Fill entry i in the matrix with the word vector for index i. Words not found in the embedding index will be all zeros.\n",
        "  if i < max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "TCd6xDx4ee3S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use a Constant initializer to load the pretrained embeddings in an Embedding layer. \n",
        "\n",
        "So as not to disrupt the pretrained representations during training, we freeze\n",
        "the layer via `trainable=False`:"
      ],
      "metadata": {
        "id": "CzZAoDmQgQal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(max_tokens, embedding_dim, \n",
        "                                   embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                                   trainable=False, mask_zero=True)"
      ],
      "metadata": {
        "id": "4k1vNrkEgXRR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re now ready to train a new model—identical to our previous model, but leveraging the `100-dimensional` pretrained GloVe embeddings instead of `128-dimensional` learned embeddings."
      ],
      "metadata": {
        "id": "mHRV6Tfkg1_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One input is a sequence of integers\n",
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "# add a bidirectional LSTM\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hxcMnAkug6RX",
        "outputId": "b31d96e4-857a-42fb-85c7-2558ae3026d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               34048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n",
        "\n",
        "model = keras.model.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "T-ogl15Qg_y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll find that on this particular task, pretrained embeddings aren’t very helpful, because the dataset contains enough samples that it is possible to learn a specialized enough embedding space from scratch. \n",
        "\n",
        "However, leveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset."
      ],
      "metadata": {
        "id": "t9fTBTPehn3v"
      }
    }
  ]
}