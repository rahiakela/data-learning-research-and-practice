{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-words-representing-approach--sets-and-sequences.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZ+D1lW/gqKiWoCJtPsin6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/01_words_representing_approach_sets_and_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words representing approache: Sets and sequences"
      ],
      "metadata": {
        "id": "bVyFMTsGCS2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A much more problematic question,\n",
        "however, is how to encode the way words are woven into sentences: word order.\n",
        "\n",
        "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don’t have a natural, canonical order.\n",
        "\n",
        "The simplest thing you could do is just discard order and\n",
        "treat text as an unordered set of words—this gives you **bag-of-words models**.\n",
        "\n",
        "You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the **recurrent models**.\n",
        "\n",
        "Finally, a hybrid approach is also possible: the **Transformer architecture** is technically order-agnostic, yet it injects word-position information into\n",
        "the representations it processes, which enables it to simultaneously look at different parts of a sentence, while still being order-aware. \n",
        "\n",
        "Because they take into account word order, **both RNNs and Transformers are called sequence models.**\n",
        "\n",
        "We’ll demonstrate each approach on a well-known text classification benchmark:\n",
        "the IMDB movie review sentiment-classification dataset.\n",
        "\n",
        "Let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwj5k3eqCizD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "8av_x5IrLvDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import os, pathlib, shutil, random"
      ],
      "metadata": {
        "id": "qqyixaVcLwJ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by downloading the dataset from the Stanford page and uncompressing it:"
      ],
      "metadata": {
        "id": "uD5ryFCIMTIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "# delete unwanted file and subdirectory\n",
        "rm -rf aclImdb/train/unsup\n",
        "rm -rf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "qikpFafHMZRp",
        "outputId": "2c3ef100-563e-4e72-b866-d534fec6c328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  16.3M      0  0:00:04  0:00:04 --:--:-- 18.3M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the content of a few of these text files\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "1P--z8OhM6ub",
        "outputId": "e0581ee3-7965-47ee-cc8b-9ec00d93be82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the IMDB movie reviews data"
      ],
      "metadata": {
        "id": "KFSx_y3EN9zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, the `train/pos/` directory contains a set of `12,500` text files, each of which\n",
        "contains the text body of a positive-sentiment movie review to be used as training data.\n",
        "The negative-sentiment reviews live in the “neg” directories. \n",
        "\n",
        "In total, there are `25,000`\n",
        "text files for training and another 25,000 for testing.\n",
        "\n",
        "Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`:"
      ],
      "metadata": {
        "id": "qlF10e0JN-gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir/\"val\"\n",
        "train_dir = base_dir/\"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir/category)\n",
        "  files = os.listdir(train_dir/category)\n",
        "  # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code\n",
        "  random.Random(1337).shuffle(files)\n",
        "  # Take 20% of the training files to use for validation\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
        "    shutil.move(train_dir/category/fname, val_dir/category/fname)"
      ],
      "metadata": {
        "id": "YMuEAvy2OQRY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember how, we used the `image_dataset_from_directory` utility to\n",
        "create a batched Dataset of images and their labels for a directory structure? You can do the exact same thing for text files using the `text_dataset_from_directory` utility.\n",
        "\n",
        "Let’s create three Dataset objects for training, validation, and testing:"
      ],
      "metadata": {
        "id": "TIAswQR1SUUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "V6vA9z3ASbd3",
        "outputId": "a36aa99a-1657-480f-c024-02d1def23177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”"
      ],
      "metadata": {
        "id": "lMPppHlCTFC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "nwyYSSmCTHml",
        "outputId": "9577ad09-a0d6-4e47-c096-2266ac462358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'This rip off of the 1984 hit \"Gremlins\" is quite possibly the biggest train wreck of a movie ever made. Even for a \\'B\\' grade movie, all other cheap horror movies on the same platform completely dwarf this movie in terms of plot, acting, and goodness.<br /><br />It begins with a random old security guard and the younger punky security guard whose name is of no importance. Why? Because a few minutes into the film he walks into the \\'forbidden\\' safe, and is killed whilst living out his fantasy of being a rock star in a cheap pub.<br /><br />This is just an appetizer for the scat-filled main course. The main character, KEVIN, struggles various times to prove himself as more than a total pussy. Perhaps he succeeds within the film, but to the audience he proves himself as nothing more than a bad actor. Kevin gets himself a job with the old security guard, and is guided through his security shift in the (wait for it) abandoned studio lot. Yes why bother making a set when you can just use the studio itself. Back to the film. Kevin somehow opens the forbidden safe and releases the Hobgoblins. The Hobgoblins force people to live out their wildest fantasies and then kill them for some reason. They must be returned before sunrise or else...or else what? Exactly.<br /><br />Other characters include Kevin\\'s \\'macho\\' army friend NICK, Nick\\'s \\'woman\\' DAPHNE whose character has no more substance than a bitch-slut attitude and prostitute worthy outfits. There is Kevin\\'s manipulative and \\'reserved\\' girlfriend AMY, whose deepest desire is apparently to be a badly portrayed Cher look-alike with fishnet stockings with a pair of blue grandma underpants on top.. Don\\'t ask me how that works. Quite possibly the most entertaining character of all is KYLE. How such groups of friends are made is up for question. Kyle is a perverted creep who can\\'t go an hour without self-stimulating. His hobbies include calling up sex-chat lines from other people\\'s houses and most likely sniffing underwear.<br /><br />The story unfolds as the heroes search for the Hobgoblins: knee-high creatures (aka. hand puppets) which, for some reason, attempt to travel no further than the borders of the local neighborhood. Each of the characters eventually lives out their wildest fantasy which never has anything to do with having millions of dollars... or the film having a big budget.<br /><br />WARNING SPOILERS AHEAD: The twist at the end of this movie will leave the watcher wondering \"What?\". The Hobgoblins are returned to the safe by...their own free will. Perhaps they lost patience waiting for sunrise to wreak havoc, or perhaps the story-writers got writer\\'s cramp and decided not to worry about the ending. Upon returning to the safe, the old security guard reveals \"What he learned in the military\" and detonates explosives which destroys the safe, signaling the end of the evil Hobgoblins and the end of this roller coaster ride; better fitted to a ride on an escalator.<br /><br />The sheer badness of this film is enough to send someone to tears. If you plan to watch it, I recommend a few alcoholic drinks beforehand to take any serious consideration of the film out of mind.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All set. Now let’s try learning something from this data."
      ],
      "metadata": {
        "id": "zCfTCoZSTro0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing words as a set: The bag-of-words approach"
      ],
      "metadata": {
        "id": "kyo9giBDTsIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to encode a piece of text for processing by a machine learning\n",
        "model is to discard order and treat it as a set (a “bag”) of tokens. \n",
        "\n",
        "You could either look at individual words (unigrams), or try to recover some local order information by looking at groups of consecutive token (N-grams)."
      ],
      "metadata": {
        "id": "lY-0GpLxTv8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Single words with binary encoding"
      ],
      "metadata": {
        "id": "_3pyKsnQS-wX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use a bag of single words, the sentence “the cat sat on the mat” becomes-\n",
        "\n",
        "```python\n",
        "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n",
        "```\n",
        "\n",
        "The main advantage of this encoding is that you can represent an entire text as a single\n",
        "vector, where each entry is a presence indicator for a given word. \n",
        "\n",
        "For instance,\n",
        "using binary encoding (multi-hot), you’d encode a text as a vector with as many\n",
        "dimensions as there are words in your vocabulary—with 0s almost everywhere and\n",
        "some 1s for dimensions that encode words present in the text.\n",
        "\n",
        "First, let’s process our raw text datasets with a TextVectorization layer so that they yield multi-hot encoded binary word vectors."
      ],
      "metadata": {
        "id": "UVs_HWi8TCEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the vocabulary to the 20,000 most frequent words.In general, 20,000 is the right vocabulary size for text classification.\n",
        "text_vectorization = layers.TextVectorization(max_tokens=20000, output_mode=\"multi_hot\") # Encode the output tokens as multi-hot binary vectors\n",
        "\n",
        "# Prepare a dataset that only yields raw text inputs (no labels).\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "u72_BiqgTjIX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare processed versions of our training, validation, and test dataset.\n",
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "3a29FT2wUsMp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try to inspect the output of one of these datasets."
      ],
      "metadata": {
        "id": "eI1_gNfuVOst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "JwH8YzizVPLE",
        "outputId": "05888b2f-0d66-4159-a22d-056b7a99b5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s write a reusable model-building function that we’ll use in all of our experiments."
      ],
      "metadata": {
        "id": "DZAZyiSdWgGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens, ))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "CTrGV_KEWjKt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let’s train and test our model."
      ],
      "metadata": {
        "id": "Ghqvx59tWgzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1TCKuhVzXfBK",
        "outputId": "81aec9a3-3ec9-41ae-caed-1d6a23b055bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)]"
      ],
      "metadata": {
        "id": "sxWAQKTrXlYr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call `cache()` on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory."
      ],
      "metadata": {
        "id": "j-3ER_JvX4Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "id": "N78xCH3bX9wj",
        "outputId": "1f36e7a5-7277-41f4-caf4-46f0d5104978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 18s 26ms/step - loss: 0.3967 - accuracy: 0.8352 - val_loss: 0.3001 - val_accuracy: 0.8804\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2655 - accuracy: 0.9009 - val_loss: 0.3007 - val_accuracy: 0.8848\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2361 - accuracy: 0.9197 - val_loss: 0.3113 - val_accuracy: 0.8894\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2214 - accuracy: 0.9257 - val_loss: 0.3351 - val_accuracy: 0.8880\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2177 - accuracy: 0.9299 - val_loss: 0.3378 - val_accuracy: 0.8902\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2155 - accuracy: 0.9319 - val_loss: 0.3459 - val_accuracy: 0.8912\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2030 - accuracy: 0.9375 - val_loss: 0.3664 - val_accuracy: 0.8860\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2073 - accuracy: 0.9380 - val_loss: 0.3565 - val_accuracy: 0.8898\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2039 - accuracy: 0.9378 - val_loss: 0.3739 - val_accuracy: 0.8882\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2006 - accuracy: 0.9391 - val_loss: 0.3814 - val_accuracy: 0.8874\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30d38eb1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "WcyU9pgcYVuU",
        "outputId": "69e5e2c1-0c3c-464c-8900-d2e6990b9a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 10s 13ms/step - loss: 0.2949 - accuracy: 0.8854\n",
            "Test acc: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gets us to a test accuracy of 89.2%: not bad! Note that in this case, since the dataset\n",
        "is a balanced two-class classification dataset (there are as many positive samples as\n",
        "negative samples), the “naive baseline” we could reach without training an actual model\n",
        "would only be 50%. \n",
        "\n",
        "Meanwhile, the best score that can be achieved on this dataset\n",
        "without leveraging external data is around 95% test accuracy."
      ],
      "metadata": {
        "id": "IWLplN1yYoRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bigrams with binary encoding"
      ],
      "metadata": {
        "id": "pglJ08BIYx2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-nbJNwsuY166"
      }
    }
  ]
}