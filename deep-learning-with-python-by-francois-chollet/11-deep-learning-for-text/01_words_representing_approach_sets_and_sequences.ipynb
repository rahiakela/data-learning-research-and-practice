{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-words-representing-approach--sets-and-sequences.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyONsnIKVOuY9bd5i2FY0JpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/01_words_representing_approach_sets_and_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words representing approache: Sets and sequences"
      ],
      "metadata": {
        "id": "bVyFMTsGCS2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A much more problematic question,\n",
        "however, is how to encode the way words are woven into sentences: word order.\n",
        "\n",
        "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don’t have a natural, canonical order.\n",
        "\n",
        "The simplest thing you could do is just discard order and\n",
        "treat text as an unordered set of words—this gives you **bag-of-words models**.\n",
        "\n",
        "You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the **recurrent models**.\n",
        "\n",
        "Finally, a hybrid approach is also possible: the **Transformer architecture** is technically order-agnostic, yet it injects word-position information into\n",
        "the representations it processes, which enables it to simultaneously look at different parts of a sentence, while still being order-aware. \n",
        "\n",
        "Because they take into account word order, **both RNNs and Transformers are called sequence models.**\n",
        "\n",
        "We’ll demonstrate each approach on a well-known text classification benchmark:\n",
        "the IMDB movie review sentiment-classification dataset.\n",
        "\n",
        "Let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwj5k3eqCizD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "8av_x5IrLvDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import os, pathlib, shutil, random"
      ],
      "metadata": {
        "id": "qqyixaVcLwJ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by downloading the dataset from the Stanford page and uncompressing it:"
      ],
      "metadata": {
        "id": "uD5ryFCIMTIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "# delete unwanted file and subdirectory\n",
        "rm -rf aclImdb/train/unsup\n",
        "rm -rf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "qikpFafHMZRp",
        "outputId": "22d675ef-d84c-4284-ecbe-50541e11b08d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  8090k      0  0:00:10  0:00:10 --:--:-- 16.7M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the content of a few of these text files\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "1P--z8OhM6ub",
        "outputId": "943b2a54-4f7e-4caa-c10f-5607099ed503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the IMDB movie reviews data"
      ],
      "metadata": {
        "id": "KFSx_y3EN9zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, the `train/pos/` directory contains a set of `12,500` text files, each of which\n",
        "contains the text body of a positive-sentiment movie review to be used as training data.\n",
        "The negative-sentiment reviews live in the “neg” directories. \n",
        "\n",
        "In total, there are `25,000`\n",
        "text files for training and another 25,000 for testing.\n",
        "\n",
        "Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`:"
      ],
      "metadata": {
        "id": "qlF10e0JN-gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir/\"val\"\n",
        "train_dir = base_dir/\"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir/category)\n",
        "  files = os.listdir(train_dir/category)\n",
        "  # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code\n",
        "  random.Random(1337).shuffle(files)\n",
        "  # Take 20% of the training files to use for validation\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
        "    shutil.move(train_dir/category/fname, val_dir/category/fname)"
      ],
      "metadata": {
        "id": "YMuEAvy2OQRY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember how, we used the `image_dataset_from_directory` utility to\n",
        "create a batched Dataset of images and their labels for a directory structure? You can do the exact same thing for text files using the `text_dataset_from_directory` utility.\n",
        "\n",
        "Let’s create three Dataset objects for training, validation, and testing:"
      ],
      "metadata": {
        "id": "TIAswQR1SUUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "V6vA9z3ASbd3",
        "outputId": "6c62a2cc-e7de-4bf6-8947-94a7c4ad92df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”"
      ],
      "metadata": {
        "id": "lMPppHlCTFC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "nwyYSSmCTHml",
        "outputId": "b99718a6-334f-46d1-bf96-09e6c521fadf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'Cheesy script, cheesy one-liners. Timothy Hutton\\'s performance a \"little\" over the top. David Duchovny still seemed to be stuck in his Fox Mulder mode. No chemistry with his large-lipped female co-star.He needs Gillian Anderson to shine. He does not seem to have any talent of his own.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All set. Now let’s try learning something from this data."
      ],
      "metadata": {
        "id": "zCfTCoZSTro0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing words as a set: The bag-of-words approach"
      ],
      "metadata": {
        "id": "kyo9giBDTsIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lY-0GpLxTv8K"
      }
    }
  ]
}