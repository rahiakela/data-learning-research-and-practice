{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-preparing-text-data.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcHD+74VtkZjMDilmN3k9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/01_preparing_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing text data"
      ],
      "metadata": {
        "id": "MDJbbxiRu98c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. \n",
        "\n",
        "**Vectorizing text is the process of transforming\n",
        "text into numeric tensors.**\n",
        "\n",
        " Text vectorization processes come in many shapes and\n",
        "forms, but they all follow the same template.\n",
        "\n",
        "- First, you standardize the text to make it easier to process, such as by converting\n",
        "it to lowercase or removing punctuation.\n",
        "- You split the text into units (called tokens), such as characters, words, or groups\n",
        "of words. This is called tokenization.\n",
        "- You convert each such token into a numerical vector. This will usually involve\n",
        "first indexing all tokens present in the data.\n",
        "\n",
        "<img src='images/1.png?raw=1' width='800'/>"
      ],
      "metadata": {
        "id": "GryfCpykvAQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text standardization"
      ],
      "metadata": {
        "id": "jh8RoIkowA17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text standardization is a basic form of feature engineering that aims to erase\n",
        "encoding differences that you don’t want your model to have to deal with. It’s not\n",
        "exclusive to machine learning, either—you’d have to do the same thing if you were\n",
        "building a search engine.\n",
        "\n",
        "One of the simplest and most widespread standardization schemes is “convert to\n",
        "lowercase and remove punctuation characters.”\n",
        "\n",
        "Of course, standardization may\n",
        "also erase some amount of information, so always keep the context in mind: for\n",
        "instance, if you’re writing a model that extracts questions from interview articles, it\n",
        "should definitely treat “?” as a separate token instead of dropping it, because it’s a useful\n",
        "signal for this specific task."
      ],
      "metadata": {
        "id": "iN8seLaqwBg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text splitting (tokenization)"
      ],
      "metadata": {
        "id": "GNH-ai5Axqhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your text is standardized, you need to break it up into units to be vectorized\n",
        "(tokens), a step called tokenization. You could do this in three different ways:\n",
        "\n",
        "- Word-level tokenization\n",
        "- N-gram tokenization\n",
        "- Character-level tokenization\n",
        "\n",
        "In general, you’ll always use either word-level or N-gram tokenization. There are two kinds of text-processing models: \n",
        "\n",
        "- those that care about word order, called **sequence models**,\n",
        "- those that treat input words as a set, discarding their original order, \n",
        "called **bag-of-words models**\n",
        "\n",
        "If you’re building a sequence model, you’ll use word-level tokeni\n",
        "zation, and if you’re building a bag-of-words model, you’ll use N-gram tokenization.\n",
        "\n",
        "N-grams are a way to artificially inject a small amount of local word order information into the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LMLrEad4xs63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vocabulary indexing"
      ],
      "metadata": {
        "id": "EB5HbxKQyaTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your text is split into tokens, you need to encode each token into a numerical\n",
        "representation. \n",
        "\n",
        "You could potentially do this in a stateless way, such as by hashing each\n",
        "token into a fixed binary vector, but in practice, the way you’d go about it is to build\n",
        "an index of all terms found in the training data (the “vocabulary”), and assign a\n",
        "unique integer to each entry in the vocabulary."
      ],
      "metadata": {
        "id": "Ed5TuRoMya66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {}\n",
        "for text in dataset:\n",
        "  text = standardize(text)\n",
        "  tokens = tokenize(text)\n",
        "  for token in tokens:\n",
        "    if token not in vocabulary:\n",
        "      vocabulary[token] = len(vocabulary)"
      ],
      "metadata": {
        "id": "HtyQypdiyt_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then convert that integer into a vector encoding that can be processed by a\n",
        "neural network, like a one-hot vector:"
      ],
      "metadata": {
        "id": "oCCOnRrczEyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_token(token):\n",
        "  vector = np.zeros((len(vocabulary), ))\n",
        "  token_index = vocabulary[token]\n",
        "  vector[token_index] = 1\n",
        "  return vector"
      ],
      "metadata": {
        "id": "-GaKlR6yzIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the TextVectorization layer"
      ],
      "metadata": {
        "id": "Kt_wILLKzguQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9uctOBzMzhfC"
      }
    }
  ]
}