{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-transformer-encoder-for-text-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKg/DTbiZkx2VlNK3XOcJe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/03_transformer_encoder_for_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer encoder for text classification"
      ],
      "metadata": {
        "id": "bVyFMTsGCS2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factoring outputs into multiple independent spaces, adding residual connections, adding normalization layers—all of these are standard architecture patterns that one would be wise to leverage in any complex model. Together, these bells and whistles form the Transformer encoder—one\n",
        "of two critical parts that make up the Transformer architecture.\n",
        "\n",
        "<img src='https://github.com/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/2.png?raw=1' width='600'/>\n",
        "\n",
        "The original Transformer architecture consists of two parts: a Transformer encoder that processes the source sequence, and a Transformer decoder that uses the source sequence to generate a translated version.\n",
        "\n",
        "Crucially, the encoder part can be used for text classification—it’s a very generic module that ingests a sequence and learns to turn it into a more useful representation.\n",
        "\n",
        "Let’s implement a Transformer encoder and try it on the movie review sentiment\n",
        "classification task.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwj5k3eqCizD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "8av_x5IrLvDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import os, pathlib, shutil, random"
      ],
      "metadata": {
        "id": "qqyixaVcLwJ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by downloading the dataset from the Stanford page and uncompressing it:"
      ],
      "metadata": {
        "id": "uD5ryFCIMTIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "# delete unwanted file and subdirectory\n",
        "rm -rf aclImdb/train/unsup\n",
        "rm -rf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "qikpFafHMZRp",
        "outputId": "30cbde77-c53e-44b1-e4b3-b0fbdcb04c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  30.2M      0  0:00:02  0:00:02 --:--:-- 30.2M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the content of a few of these text files\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "1P--z8OhM6ub",
        "outputId": "d284b326-a6cd-4ca8-e67d-f8963f4ea1f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the IMDB movie reviews data"
      ],
      "metadata": {
        "id": "KFSx_y3EN9zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, the `train/pos/` directory contains a set of `12,500` text files, each of which\n",
        "contains the text body of a positive-sentiment movie review to be used as training data.\n",
        "The negative-sentiment reviews live in the “neg” directories. \n",
        "\n",
        "In total, there are `25,000`\n",
        "text files for training and another 25,000 for testing.\n",
        "\n",
        "Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`:"
      ],
      "metadata": {
        "id": "qlF10e0JN-gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir/\"val\"\n",
        "train_dir = base_dir/\"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir/category)\n",
        "  files = os.listdir(train_dir/category)\n",
        "  # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code\n",
        "  random.Random(1337).shuffle(files)\n",
        "  # Take 20% of the training files to use for validation\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
        "    shutil.move(train_dir/category/fname, val_dir/category/fname)"
      ],
      "metadata": {
        "id": "YMuEAvy2OQRY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember how, we used the `image_dataset_from_directory` utility to\n",
        "create a batched Dataset of images and their labels for a directory structure? You can do the exact same thing for text files using the `text_dataset_from_directory` utility.\n",
        "\n",
        "Let’s create three Dataset objects for training, validation, and testing:"
      ],
      "metadata": {
        "id": "TIAswQR1SUUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "V6vA9z3ASbd3",
        "outputId": "8e3e5cf0-3d70-434d-9795-5b3a166ad542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”"
      ],
      "metadata": {
        "id": "lMPppHlCTFC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "nwyYSSmCTHml",
        "outputId": "fc3ad156-7fc4-44bf-d376-d2b28e08524c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"Comparable to Fight Club, The Matrix, A.I., Sixth Sense, among others. This film approaches the psyche in a way never done before. The first 30 minutes builds a interesting love story between Diaz-Cruise-Cruz. The rest of the movie is, well, confusing, you'll pick more every time you watch it (i've gone to the movies to see it 3 times now)\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preparing datasets for sequences"
      ],
      "metadata": {
        "id": "s4JUN4qkQY9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let’s prepare datasets that return integer sequences."
      ],
      "metadata": {
        "id": "h7T3QwANQfoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "# Prepare a dataset that only yields raw text inputs (no labels).\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words.\n",
        "text_vectorization = layers.TextVectorization(max_tokens=max_tokens,\n",
        "                                              output_mode=\"int\",\n",
        "                                              output_sequence_length=max_length)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "eKQRxZI9Qmk2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer encoder"
      ],
      "metadata": {
        "id": "4S4HkAW2aue8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s implement a Transformer encoder and try it on the movie review sentiment\n",
        "classification task."
      ],
      "metadata": {
        "id": "F020b46bd13J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  \n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim    # Size of the input token vectors\n",
        "    self.dense_dim = dense_dim    # Size of the inner dense layer\n",
        "    self.num_heads = num_heads  # Number of attention heads\n",
        "\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_projection = keras.Sequential([\n",
        "         layers.Dense(dense_dim, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim)                                     \n",
        "    ])\n",
        "\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    # The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    projection_input = self.layernorm_1(inputs + attention_output)\n",
        "    projection_output = self.dense_projection(projection_input)\n",
        "    return self.layernorm_2(projection_input + projection_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "XcmggiT3eAz0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because `BatchNormalization` doesn’t work well for sequence data. Instead, we’re using the layer, `LayerNormalization` which normalizes each sequence independently from other sequences in the batch.\n",
        "\n",
        "```python\n",
        "def layer_normalization(batch_of_sequences):\n",
        "  # Input shape: (batch_size, sequence_length, embedding_dim)\n",
        "  # To compute mean and variance, we only pool data over the last axis (axis -1).\n",
        "  mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
        "  variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
        "  return (batch_of_sequences - mean) / variance\n",
        "```\n",
        "\n",
        "Compare to BatchNormalization (during training):\n",
        "\n",
        "```python\n",
        "def batch_normalization(batch_of_images):\n",
        "  # Input shape: (batch_size, height, width, channels)\n",
        "  # Pool data over the batch axis (axis 0), which creates interactions between samples in a batch.\n",
        "  mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "  variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "  return (batch_of_images - mean) / variance\n",
        "```\n",
        "\n",
        "While collects information from many samples to obtain accu\n",
        "`BatchNormalization` rate statistics for the feature means and variances,\n",
        "pools data `LayerNormalization` within each sequence separately, which is more appropriate for sequence data.\n",
        "\n",
        "Now that we’ve implemented our TransformerEncoder, we can use it to assemble a\n",
        "text-classification model.\n",
        "\n"
      ],
      "metadata": {
        "id": "UCu7BVXoDP5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32"
      ],
      "metadata": {
        "id": "b_S16wGHE8co"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One input is a sequence of integers\n",
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "# Since TransformerEncoder returns full sequences, we need to reduce each sequence to a single vector for classification via a global pooling layer.\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hxcMnAkug6RX",
        "outputId": "507a2d1d-48da-4032-f900-c9f7c91f84f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 256)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,664,033\n",
            "Trainable params: 5,664,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "T-ogl15Qg_y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the custom TransformerEncoder class to the model-loading process.\n",
        "model = tf.keras.models.load_model(\"transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "Bwx9nh6AWxJv",
        "outputId": "8d85a26b-cef4-49d1-a58b-76b6bb023a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 636s 812ms/step - loss: 0.2987 - accuracy: 0.8764\n",
            "Test accuracy: 0.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It gets to 87.5% test accuracy—slightly worse than the GRU model.\n",
        "\n",
        "`Self-attention` is a set-processing mechanism, focused on the relationships between pairs of sequence elements—it’s blind to whether these elements occur at the beginning, at the end, or in the middle of a sequence.\n",
        "\n",
        "`Transformer` was a hybrid approach that is technically order-agnostic, but that manually injects order information in the representations it processes. \n",
        "\n",
        "This is the missing ingredient! It’s called `positional encoding`."
      ],
      "metadata": {
        "id": "t9fTBTPehn3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Re-inject order information using positional encoding"
      ],
      "metadata": {
        "id": "yg7DLtm7Ippv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind positional encoding is very simple: to give the model access to wordorder information.\n",
        "\n",
        "The simplest scheme you could come up with would be to concatenate the word’s\n",
        "position to its embedding vector. You’d add a “position” axis to the vector and fill it with 0 for the first word in the sequence, 1 for the second, and so on.\n",
        "\n",
        "That may not be ideal, however, because the positions can potentially be very large\n",
        "integers, which will disrupt the range of values in the embedding vector. As you know,\n",
        "neural networks don’t like very large input values, or discrete input distributions.\n",
        "\n",
        "The original “Attention is all you need” paper used an interesting trick to encode\n",
        "word positions: it added to the word embeddings a vector containing values in the\n",
        "range `[-1, 1]` that varied cyclically depending on the position (it used cosine functions\n",
        "to achieve this). This trick offers a way to uniquely characterize any integer in a\n",
        "large range via a vector of small values.\n",
        "\n",
        "We’ll do something simpler and more effective: we’ll learn positionembedding\n",
        "vectors the same way we learn to embed word indices. We’ll then proceed\n",
        "to add our position embeddings to the corresponding word embeddings, to obtain a\n",
        "position-aware word embedding. This technique is called `positional embedding`.\n",
        "\n"
      ],
      "metadata": {
        "id": "LNH2w2K0I6-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "\n",
        "  # A downside of position embeddings is that the sequence length needs to be known in advance\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    # Prepare an Embedding layer for the token indices.\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "    # And another one for the token positions\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    # add both embedding vectors together\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    \"\"\"\n",
        "    Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. \n",
        "    The compute_mask method will called automatically by the framework, and the mask will get propagated to the next layer.\n",
        "    \"\"\"\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "y_H5Cgdo_qB-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would use this `PositionEmbedding` layer just like a regular `Embedding` layer."
      ],
      "metadata": {
        "id": "xAU0eJlBIuTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A text classification Transformer"
      ],
      "metadata": {
        "id": "oT1kGm3mI0uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All you have to do to start taking word order into account is swap the old `Embedding` layer with our position-aware version."
      ],
      "metadata": {
        "id": "b2S0BmWKI8ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32"
      ],
      "metadata": {
        "id": "P1jdhcTGJH3l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One input is a sequence of integers\n",
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "# Since TransformerEncoder returns full sequences, we need to reduce each sequence to a single vector for classification via a global pooling layer.\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "INZhjmP-JT1v",
        "outputId": "906a84e8-ff95-438a-ce97-3ca56879070c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding_1 (Pos  (None, None, 256)        5273600   \n",
            " itionalEmbedding)                                               \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 256)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,817,633\n",
            "Trainable params: 5,817,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "LUxDWXWjJvpO",
        "outputId": "03f0963f-a9e3-4436-ed73-5b73e5678a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 46s 70ms/step - loss: 0.4445 - accuracy: 0.7965 - val_loss: 0.2685 - val_accuracy: 0.8902\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.2463 - accuracy: 0.9057 - val_loss: 0.2561 - val_accuracy: 0.8968\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.1892 - accuracy: 0.9286 - val_loss: 0.2532 - val_accuracy: 0.9010\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.1577 - accuracy: 0.9407 - val_loss: 0.2781 - val_accuracy: 0.8908\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.1329 - accuracy: 0.9518 - val_loss: 0.2761 - val_accuracy: 0.8912\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.1138 - accuracy: 0.9596 - val_loss: 0.3861 - val_accuracy: 0.8876\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0963 - accuracy: 0.9678 - val_loss: 0.3837 - val_accuracy: 0.8824\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0787 - accuracy: 0.9748 - val_loss: 0.4248 - val_accuracy: 0.8768\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0646 - accuracy: 0.9789 - val_loss: 0.6643 - val_accuracy: 0.8494\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0533 - accuracy: 0.9830 - val_loss: 0.7201 - val_accuracy: 0.8614\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.5729 - val_accuracy: 0.8740\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0338 - accuracy: 0.9893 - val_loss: 0.6636 - val_accuracy: 0.8734\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.0263 - accuracy: 0.9929 - val_loss: 0.6463 - val_accuracy: 0.8780\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.0214 - accuracy: 0.9937 - val_loss: 0.9238 - val_accuracy: 0.8710\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.8146 - val_accuracy: 0.8696\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.0165 - accuracy: 0.9958 - val_loss: 0.8583 - val_accuracy: 0.8630\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0121 - accuracy: 0.9970 - val_loss: 1.0626 - val_accuracy: 0.8662\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.8746 - val_accuracy: 0.8720\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0104 - accuracy: 0.9974 - val_loss: 1.0147 - val_accuracy: 0.8758\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 1.1468 - val_accuracy: 0.8694\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9d94fbc690>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the custom TransformerEncoder class to the model-loading process.\n",
        "model = tf.keras.models.load_model(\"full_transformer_encoder.keras\", \n",
        "                                   custom_objects={\"TransformerEncoder\": TransformerEncoder, \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "6XBHl3zwJwGV",
        "outputId": "26ee6014-f056-4399-d8ea-5bb1c61c861f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 20s 25ms/step - loss: 0.2898 - accuracy: 0.8846\n",
            "Test accuracy: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get to 88.3% test accuracy, a solid improvement that clearly demonstrates the value of word order information for text classification. This is our best sequence model so far!"
      ],
      "metadata": {
        "id": "Jp52SfIeNFLw"
      }
    }
  ]
}