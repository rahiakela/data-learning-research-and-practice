{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/dive-into-deep-learning/pytorch/chapter_attention-mechanisms/03-attention-scoring-functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "x03z5_l035FC"
      },
      "source": [
        "## Attention Scoring Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used a Gaussian kernel to model\n",
        "interactions between queries and keys.\n",
        "Treating the exponent of the Gaussian kernel\n",
        "as an *attention scoring function* (or *scoring function* for short),\n",
        "the results of this function were\n",
        "essentially fed into\n",
        "a softmax operation.\n",
        "As a result,\n",
        "we obtained\n",
        "a probability distribution (attention weights)\n",
        "over values that are paired with keys.\n",
        "In the end,\n",
        "the output of the attention pooling\n",
        "is simply a weighted sum of the values\n",
        "based on these attention weights.\n",
        "\n",
        "At a high level,\n",
        "we can use the above algorithm\n",
        "to instantiate the framework of attention mechanisms.\n",
        "Denoting an attention scoring function by $a$,\n",
        "the figure\n",
        "illustrates how the output of attention pooling\n",
        "can be computed as a weighted sum of values.\n",
        "Since attention weights are\n",
        "a probability distribution,\n",
        "the weighted sum is essentially\n",
        "a weighted average.\n",
        "\n",
        "![Computing the output of attention pooling as a weighted average of values.](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/dive-into-deep-learning/img/attention-output.svg?raw=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Mathematically,\n",
        "suppose that we have\n",
        "a query $\\mathbf{q} \\in \\mathbb{R}^q$\n",
        "and $m$ key-value pairs $(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)$, where any $\\mathbf{k}_i \\in \\mathbb{R}^k$ and any $\\mathbf{v}_i \\in \\mathbb{R}^v$.\n",
        "The attention pooling $f$\n",
        "is instantiated as a weighted sum of the values:\n",
        "\n",
        "$$f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,$$\n",
        "\n",
        "where\n",
        "the attention weight (scalar) for the query $\\mathbf{q}$\n",
        "and key $\\mathbf{k}_i$\n",
        "is computed by\n",
        "the softmax operation of\n",
        "an attention scoring function $a$ that maps two vectors to a scalar:\n",
        "\n",
        "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}.$$\n",
        "\n",
        "As we can see,\n",
        "different choices of the attention scoring function $a$\n",
        "lead to different behaviors of attention pooling.\n",
        "In this section,\n",
        "we introduce two popular scoring functions\n",
        "that we will use to develop more\n",
        "sophisticated attention mechanisms later."
      ],
      "metadata": {
        "id": "Y_W0I7gABfR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "bEXf_Wqt4QHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "pip install d2l\n",
        "pip uninstall matplotlib\n",
        "python -m pip install --upgrade pip\n",
        "pip install matplotlib"
      ],
      "metadata": {
        "id": "F-IyH72J4RxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "06jdqIVv35FK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Masked Softmax Operation"
      ],
      "metadata": {
        "id": "nKYhMEyXDWlt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "qDY4EtXF35FM"
      },
      "source": [
        "As we just mentioned,\n",
        "a softmax operation is used to\n",
        "output a probability distribution as attention weights.\n",
        "In some cases,\n",
        "not all the values should be fed into attention pooling.\n",
        "For instance,\n",
        "for efficient minibatch processing,\n",
        "some text sequences are padded with\n",
        "special tokens that do not carry meaning.\n",
        "To get an attention pooling\n",
        "over\n",
        "only meaningful tokens as values,\n",
        "we can specify a valid sequence length (in number of tokens)\n",
        "to filter out those beyond this specified range\n",
        "when computing softmax.\n",
        "In this way,\n",
        "we can implement such a *masked softmax operation*\n",
        "in the following `masked_softmax` function,\n",
        "where any value beyond the valid length\n",
        "is masked as zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "Vui5BnEu35FN"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        # On the last axis, replace masked elements with a very large negative value, whose exponentiation outputs 0\n",
        "        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "OldRFnHw35FO"
      },
      "source": [
        "To [**demonstrate how this function works**],\n",
        "consider a minibatch of two $2 \\times 4$ matrix examples,\n",
        "where the valid lengths for these two examples\n",
        "are two and three, respectively.\n",
        "As a result of the masked softmax operation,\n",
        "values beyond the valid lengths\n",
        "are all masked as zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "YyzSf22435FP",
        "outputId": "a649cd09-1828-49a8-f442-41a47ae08af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.5839, 0.4161, 0.0000, 0.0000],\n",
              "         [0.6304, 0.3696, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.3128, 0.4556, 0.2317, 0.0000],\n",
              "         [0.3241, 0.2354, 0.4405, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "tg4wV1fG35FQ"
      },
      "source": [
        "Similarly, we can also\n",
        "use a two-dimensional tensor\n",
        "to specify valid lengths\n",
        "for every row in each matrix example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "haJ10WKQ35FS",
        "outputId": "13ee23df-8c15-4b9b-ef3f-6b1bbab62970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4160, 0.2986, 0.2853, 0.0000]],\n",
              "\n",
              "        [[0.5296, 0.4704, 0.0000, 0.0000],\n",
              "         [0.2385, 0.2362, 0.2346, 0.2907]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Additive Attention"
      ],
      "metadata": {
        "id": "VaycpOyGFQCP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "q8JJr0er35FT"
      },
      "source": [
        "In general,\n",
        "when queries and keys are vectors of different lengths,\n",
        "we can use additive attention\n",
        "as the scoring function.\n",
        "Given a query $\\mathbf{q} \\in \\mathbb{R}^q$\n",
        "and a key $\\mathbf{k} \\in \\mathbb{R}^k$,\n",
        "the *additive attention* scoring function\n",
        "\n",
        "$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$\n",
        "\n",
        "where\n",
        "learnable parameters\n",
        "$\\mathbf W_q\\in\\mathbb R^{h\\times q}$, $\\mathbf W_k\\in\\mathbb R^{h\\times k}$, and $\\mathbf w_v\\in\\mathbb R^{h}$.\n",
        "The query and the key are concatenated\n",
        "and fed into an MLP with a single hidden layer\n",
        "whose number of hidden units is $h$, a hyperparameter.\n",
        "By using $\\tanh$ as the activation function and disabling\n",
        "bias terms,\n",
        "we implement additive attention in the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "AorfBYbO35FU"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    \"\"\"Additive attention.\"\"\"\n",
        "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
        "        super(AdditiveAttention, self).__init__(**kwargs)\n",
        "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
        "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
        "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "        # After dimension expansion, shape of `queries`: (`batch_size`, no. of queries, 1, `num_hiddens`) \n",
        "        # and shape of `keys`: (`batch_size`, 1, no. of key-value pairs, `num_hiddens`). Sum them up with broadcasting\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "        features = torch.tanh(features)\n",
        "        # There is only one output of `self.w_v`, so we remove the last one-dimensional entry from the shape.\n",
        "        # Shape of `scores`: (`batch_size`, no. of queries, no. of key-value pairs)\n",
        "        scores = self.w_v(features).squeeze(-1)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        # Shape of `values`: (`batch_size`, no. of key-value pairs, value dimension)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "eaX9Pfl635FV"
      },
      "source": [
        "Let us [**demonstrate the above `AdditiveAttention` class**]\n",
        "with a toy example,\n",
        "where shapes (batch size, number of steps or sequence length in tokens, feature size)\n",
        "of queries, keys, and values\n",
        "are ($2$, $1$, $20$), ($2$, $10$, $2$),\n",
        "and ($2$, $10$, $4$), respectively.\n",
        "The attention pooling output\n",
        "has a shape of (batch size, number of steps for queries, feature size for values).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "oxVfSA-d35FW",
        "outputId": "955c820a-7013-4f90-eeb1-67a5e572129a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
        "# The two value matrices in the `values` minibatch are identical\n",
        "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=0.1)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 24,
        "id": "XIdxOmXC35FX"
      },
      "source": [
        "Although additive attention contains learnable parameters,\n",
        "since every key is the same in this example,\n",
        "[**the attention weights**] are uniform,\n",
        "determined by the specified valid lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "id": "-fejPSHF35FX",
        "outputId": "c5500e29-e8a5-477d-8bfd-2f0c374cc548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 180x180 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"186.99575pt\" height=\"101.818906pt\" viewBox=\"0 0 186.99575 101.818906\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-28T12:19:07.582918</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 101.818906 \nL 186.99575 101.818906 \nL 186.99575 0 \nL -0 0 \nL -0 101.818906 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 59.13 \nL 145.840625 59.13 \nL 145.840625 36.81 \nL 34.240625 36.81 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pabbe553820)\">\n    <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAHAAAAAXCAYAAADTEcupAAAAeElEQVR4nO3YsQmAQBAF0X+aWp8tGFqDXZjZnC2YCV5uJnIcA/MKWBaGTbbcx/ZESZJxXnuv8NnQewH9Y0A4A8IZEM6AcAaEMyCcAeEMCGdAOAPClSVTk1/ofp0txurFC4QzIJwB4QwIZ0A4A8IZEM6AcAaEMyBcBc2IB/NA+hblAAAAAElFTkSuQmCC\" id=\"image607224f926\" transform=\"scale(1 -1)translate(0 -23)\" x=\"34.240625\" y=\"-36.13\" width=\"112\" height=\"23\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m0dce7deeae\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m0dce7deeae\" x=\"39.820625\" y=\"59.13\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(36.639375 73.728437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m0dce7deeae\" x=\"95.620625\" y=\"59.13\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(92.439375 73.728437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Keys -->\n     <g transform=\"translate(78.371094 87.406562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2694 \nL 3353 4666 \nL 4166 4666 \nL 1850 2491 \nL 4331 0 \nL 3500 0 \nL 1259 2247 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4b\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"60.576172\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"122.099609\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"181.279297\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path id=\"m49c3f1fd6d\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m49c3f1fd6d\" x=\"34.240625\" y=\"42.39\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(20.878125 46.189219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m49c3f1fd6d\" x=\"34.240625\" y=\"53.55\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(20.878125 57.349219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Queries -->\n     <g transform=\"translate(14.798437 67.277031)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-51\" d=\"M 2522 4238 \nQ 1834 4238 1429 3725 \nQ 1025 3213 1025 2328 \nQ 1025 1447 1429 934 \nQ 1834 422 2522 422 \nQ 3209 422 3611 934 \nQ 4013 1447 4013 2328 \nQ 4013 3213 3611 3725 \nQ 3209 4238 2522 4238 \nz\nM 3406 84 \nL 4238 -825 \nL 3475 -825 \nL 2784 -78 \nQ 2681 -84 2626 -87 \nQ 2572 -91 2522 -91 \nQ 1538 -91 948 567 \nQ 359 1225 359 2328 \nQ 359 3434 948 4092 \nQ 1538 4750 2522 4750 \nQ 3503 4750 4090 4092 \nQ 4678 3434 4678 2328 \nQ 4678 1516 4351 937 \nQ 4025 359 3406 84 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-51\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"78.710938\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"142.089844\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"203.613281\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"244.726562\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"272.509766\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"334.033203\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 59.13 \nL 34.240625 36.81 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 145.840625 59.13 \nL 145.840625 36.81 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 59.13 \nL 145.840625 59.13 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 36.81 \nL 145.840625 36.81 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 152.815625 88.74 \nL 156.892625 88.74 \nL 156.892625 7.2 \nL 152.815625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pbc72691de0)\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.01; stroke-linejoin: miter\"/>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAQAAABSCAYAAABzJnWUAAAAm0lEQVR4nJ2SOw7CUBADHxL3vyoNBeyXFsZIFqQcje1NlMveb3venuuZPg6MMXbKdvxcqqASoDG7+TSG7xBjm5GSSLgVjXiDQO4Izvq39SsepHxC/g5/lLJDjA2C4mzKHQ5MjjF01q50obR7P0E1jYIhkZRSgmKk7UoNI8tZgiLA6hdDI0cM1yF3BE//YqA0xCB4SClB8FK5g6UvE4PMuTPJ8jwAAAAASUVORK5CYII=\" id=\"imagebf3229080c\" transform=\"scale(1 -1)translate(0 -82)\" x=\"153\" y=\"-6\" width=\"4\" height=\"82\"/>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path id=\"mdd40242e57\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mdd40242e57\" x=\"156.892625\" y=\"88.74\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(163.892625 92.539219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mdd40242e57\" x=\"156.892625\" y=\"56.124\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(163.892625 59.923219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#mdd40242e57\" x=\"156.892625\" y=\"23.508\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(163.892625 27.307219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_9\">\n    <path d=\"M 152.815625 88.74 \nL 154.854125 88.74 \nL 156.892625 88.74 \nL 156.892625 7.2 \nL 154.854125 7.2 \nL 152.815625 7.2 \nL 152.815625 88.74 \nz\n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pabbe553820\">\n   <rect x=\"34.240625\" y=\"36.81\" width=\"111.6\" height=\"22.32\"/>\n  </clipPath>\n  <clipPath id=\"pbc72691de0\">\n   <rect x=\"152.815625\" y=\"7.2\" width=\"4.077\" height=\"81.54\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
        "                  xlabel='Keys', ylabel='Queries')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "8eU4ef1YoD-K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 26,
        "id": "O6eVIX4F35FY"
      },
      "source": [
        "A more computationally efficient\n",
        "design for the scoring function can be\n",
        "simply dot product.\n",
        "However,\n",
        "the dot product operation\n",
        "requires that both the query and the key\n",
        "have the same vector length, say $d$.\n",
        "Assume that\n",
        "all the elements of the query and the key\n",
        "are independent random variables\n",
        "with zero mean and unit variance.\n",
        "The dot product of\n",
        "both vectors has zero mean and a variance of $d$.\n",
        "To ensure that the variance of the dot product\n",
        "still remains one regardless of vector length,\n",
        "the *scaled dot-product attention* scoring function\n",
        "\n",
        "\n",
        "$$a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k}  /\\sqrt{d}$$\n",
        "\n",
        "divides the dot product by $\\sqrt{d}$.\n",
        "In practice,\n",
        "we often think in minibatches\n",
        "for efficiency,\n",
        "such as computing attention\n",
        "for\n",
        "$n$ queries and $m$ key-value pairs,\n",
        "where queries and keys are of length $d$\n",
        "and values are of length $v$.\n",
        "The scaled dot-product attention\n",
        "of queries $\\mathbf Q\\in\\mathbb R^{n\\times d}$,\n",
        "keys $\\mathbf K\\in\\mathbb R^{m\\times d}$,\n",
        "and values $\\mathbf V\\in\\mathbb R^{m\\times v}$\n",
        "is\n",
        "\n",
        "\n",
        "$$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$\n",
        "\n",
        "In the following implementation of the scaled dot product attention, we use dropout for model regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "SF-1UI7L35FY"
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\n",
        "    def __init__(self, dropout, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n",
        "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n",
        "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value dimension)\n",
        "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        # Set `transpose_b=True` to swap the last two dimensions of `keys`\n",
        "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 30,
        "id": "jqTEAeZh35FZ"
      },
      "source": [
        "To [**demonstrate the above `DotProductAttention` class**],\n",
        "we use the same keys, values, and valid lengths from the earlier toy example\n",
        "for additive attention.\n",
        "For the dot product operation,\n",
        "we make the feature size of queries\n",
        "the same as that of keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "Tz4eas9e35FZ",
        "outputId": "5599990c-403d-40d8-da0e-77fe9c729b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 2))\n",
        "attention = DotProductAttention(dropout=0.5)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 34,
        "id": "Gdw9umC235Fa"
      },
      "source": [
        "Same as in the additive attention demonstration,\n",
        "since `keys` contains the same element\n",
        "that cannot be differentiated by any query,\n",
        "[**uniform attention weights**] are obtained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "origin_pos": 35,
        "tab": [
          "pytorch"
        ],
        "id": "wLBP6tto35Fa",
        "outputId": "0996daf2-b28a-4af5-ee44-c640a118fb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 180x180 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"186.99575pt\" height=\"101.818906pt\" viewBox=\"0 0 186.99575 101.818906\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-28T12:28:02.251437</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 101.818906 \nL 186.99575 101.818906 \nL 186.99575 0 \nL -0 0 \nL -0 101.818906 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 59.13 \nL 145.840625 59.13 \nL 145.840625 36.81 \nL 34.240625 36.81 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pedc29ff2df)\">\n    <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAHAAAAAXCAYAAADTEcupAAAAeElEQVR4nO3YsQmAQBAF0X+aWp8tGFqDXZjZnC2YCV5uJnIcA/MKWBaGTbbcx/ZESZJxXnuv8NnQewH9Y0A4A8IZEM6AcAaEMyCcAeEMCGdAOAPClSVTk1/ofp0txurFC4QzIJwB4QwIZ0A4A8IZEM6AcAaEMyBcBc2IB/NA+hblAAAAAElFTkSuQmCC\" id=\"image943725c80a\" transform=\"scale(1 -1)translate(0 -23)\" x=\"34.240625\" y=\"-36.13\" width=\"112\" height=\"23\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m96e8801393\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m96e8801393\" x=\"39.820625\" y=\"59.13\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(36.639375 73.728437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m96e8801393\" x=\"95.620625\" y=\"59.13\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(92.439375 73.728437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Keys -->\n     <g transform=\"translate(78.371094 87.406562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2694 \nL 3353 4666 \nL 4166 4666 \nL 1850 2491 \nL 4331 0 \nL 3500 0 \nL 1259 2247 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4b\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"60.576172\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"122.099609\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"181.279297\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path id=\"m76b463a49f\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m76b463a49f\" x=\"34.240625\" y=\"42.39\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(20.878125 46.189219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m76b463a49f\" x=\"34.240625\" y=\"53.55\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(20.878125 57.349219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Queries -->\n     <g transform=\"translate(14.798437 67.277031)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-51\" d=\"M 2522 4238 \nQ 1834 4238 1429 3725 \nQ 1025 3213 1025 2328 \nQ 1025 1447 1429 934 \nQ 1834 422 2522 422 \nQ 3209 422 3611 934 \nQ 4013 1447 4013 2328 \nQ 4013 3213 3611 3725 \nQ 3209 4238 2522 4238 \nz\nM 3406 84 \nL 4238 -825 \nL 3475 -825 \nL 2784 -78 \nQ 2681 -84 2626 -87 \nQ 2572 -91 2522 -91 \nQ 1538 -91 948 567 \nQ 359 1225 359 2328 \nQ 359 3434 948 4092 \nQ 1538 4750 2522 4750 \nQ 3503 4750 4090 4092 \nQ 4678 3434 4678 2328 \nQ 4678 1516 4351 937 \nQ 4025 359 3406 84 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-51\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"78.710938\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"142.089844\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"203.613281\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"244.726562\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"272.509766\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"334.033203\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 59.13 \nL 34.240625 36.81 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 145.840625 59.13 \nL 145.840625 36.81 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 59.13 \nL 145.840625 59.13 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 36.81 \nL 145.840625 36.81 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 152.815625 88.74 \nL 156.892625 88.74 \nL 156.892625 7.2 \nL 152.815625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p1c629511bc)\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.01; stroke-linejoin: miter\"/>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAQAAABSCAYAAABzJnWUAAAAm0lEQVR4nJ2SOw7CUBADHxL3vyoNBeyXFsZIFqQcje1NlMveb3venuuZPg6MMXbKdvxcqqASoDG7+TSG7xBjm5GSSLgVjXiDQO4Izvq39SsepHxC/g5/lLJDjA2C4mzKHQ5MjjF01q50obR7P0E1jYIhkZRSgmKk7UoNI8tZgiLA6hdDI0cM1yF3BE//YqA0xCB4SClB8FK5g6UvE4PMuTPJ8jwAAAAASUVORK5CYII=\" id=\"image6b67519de1\" transform=\"scale(1 -1)translate(0 -82)\" x=\"153\" y=\"-6\" width=\"4\" height=\"82\"/>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path id=\"m78be220840\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m78be220840\" x=\"156.892625\" y=\"88.74\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(163.892625 92.539219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m78be220840\" x=\"156.892625\" y=\"56.124\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(163.892625 59.923219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m78be220840\" x=\"156.892625\" y=\"23.508\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(163.892625 27.307219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_9\">\n    <path d=\"M 152.815625 88.74 \nL 154.854125 88.74 \nL 156.892625 88.74 \nL 156.892625 7.2 \nL 154.854125 7.2 \nL 152.815625 7.2 \nL 152.815625 88.74 \nz\n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pedc29ff2df\">\n   <rect x=\"34.240625\" y=\"36.81\" width=\"111.6\" height=\"22.32\"/>\n  </clipPath>\n  <clipPath id=\"p1c629511bc\">\n   <rect x=\"152.815625\" y=\"7.2\" width=\"4.077\" height=\"81.54\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
        "                  xlabel='Keys', ylabel='Queries')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "nvSfMpojqEt_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 36,
        "id": "yuJSLurV35Fb"
      },
      "source": [
        "* We can compute the output of attention pooling as a weighted average of values, where different choices of the attention scoring function lead to different behaviors of attention pooling.\n",
        "* When queries and keys are vectors of different lengths, we can use the additive attention scoring function. When they are the same, the scaled dot-product attention scoring function is more computationally efficient.\n",
        "\n",
        "\n",
        "\n",
        "**Exercises**\n",
        "\n",
        "1. Modify keys in the toy example and visualize attention weights. Do additive attention and scaled dot-product attention still output the same attention weights? Why or why not?\n",
        "1. Using matrix multiplications only, can you design a new scoring function for queries and keys with different vector lengths?\n",
        "1. When queries and keys have the same vector length, is vector summation a better design than dot product for the scoring function? Why or why not?\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "03-attention-scoring-functions.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}