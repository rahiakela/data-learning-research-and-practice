{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-fundamentals/unit08-NLP/01-bag-of-words/02_self_attention_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed46618-9b66-425c-b7a0-f606d4593e2d",
      "metadata": {
        "id": "eed46618-9b66-425c-b7a0-f606d4593e2d"
      },
      "source": [
        "##Self-attention works from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07784975-91ec-4a0b-8761-0f2a1790a2c9",
      "metadata": {
        "id": "07784975-91ec-4a0b-8761-0f2a1790a2c9"
      },
      "source": [
        "**Reference**\n",
        "\n",
        "[Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "kywwhUORWSgA"
      },
      "id": "kywwhUORWSgA"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "YcGxibJO0wM8"
      },
      "id": "YcGxibJO0wM8",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d98f84c6-761a-46cb-a566-ce98779dfb90",
      "metadata": {
        "id": "d98f84c6-761a-46cb-a566-ce98779dfb90"
      },
      "source": [
        "## 1) Embedding Input Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cd96d52b-94bd-4554-9a8c-ff9af76e09a8",
      "metadata": {
        "id": "cd96d52b-94bd-4554-9a8c-ff9af76e09a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9aa465f-f549-454a-d4ea-f8529f408ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "sentence = \"Life is short, eat dessert first\"\n",
        "\n",
        "# create vocab\n",
        "vocab = {w: i for i, w in enumerate(sorted(sentence.replace(\",\", \"\").split()))}\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make integer-vector representation\n",
        "sentence_vec = torch.tensor([vocab[w] for w in sentence.replace(\",\", \"\").split()])\n",
        "sentence_vec"
      ],
      "metadata": {
        "id": "-vCUMM0OYA5k",
        "outputId": "eb9df2ce-7c93-4f35-bff4-2c77188ad096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-vCUMM0OYA5k",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 4, 5, 2, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ec41419-b265-45f9-b1c9-f56aba6fe2f1",
      "metadata": {
        "id": "0ec41419-b265-45f9-b1c9-f56aba6fe2f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77dafb0b-d1c4-4aad-ffc5-19f81ea60f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 16])\n",
            "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
            "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
            "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
            "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
            "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
            "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
            "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
            "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
            "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
            "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
            "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
            "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# let's embed this integer-vector representation\n",
        "embedding = torch.nn.Embedding(6, 16)\n",
        "embedded_sentence = embedding(sentence_vec).detach()\n",
        "\n",
        "print(embedded_sentence.shape)\n",
        "print(embedded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01ee00f-6ac3-40c5-85d2-f5f43566d9a7",
      "metadata": {
        "id": "b01ee00f-6ac3-40c5-85d2-f5f43566d9a7"
      },
      "source": [
        "## 2)  Weight Matrices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "d = embedded_sentence.shape[1]\n",
        "\n",
        "# For computing dot-product between the query and key vectors, it contain the same number of elements\n",
        "d_q, d_k, d_v = 24, 24, 28\n",
        "\n",
        "W_Q = torch.nn.Parameter(torch.rand(d_q, d))\n",
        "W_K = torch.nn.Parameter(torch.rand(d_k, d))\n",
        "W_V = torch.nn.Parameter(torch.rand(d_v, d))"
      ],
      "metadata": {
        "id": "itTmD8VVZweC"
      },
      "id": "itTmD8VVZweC",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_Q.shape, W_K.shape, W_V.shape"
      ],
      "metadata": {
        "id": "5SLyXO8CZqVW",
        "outputId": "095a84a0-6941-4f52-e200-f8d89b0bd1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5SLyXO8CZqVW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([24, 16]), torch.Size([24, 16]), torch.Size([28, 16]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Computing Attention Weights"
      ],
      "metadata": {
        "id": "VqClvV5DY2T0"
      },
      "id": "VqClvV5DY2T0"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "65d50678-3ab0-4075-ab50-3c12e63fd95f",
      "metadata": {
        "id": "65d50678-3ab0-4075-ab50-3c12e63fd95f",
        "outputId": "8f018beb-fcfb-485e-e4be-1b806a6ebed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24])\n",
            "torch.Size([24])\n",
            "torch.Size([28])\n"
          ]
        }
      ],
      "source": [
        "# let's make the second input element acts as the query for computing the attention-vector\n",
        "x_2 = embedded_sentence[1]\n",
        "\n",
        "query_2 = W_Q.matmul(x_2)\n",
        "key_2 = W_K.matmul(x_2)\n",
        "value_2 = W_V.matmul(x_2)\n",
        "\n",
        "print(query_2.shape)\n",
        "print(key_2.shape)\n",
        "print(value_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "03491130-ad33-4fb7-8ed2-12de9662d400",
      "metadata": {
        "id": "03491130-ad33-4fb7-8ed2-12de9662d400",
        "outputId": "3dfae41b-ac43-40a8-83cd-3f9bd6087133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries shape: torch.Size([6, 24])\n",
            "keys shape: torch.Size([6, 24])\n",
            "values shape: torch.Size([6, 28])\n"
          ]
        }
      ],
      "source": [
        "# We can then generalize this to compute th remaining key\n",
        "queries = W_Q.matmul(embedded_sentence.T).T\n",
        "keys = W_K.matmul(embedded_sentence.T).T\n",
        "values = W_V.matmul(embedded_sentence.T).T\n",
        "\n",
        "print(f\"queries shape: {queries.shape}\")\n",
        "print(f\"keys shape: {keys.shape}\")\n",
        "print(f\"values shape: {values.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12dd8956-f4db-410b-94c0-78190a8881cf",
      "metadata": {
        "id": "12dd8956-f4db-410b-94c0-78190a8881cf",
        "outputId": "d0176a2d-5f04-4e4d-eb9b-301ba441cd4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3274,  0.9452,  0.5531,  0.1000, -1.5909,  0.8779,  0.8645,  1.1643,\n",
              "          2.2148,  1.3088,  0.8315, -1.5550, -0.2360,  2.0408, -0.3657,  1.7449,\n",
              "          0.1180,  2.2120, -0.2037, -0.6591, -0.1771, -0.1563,  0.7547,  1.0151],\n",
              "        [-0.0809, -1.2746, -2.3948, -0.3425,  1.5967,  0.5399,  0.9113,  0.0962,\n",
              "          0.7300, -1.0553,  1.2533, -0.2113,  1.0208, -0.7470,  1.5171,  0.2773,\n",
              "         -0.3173,  0.2698,  1.5237, -1.0970,  1.3849,  0.4400, -2.4926,  0.3594],\n",
              "        [-2.1744, -2.7340, -1.0410, -1.8867, -4.0902, -0.3303, -3.1343, -2.4864,\n",
              "         -1.1285, -3.5427, -4.7195, -6.1590, -0.9058, -3.2316, -1.3660, -3.5371,\n",
              "         -2.7504, -2.3356, -1.2755, -3.0702, -2.3168,  0.4209, -2.5422, -3.8700],\n",
              "        [-0.4005,  0.6766, -1.7351,  1.0082, -1.1248, -3.2161,  0.5959, -2.3485,\n",
              "         -1.7592, -1.2618, -1.8644, -0.5954, -1.4934, -3.0239,  0.3801, -1.9108,\n",
              "         -0.6866, -2.1498, -1.9909, -0.8837, -1.6336, -2.1131, -0.4434, -0.1749],\n",
              "        [ 1.1230,  1.3014,  0.7475,  0.2554, -2.3979, -0.9883, -1.1096, -1.3873,\n",
              "          0.9164, -2.3064, -2.7067, -3.1677, -1.4181, -1.0188, -0.8252,  1.0323,\n",
              "         -2.0219, -0.7073, -0.7288, -2.5216, -2.8680, -0.9919, -0.9798, -1.2008],\n",
              "        [ 2.1795,  4.3551,  1.7887,  3.0898,  1.5155,  3.2049,  2.5387,  2.0061,\n",
              "          1.2701,  2.4616,  2.2505,  3.6273,  2.9122,  0.9838,  3.3078,  2.7714,\n",
              "          3.6295,  1.1091,  3.0613,  2.8257,  1.8345,  1.8981,  0.4260,  1.3638]],\n",
              "       grad_fn=<PermuteBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9e42b356-2aad-4447-b714-be0880df3c6c",
      "metadata": {
        "id": "9e42b356-2aad-4447-b714-be0880df3c6c",
        "outputId": "b8bd502c-74cb-42c3-9a9f-ea18d12fc82f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.1466, grad_fn=<DotBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# let's compute the unnormalized attention weight for the query and 5th input element\n",
        "omega_24 = query_2.dot(keys[4])\n",
        "print(omega_24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3f185b5b-b7b2-4660-b6df-8c6faee8bf19",
      "metadata": {
        "id": "3f185b5b-b7b2-4660-b6df-8c6faee8bf19",
        "outputId": "919ebe03-6ec0-4ae6-b6f6-d41625ea356d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "# let’s compute the ω values for all input\n",
        "omega_2 = query_2.matmul(keys.T)\n",
        "print(omega_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s compute the ω values for all query and key\n",
        "omegas = queries.matmul(keys.T)\n",
        "print(omegas)"
      ],
      "metadata": {
        "id": "Yiqyr6mQsOmz",
        "outputId": "c96ced13-39ac-4e5b-d793-1bbd482569bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Yiqyr6mQsOmz",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  16.4255,    8.1306,  -24.5414,  -19.6606,   -9.5164,   19.2777],\n",
            "        [   8.5808,   -7.6597,    3.2558,    1.0395,   11.1466,   -0.4800],\n",
            "        [ -39.2836,   -1.5165,  145.4603,   74.2561,   58.8007, -141.6884],\n",
            "        [  -5.2174,   -4.6914,   74.9203,   30.6948,   35.7423,  -73.7313],\n",
            "        [ -21.6147,   10.6362,   65.4888,   39.2832,   21.8495,  -80.2922],\n",
            "        [  40.0111,   -8.6863, -129.7707,  -64.2901,  -39.9964,  102.5287]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Computing Attention Scores"
      ],
      "metadata": {
        "id": "1p5zLcNOGj8F"
      },
      "id": "1p5zLcNOGj8F"
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights_2 = F.softmax(omega_2 / d_k ** 0.5, dim=0)\n",
        "print(attention_weights_2)"
      ],
      "metadata": {
        "id": "UbozZ3ZSGmjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5827fe26-f3c8-4b35-92f1-cf32f7e69ddb"
      },
      "id": "UbozZ3ZSGmjF",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s compute the attention score for all input\n",
        "attention_weights = F.softmax(omegas / d_k ** 0.5, dim=0)\n",
        "print(attention_weights)"
      ],
      "metadata": {
        "id": "IAWTtYdGtOMH",
        "outputId": "a58af63b-96ca-409d-f996-55b00caae589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IAWTtYdGtOMH",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8.0331e-03, 3.3870e-01, 8.4988e-16, 4.7194e-09, 8.7001e-07, 4.1668e-08],\n",
            "        [1.6198e-03, 1.3490e-02, 2.4750e-13, 3.2281e-07, 5.9061e-05, 7.3838e-10],\n",
            "        [9.2528e-08, 4.7273e-02, 1.0000e+00, 9.9907e-01, 9.9047e-01, 2.2396e-22],\n",
            "        [9.6882e-05, 2.4726e-02, 5.5800e-07, 1.3737e-04, 8.9477e-03, 2.3690e-16],\n",
            "        [3.4089e-06, 5.6487e-01, 8.1381e-08, 7.9295e-04, 5.2495e-04, 6.2078e-17],\n",
            "        [9.9025e-01, 1.0940e-02, 3.9882e-25, 5.2177e-13, 1.7277e-09, 1.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Computing Context Vector"
      ],
      "metadata": {
        "id": "_Cfr8RnLp34-"
      },
      "id": "_Cfr8RnLp34-"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ae181592-134d-4cff-983f-90d6fcae5068",
      "metadata": {
        "id": "ae181592-134d-4cff-983f-90d6fcae5068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af5b361-1c4d-4eb0-87a5-5cf2807fdd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28])\n",
            "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
            "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
            "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
            "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "context_vector_2 = attention_weights_2.matmul(values)\n",
        "\n",
        "print(context_vector_2.shape)\n",
        "print(context_vector_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (1x6)(6x28) = (1x28)\n",
        "attention_weights_2.shape, values.shape, context_vector_2.shape"
      ],
      "metadata": {
        "id": "YfsaCo8jt93L",
        "outputId": "b582b1e1-897a-4dc7-b2f2-0db557daba87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YfsaCo8jt93L",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6]), torch.Size([6, 28]), torch.Size([28]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s compute the context vector for all\n",
        "context_vector = attention_weights.matmul(values)\n",
        "\n",
        "print(context_vector.shape)\n",
        "print(context_vector)"
      ],
      "metadata": {
        "id": "xA-qcIq-tla0",
        "outputId": "d256419f-3fb8-419e-f02e-382aaafc10ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xA-qcIq-tla0",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 28])\n",
            "tensor([[ 1.1567e-02, -1.3905e-02,  4.4249e-01,  4.4479e-01,  5.6050e-01,\n",
            "          1.8654e-01,  1.6815e-01,  9.1608e-02,  7.9842e-02,  5.6846e-02,\n",
            "         -5.6383e-02, -4.1586e-01, -3.0046e-01, -4.4129e-01, -1.2487e-01,\n",
            "         -2.1360e-02,  3.8059e-01,  1.4030e-01, -2.2683e-01,  4.0361e-02,\n",
            "         -3.5513e-01,  1.2286e-01, -1.1625e-01,  2.7429e-01, -3.7994e-01,\n",
            "          1.0395e+00, -9.2950e-02,  5.3336e-01],\n",
            "        [ 1.2364e-03,  1.5917e-03,  2.1337e-02,  1.9573e-02,  2.1137e-02,\n",
            "          8.7816e-03,  5.5823e-03,  3.8541e-03,  5.4790e-03,  4.4451e-03,\n",
            "          5.5909e-04, -1.5194e-02, -9.8078e-03, -1.5460e-02, -2.7285e-03,\n",
            "          1.4838e-04,  1.6907e-02,  5.7196e-03, -8.7047e-03,  1.8788e-03,\n",
            "         -1.0980e-02,  7.3831e-03, -6.1784e-03,  1.1782e-02, -1.5514e-02,\n",
            "          4.2404e-02, -3.0385e-03,  2.2697e-02],\n",
            "        [-6.7802e+00, -3.3392e+00, -3.0078e+00, -2.6895e+00, -1.4771e+00,\n",
            "         -9.8012e+00, -2.7667e+00, -6.6885e+00, -3.6154e+00, -2.4360e-01,\n",
            "         -3.0820e+00, -4.3426e+00, -2.8522e+00, -3.5538e+00, -5.1005e+00,\n",
            "         -7.2690e+00, -3.7278e+00, -8.3951e+00, -5.9503e+00, -7.3190e+00,\n",
            "         -3.8952e+00, -3.9224e+00, -5.4866e+00, -5.0593e+00, -3.8241e+00,\n",
            "         -6.3153e+00, -2.8994e+00,  1.3821e-02],\n",
            "        [-2.7544e-02, -7.4963e-03,  4.3081e-02,  2.5346e-02,  3.3384e-02,\n",
            "         -3.2097e-03,  9.8977e-03, -1.8144e-02,  1.1505e-02,  1.6450e-02,\n",
            "         -5.0612e-03, -4.7903e-02, -2.8997e-02, -2.2593e-02, -1.8805e-02,\n",
            "         -4.0017e-02,  3.4555e-02, -2.3593e-02, -2.7470e-02, -2.3746e-02,\n",
            "         -2.0628e-02, -6.1066e-03, -2.3945e-02,  1.5763e-02, -4.2603e-02,\n",
            "          6.6009e-02, -5.2692e-03,  6.3972e-02],\n",
            "        [ 8.1471e-03, -4.6856e-02,  6.9930e-01,  7.2154e-01,  9.4626e-01,\n",
            "          2.9276e-01,  2.9075e-01,  1.4682e-01,  1.0794e-01,  7.3446e-02,\n",
            "         -1.2462e-01, -7.0970e-01, -5.2412e-01, -7.5864e-01, -2.3416e-01,\n",
            "         -5.0169e-02,  6.1706e-01,  2.2714e-01, -3.8508e-01,  6.0137e-02,\n",
            "         -6.2473e-01,  1.7705e-01, -1.8139e-01,  4.4577e-01, -6.3216e-01,\n",
            "          1.7212e+00, -1.6127e-01,  8.7691e-01],\n",
            "        [ 3.0823e+00,  2.9577e+00,  5.0124e+00,  3.6570e+00,  1.5304e+00,\n",
            "          2.9358e+00,  1.4026e+00,  3.7660e+00, -2.4787e-01,  4.6674e+00,\n",
            "          2.8056e+00,  4.2921e+00,  4.4657e+00,  2.3195e+00,  4.4658e+00,\n",
            "          3.5708e+00,  1.4168e+00,  1.2352e+00,  1.4115e+00,  3.9045e+00,\n",
            "          5.8984e+00,  4.4520e+00,  1.7051e+00,  2.9876e+00,  2.3738e+00,\n",
            "          5.2562e+00,  3.6403e+00,  6.2303e+00]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (6x6)(6x28) = (6x28)\n",
        "attention_weights.shape, values.shape, context_vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxfOCNguqbtF",
        "outputId": "b318ff9e-4052-4a15-b746-3416df10268f"
      },
      "id": "CxfOCNguqbtF",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 6]), torch.Size([6, 28]), torch.Size([6, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6) Multi-Head Attention"
      ],
      "metadata": {
        "id": "yN-Vqe-3rCbg"
      },
      "id": "yN-Vqe-3rCbg"
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose we have 3 attention heads\n",
        "h = 3\n",
        "\n",
        "M_W_Q = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
        "M_W_K = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
        "M_W_V = torch.nn.Parameter(torch.rand(h, d_v, d))\n",
        "\n",
        "print(M_W_Q.shape, M_W_K.shape, M_W_V.shape)"
      ],
      "metadata": {
        "id": "1j_PgpHMrFng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd73678-7b6a-4e4c-9239-7589b65ace7c"
      },
      "id": "1j_PgpHMrFng",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 24, 16]) torch.Size([3, 24, 16]) torch.Size([3, 28, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s keep the focus on the 3rd element\n",
        "multihead_query_2 = M_W_Q.matmul(x_2)\n",
        "print(multihead_query_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btqWaxYpUMJJ",
        "outputId": "38f84cdf-56a1-4897-c783-e9188530712f"
      },
      "id": "btqWaxYpUMJJ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can then obtain the keys and values in a similar fashion\n",
        "multihead_key_2 = M_W_K.matmul(x_2)\n",
        "multihead_value_2 = M_W_V.matmul(x_2)\n",
        "\n",
        "multihead_key_2.shape, multihead_value_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mngmyS2uUgkK",
        "outputId": "0f04360c-0b3b-4092-f4a9-6f4500390c42"
      },
      "id": "mngmyS2uUgkK",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 24]), torch.Size([3, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedded_sentence.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HsrfRVrUuNG",
        "outputId": "3ad8da30-f4f6-46d2-b651-70f5e46745b7"
      },
      "id": "4HsrfRVrUuNG",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expanding the input sequence embeddings to size 3, i.e., the number of attention heads\n",
        "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
        "stacked_inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ4JRvSSVbXF",
        "outputId": "42a72b58-fc51-4dea-a6fc-80f407a0c4da"
      },
      "id": "jQ4JRvSSVbXF",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 16, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can compute all the keys and values\n",
        "multihead_keys = torch.bmm(M_W_K, stacked_inputs)\n",
        "multihead_values = torch.bmm(M_W_V, stacked_inputs)\n",
        "\n",
        "print(f\"multihead_keys shape: {multihead_keys.shape}\")\n",
        "print(f\"multihead_values shape: {multihead_values.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3c0RapJVvHX",
        "outputId": "f4597a89-6e54-48a0-bc54-34ebe1b03b42"
      },
      "id": "q3c0RapJVvHX",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multihead_keys shape: torch.Size([3, 24, 6])\n",
            "multihead_values shape: torch.Size([3, 28, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's swap the second and third dimensions\n",
        "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
        "multihead_values = multihead_values.permute(0, 2, 1)\n",
        "\n",
        "print(f\"multihead_keys shape: {multihead_keys.shape}\")\n",
        "print(f\"multihead_values shape: {multihead_values.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS5L8d-4WPjz",
        "outputId": "5890f3be-8867-43d0-9450-4f216c033ce7"
      },
      "id": "IS5L8d-4WPjz",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multihead_keys shape: torch.Size([3, 6, 24])\n",
            "multihead_values shape: torch.Size([3, 6, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_queries = torch.bmm(M_W_Q, stacked_inputs)\n",
        "print(f\"multihead_queries shape: {multihead_queries.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ASwmBhYBNc",
        "outputId": "f9e03136-2a73-4b60-c237-f971e574e1aa"
      },
      "id": "_2ASwmBhYBNc",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multihead_queries shape: torch.Size([3, 24, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_queries = multihead_queries.permute(1, 0, 2)\n",
        "print(f\"multihead_queries shape: {multihead_queries.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eKSEQdTZyhZ",
        "outputId": "793f0349-8ddc-4af8-e990-32caf130788b"
      },
      "id": "8eKSEQdTZyhZ",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multihead_queries shape: torch.Size([24, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s compute the ω values for all query and key\n",
        "# (3, 24, 6)(3, 6, 24) = (3, 24, 24)\n",
        "#omegas = multihead_queries.bmm(multihead_keys)\n",
        "#print(omegas.shape)"
      ],
      "metadata": {
        "id": "85phxx3LZrjJ"
      },
      "id": "85phxx3LZrjJ",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s compute the attention score for all input\n",
        "attention_weights = F.softmax(omegas / d_k ** 0.5, dim=0)\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kkaObFPZVxP",
        "outputId": "3aa81f2d-bb46-4403-eadf-094212692511"
      },
      "id": "5kkaObFPZVxP",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#attention_weights = attention_weights.permute(0, 2, 1)\n",
        "#print(f\"attention_weights shape: {attention_weights.shape}\")"
      ],
      "metadata": {
        "id": "7Gq4ZdLwbhNe"
      },
      "id": "7Gq4ZdLwbhNe",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s compute the context vector for all\n",
        "# (3, 24, 24)(3, 6, 28) = (3, 24, 24)\n",
        "#context_vector = attention_weights.bmm(multihead_values.T)\n",
        "\n",
        "print(context_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPDeUErbFTc",
        "outputId": "eada6db4-6de5-40f9-eec9-7b44b4adafd6"
      },
      "id": "HaPDeUErbFTc",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7) Cross-Attention"
      ],
      "metadata": {
        "id": "St2JwcEyhOmv"
      },
      "id": "St2JwcEyhOmv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the queries usually come from the decoder, and the keys and values usually come from the encoder."
      ],
      "metadata": {
        "id": "ZLXAK_Q0tVLU"
      },
      "id": "ZLXAK_Q0tVLU"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "d = embedded_sentence.shape[1]\n",
        "print(f\"embedded_sentence shape: {embedded_sentence.shape}\")\n",
        "\n",
        "d_q, d_k, d_v = 24, 24, 28\n",
        "\n",
        "W_Q = torch.nn.Parameter(torch.rand(d_q, d))\n",
        "W_K = torch.nn.Parameter(torch.rand(d_k, d))\n",
        "W_V = torch.nn.Parameter(torch.rand(d_v, d))\n",
        "\n",
        "x_2 = embedded_sentence[1]\n",
        "query_2 = W_Q.matmul(x_2)\n",
        "print(f\"query shape: {query_2.shape}\")\n",
        "\n",
        "keys = W_Q.matmul(embedded_sentence.T).T\n",
        "values = W_V.matmul(embedded_sentence.T).T\n",
        "print(f\"keys shape: {keys.shape}\")\n",
        "print(f\"values shape: {values.shape}\")"
      ],
      "metadata": {
        "id": "ZXa7dVhvhQhO",
        "outputId": "938b1b56-6389-4353-fbe6-b95bcf19d711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZXa7dVhvhQhO",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedded_sentence shape: torch.Size([6, 16])\n",
            "query shape: torch.Size([24])\n",
            "keys shape: torch.Size([6, 24])\n",
            "values shape: torch.Size([6, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only part that changes in cross attention is that we now have a second input sequence, for example, a second sentence with 8 instead of 6 input elements."
      ],
      "metadata": {
        "id": "frlhSE-FvQqm"
      },
      "id": "frlhSE-FvQqm"
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_sentence_2 = torch.rand(8, 16)  # 2nd input sequence\n",
        "\n",
        "keys = W_Q.matmul(embedded_sentence_2.T).T\n",
        "values = W_V.matmul(embedded_sentence_2.T).T\n",
        "print(f\"keys shape: {keys.shape}\")\n",
        "print(f\"values shape: {values.shape}\")"
      ],
      "metadata": {
        "id": "fD1-JIQOvRYF",
        "outputId": "f5b3c723-02a7-4833-9feb-33f3cfd6dbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fD1-JIQOvRYF",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys shape: torch.Size([8, 24])\n",
            "values shape: torch.Size([8, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that compared to self-attention, the keys and values now have 8 instead of 6 rows. Everything else stays the same."
      ],
      "metadata": {
        "id": "qrl2hVIevn4h"
      },
      "id": "qrl2hVIevn4h"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}