{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-fundamentals/unit03-model-training/03_exercise_banknotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dc571a-e16a-47fa-b456-c4d9367b87a5",
      "metadata": {
        "id": "29dc571a-e16a-47fa-b456-c4d9367b87a5"
      },
      "source": [
        "## Exercise: Banknote Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f20142-5f08-4b33-a4e5-ec1315c5a8ff",
      "metadata": {
        "id": "48f20142-5f08-4b33-a4e5-ec1315c5a8ff"
      },
      "source": [
        "In this exercise, we are applying logistic regression to a banknote authentication dataset to distinguish between genuine and forged bank notes.\n",
        "\n",
        "\n",
        "**The dataset consists of 1372 examples and 4 features for binary classification.** The features are \n",
        "\n",
        "1. variance of a wavelet-transformed image (continuous) \n",
        "2. skewness of a wavelet-transformed image (continuous) \n",
        "3. kurtosis of a wavelet-transformed image (continuous) \n",
        "4. entropy of the image (continuous) \n",
        "\n",
        "(You can fine more details about this dataset at [https://archive.ics.uci.edu/ml/datasets/banknote+authentication](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).)\n",
        "\n",
        "\n",
        "In essence, these four features represent features that were manually extracted from image data. Note that you do not need the details of these features for this exercise. \n",
        "\n",
        "However, you are encouraged to explore the dataset further, e.g., by plotting the features, looking at the value ranges, and so forth. (We will skip these steps for brevity in this exercise)\n",
        "\n",
        "Most of the code should look familiar to you since it is based on the logistic regression code from Unit 3.6."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setup"
      ],
      "metadata": {
        "id": "u9oYPc6lsFfv"
      },
      "id": "u9oYPc6lsFfv"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "zYQjCYG_sIm6"
      },
      "id": "zYQjCYG_sIm6",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Lightning-AI/dl-fundamentals/raw/main/unit03-pytorch-training/exercises/1_banknotes/data_banknote_authentication.txt"
      ],
      "metadata": {
        "id": "68sNlMgJtsX4",
        "outputId": "e7743681-412a-410a-8106-49e28c8b8185",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "68sNlMgJtsX4",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-14 12:44:46--  https://github.com/Lightning-AI/dl-fundamentals/raw/main/unit03-pytorch-training/exercises/1_banknotes/data_banknote_authentication.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Lightning-AI/dl-fundamentals/main/unit03-pytorch-training/exercises/1_banknotes/data_banknote_authentication.txt [following]\n",
            "--2023-02-14 12:44:46--  https://raw.githubusercontent.com/Lightning-AI/dl-fundamentals/main/unit03-pytorch-training/exercises/1_banknotes/data_banknote_authentication.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46400 (45K) [text/plain]\n",
            "Saving to: ‘data_banknote_authentication.txt’\n",
            "\n",
            "\r          data_bank   0%[                    ]       0  --.-KB/s               \rdata_banknote_authe 100%[===================>]  45.31K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-02-14 12:44:46 (57.3 MB/s) - ‘data_banknote_authentication.txt’ saved [46400/46400]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157c6970-2b47-49a1-ba50-59bf738526ce",
      "metadata": {
        "tags": [],
        "id": "157c6970-2b47-49a1-ba50-59bf738526ce"
      },
      "source": [
        "## 2) Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6638725c-02ee-44db-b661-d882dd191185",
      "metadata": {
        "id": "6638725c-02ee-44db-b661-d882dd191185"
      },
      "source": [
        "We are using the familiar `read_csv` function from pandas to load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bb8965ae-5222-4541-a7c6-7a9aaa4d1033",
      "metadata": {
        "id": "bb8965ae-5222-4541-a7c6-7a9aaa4d1033",
        "outputId": "8e7a8e63-d577-4eb0-bc7b-d335fb45d051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0       1       2        3  4\n",
              "0  3.62160  8.6661 -2.8073 -0.44699  0\n",
              "1  4.54590  8.1674 -2.4586 -1.46210  0\n",
              "2  3.86600 -2.6383  1.9242  0.10645  0\n",
              "3  3.45660  9.5228 -4.0112 -3.59440  0\n",
              "4  0.32924 -4.4552  4.5718 -0.98880  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d68f9667-14e2-4025-9154-6febd319bcfd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d68f9667-14e2-4025-9154-6febd319bcfd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d68f9667-14e2-4025-9154-6febd319bcfd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d68f9667-14e2-4025-9154-6febd319bcfd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv(\"data_banknote_authentication.txt\", header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "860304f1-1b8c-4993-b547-20e2dcceb03b",
      "metadata": {
        "id": "860304f1-1b8c-4993-b547-20e2dcceb03b"
      },
      "outputs": [],
      "source": [
        "X_features = df[[0, 1, 2, 3]].values\n",
        "y_labels = df[4].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d2ebb2-d83f-4729-85ed-9437e105b9b8",
      "metadata": {
        "id": "a4d2ebb2-d83f-4729-85ed-9437e105b9b8"
      },
      "source": [
        "Number of examples and features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f342b22-0fde-436a-a121-00e9ce627512",
      "metadata": {
        "id": "6f342b22-0fde-436a-a121-00e9ce627512",
        "outputId": "812b26fe-a70a-4ff1-fecd-acc76f079b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1372, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8e94b9-4847-4833-a7d1-afee3c18991a",
      "metadata": {
        "id": "7c8e94b9-4847-4833-a7d1-afee3c18991a"
      },
      "source": [
        "It is usually a good idea to look at the label distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1e8247a8-101d-4195-84d3-12b6593c0099",
      "metadata": {
        "id": "1e8247a8-101d-4195-84d3-12b6593c0099",
        "outputId": "d7fb17e4-8c32-443f-f288-62c799b8c02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([762, 610])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "np.bincount(y_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f7e7578-c57d-4aae-99fc-77603e202185",
      "metadata": {
        "id": "6f7e7578-c57d-4aae-99fc-77603e202185"
      },
      "source": [
        "## 3) Defining a DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "705b52c8-1635-40c8-a6f3-8c4d0d91952e",
      "metadata": {
        "id": "705b52c8-1635-40c8-a6f3-8c4d0d91952e"
      },
      "source": [
        "The `DataLoader` code is the same code we used in Unit 3.6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fa4ba92f-f294-4572-8aa2-d2fa50788a9d",
      "metadata": {
        "id": "fa4ba92f-f294-4572-8aa2-d2fa50788a9d"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.features = torch.tensor(X, dtype=torch.float32)\n",
        "    self.labels = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.features[index]\n",
        "    y = self.labels[index]        \n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2096f23-539a-40e0-affa-db0ffcd0f371",
      "metadata": {
        "id": "e2096f23-539a-40e0-affa-db0ffcd0f371"
      },
      "source": [
        "We will be using 80% of the data for training, 20% of the data for validation. \n",
        "\n",
        "In a real-project, we would also have a separate dataset for the final test set (in this case, we do not have an explicit test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f8bbd768-c15c-40f5-8500-83fad5bb1722",
      "metadata": {
        "id": "f8bbd768-c15c-40f5-8500-83fad5bb1722",
        "outputId": "79dd6f5e-048c-4ba7-d9c0-7643fafc82e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1097"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_size = int(X_features.shape[0]*0.80)\n",
        "train_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b09d3d5-f4f7-47df-8160-8d883c0c5e19",
      "metadata": {
        "id": "0b09d3d5-f4f7-47df-8160-8d883c0c5e19",
        "outputId": "e5caeb9c-39db-48ac-f66d-d41baabe5ef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "val_size = X_features.shape[0] - train_size\n",
        "val_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e21d88e-1408-4457-a7fd-3306a9fac5a6",
      "metadata": {
        "id": "3e21d88e-1408-4457-a7fd-3306a9fac5a6"
      },
      "source": [
        "Using `torch.utils.data.random_split`, we generate the training and validation sets along with the respective data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "22a0e19a-de40-4309-b197-368a781a5633",
      "metadata": {
        "id": "22a0e19a-de40-4309-b197-368a781a5633"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "dataset = MyDataset(X_features, y_labels)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "  dataset=train_set,\n",
        "  batch_size=10,\n",
        "  shuffle=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "  dataset=val_set,\n",
        "  batch_size=10,\n",
        "  shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Standardization"
      ],
      "metadata": {
        "id": "LjUmH6ds9pdd"
      },
      "id": "LjUmH6ds9pdd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple ways to implement the standardization procedure. For this exercise, we are going to implement a procedure that standardizes the features after we created the data loader.\n",
        "\n",
        "Since this dataset has 4 features, there should be 4 means and 4 standard deviations we compute from the training set. We can do this as follows:"
      ],
      "metadata": {
        "id": "CiKI6t9L9p5g"
      },
      "id": "CiKI6t9L9p5g"
    },
    {
      "cell_type": "code",
      "source": [
        "train_mean = torch.zeros(X_features.shape[1])\n",
        "\n",
        "for x, y in train_loader:\n",
        "  train_mean += x.sum(dim=0)\n",
        "train_mean /= len(train_set)\n",
        "\n",
        "train_std = torch.zeros(X_features.shape[1])\n",
        "for x, y in train_loader:\n",
        "  train_std += ((x - train_mean) ** 2).sum(dim=0)\n",
        "train_std = torch.sqrt(train_std / len(train_set))"
      ],
      "metadata": {
        "id": "s_t-82mM9rsv"
      },
      "id": "s_t-82mM9rsv",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature means:\", train_mean)\n",
        "print(\"Feature std. devs:\", train_std)"
      ],
      "metadata": {
        "id": "coMdmb8e_DPB",
        "outputId": "d278df79-2a30-42f7-8661-ebf53aa0c0f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "coMdmb8e_DPB",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature means: tensor([ 0.3854,  1.8680,  1.4923, -1.1999])\n",
            "Feature std. devs: tensor([2.8562, 5.9189, 4.3849, 2.1031])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the means and standard deviations by iterating over the training loader. This is an approach that even works for large datasets where the entire dataset doesn't fit into memory. \n",
        "\n",
        "A simpler approach, which only works for smaller datasets that fit into memory, is as follows:"
      ],
      "metadata": {
        "id": "I6r80Ier_LBX"
      },
      "id": "I6r80Ier_LBX"
    },
    {
      "cell_type": "code",
      "source": [
        "all_x = []\n",
        "\n",
        "for x, y in train_loader:\n",
        "  all_x.append(x)\n",
        "\n",
        "train_std = torch.concat(all_x).std(dim=0)\n",
        "train_mean = torch.concat(all_x).mean(dim=0)\n",
        "\n",
        "print(\"Feature means:\", train_mean)\n",
        "print(\"Feature std. devs:\", train_std)"
      ],
      "metadata": {
        "id": "9eit8o0b_S5u",
        "outputId": "34983888-84cd-4b3a-8272-6a23cc2f5ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9eit8o0b_S5u",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature means: tensor([ 0.3854,  1.8680,  1.4923, -1.1999])\n",
            "Feature std. devs: tensor([2.8575, 5.9216, 4.3869, 2.1041])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>YOUR TASK is now to implement a standardization function based on these training set parameters above:</font>"
      ],
      "metadata": {
        "id": "-I025kBD_ujn"
      },
      "id": "-I025kBD_ujn"
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(df, mean, std):\n",
        "  return (df - mean) / std"
      ],
      "metadata": {
        "id": "fFc3Dt7d_u87"
      },
      "id": "fFc3Dt7d_u87",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e3a0b2f5-66f5-45e5-9b0a-f4960fc40388",
      "metadata": {
        "id": "e3a0b2f5-66f5-45e5-9b0a-f4960fc40388"
      },
      "source": [
        "## 5) Implementing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee409f0-02e0-4591-abf1-5e2c6c41a187",
      "metadata": {
        "id": "dee409f0-02e0-4591-abf1-5e2c6c41a187"
      },
      "source": [
        "Here, we are resusing the same model code we used in Unit 3.6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3da86d9a-7cd5-467c-bf65-3388fe272bd5",
      "metadata": {
        "id": "3da86d9a-7cd5-467c-bf65-3388fe272bd5"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    \n",
        "  def __init__(self, num_features):\n",
        "    super().__init__()\n",
        "    self.linear = torch.nn.Linear(num_features, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    logits = self.linear(x)\n",
        "    probas = torch.sigmoid(logits)\n",
        "    return probas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8340676-a3da-49cf-aeae-c0a3329734c5",
      "metadata": {
        "id": "c8340676-a3da-49cf-aeae-c0a3329734c5"
      },
      "source": [
        "## 6) The training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1660ca15-18d5-4a55-94f1-e9f543bd8748",
      "metadata": {
        "id": "1660ca15-18d5-4a55-94f1-e9f543bd8748"
      },
      "source": [
        "In this section, we are using the training loop from Unit 3.6. It's the exact same code except for some small modification: We added the line `if not batch_idx % 20` to only print the lost for every 20th batch (to reduce the number of output lines).\n",
        "\n",
        "<font color='red'>YOUR TASK is to use the standardization code correctly in the for loop. Then, find a good learning rate and epoch number to that you achieve a training and validation performance of at least 98%.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "79c712f6-4e2a-43e9-8563-215f88beb4a8",
      "metadata": {
        "id": "79c712f6-4e2a-43e9-8563-215f88beb4a8",
        "outputId": "56c427a7-b683-47fd-a6bf-b0d739fe22ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/100 | Batch 000/110 | Loss: 0.93\n",
            "Epoch: 001/100 | Batch 020/110 | Loss: 0.85\n",
            "Epoch: 001/100 | Batch 040/110 | Loss: 0.76\n",
            "Epoch: 001/100 | Batch 060/110 | Loss: 0.63\n",
            "Epoch: 001/100 | Batch 080/110 | Loss: 0.63\n",
            "Epoch: 001/100 | Batch 100/110 | Loss: 0.65\n",
            "Epoch: 002/100 | Batch 000/110 | Loss: 0.68\n",
            "Epoch: 002/100 | Batch 020/110 | Loss: 0.53\n",
            "Epoch: 002/100 | Batch 040/110 | Loss: 0.56\n",
            "Epoch: 002/100 | Batch 060/110 | Loss: 0.55\n",
            "Epoch: 002/100 | Batch 080/110 | Loss: 0.44\n",
            "Epoch: 002/100 | Batch 100/110 | Loss: 0.42\n",
            "Epoch: 003/100 | Batch 000/110 | Loss: 0.43\n",
            "Epoch: 003/100 | Batch 020/110 | Loss: 0.52\n",
            "Epoch: 003/100 | Batch 040/110 | Loss: 0.53\n",
            "Epoch: 003/100 | Batch 060/110 | Loss: 0.57\n",
            "Epoch: 003/100 | Batch 080/110 | Loss: 0.38\n",
            "Epoch: 003/100 | Batch 100/110 | Loss: 0.48\n",
            "Epoch: 004/100 | Batch 000/110 | Loss: 0.38\n",
            "Epoch: 004/100 | Batch 020/110 | Loss: 0.29\n",
            "Epoch: 004/100 | Batch 040/110 | Loss: 0.48\n",
            "Epoch: 004/100 | Batch 060/110 | Loss: 0.41\n",
            "Epoch: 004/100 | Batch 080/110 | Loss: 0.29\n",
            "Epoch: 004/100 | Batch 100/110 | Loss: 0.32\n",
            "Epoch: 005/100 | Batch 000/110 | Loss: 0.55\n",
            "Epoch: 005/100 | Batch 020/110 | Loss: 0.37\n",
            "Epoch: 005/100 | Batch 040/110 | Loss: 0.33\n",
            "Epoch: 005/100 | Batch 060/110 | Loss: 0.33\n",
            "Epoch: 005/100 | Batch 080/110 | Loss: 0.42\n",
            "Epoch: 005/100 | Batch 100/110 | Loss: 0.40\n",
            "Epoch: 006/100 | Batch 000/110 | Loss: 0.31\n",
            "Epoch: 006/100 | Batch 020/110 | Loss: 0.33\n",
            "Epoch: 006/100 | Batch 040/110 | Loss: 0.40\n",
            "Epoch: 006/100 | Batch 060/110 | Loss: 0.37\n",
            "Epoch: 006/100 | Batch 080/110 | Loss: 0.30\n",
            "Epoch: 006/100 | Batch 100/110 | Loss: 0.45\n",
            "Epoch: 007/100 | Batch 000/110 | Loss: 0.31\n",
            "Epoch: 007/100 | Batch 020/110 | Loss: 0.17\n",
            "Epoch: 007/100 | Batch 040/110 | Loss: 0.37\n",
            "Epoch: 007/100 | Batch 060/110 | Loss: 0.18\n",
            "Epoch: 007/100 | Batch 080/110 | Loss: 0.42\n",
            "Epoch: 007/100 | Batch 100/110 | Loss: 0.29\n",
            "Epoch: 008/100 | Batch 000/110 | Loss: 0.27\n",
            "Epoch: 008/100 | Batch 020/110 | Loss: 0.31\n",
            "Epoch: 008/100 | Batch 040/110 | Loss: 0.28\n",
            "Epoch: 008/100 | Batch 060/110 | Loss: 0.34\n",
            "Epoch: 008/100 | Batch 080/110 | Loss: 0.24\n",
            "Epoch: 008/100 | Batch 100/110 | Loss: 0.39\n",
            "Epoch: 009/100 | Batch 000/110 | Loss: 0.40\n",
            "Epoch: 009/100 | Batch 020/110 | Loss: 0.34\n",
            "Epoch: 009/100 | Batch 040/110 | Loss: 0.17\n",
            "Epoch: 009/100 | Batch 060/110 | Loss: 0.24\n",
            "Epoch: 009/100 | Batch 080/110 | Loss: 0.26\n",
            "Epoch: 009/100 | Batch 100/110 | Loss: 0.26\n",
            "Epoch: 010/100 | Batch 000/110 | Loss: 0.29\n",
            "Epoch: 010/100 | Batch 020/110 | Loss: 0.24\n",
            "Epoch: 010/100 | Batch 040/110 | Loss: 0.23\n",
            "Epoch: 010/100 | Batch 060/110 | Loss: 0.19\n",
            "Epoch: 010/100 | Batch 080/110 | Loss: 0.27\n",
            "Epoch: 010/100 | Batch 100/110 | Loss: 0.11\n",
            "Epoch: 011/100 | Batch 000/110 | Loss: 0.19\n",
            "Epoch: 011/100 | Batch 020/110 | Loss: 0.15\n",
            "Epoch: 011/100 | Batch 040/110 | Loss: 0.36\n",
            "Epoch: 011/100 | Batch 060/110 | Loss: 0.25\n",
            "Epoch: 011/100 | Batch 080/110 | Loss: 0.29\n",
            "Epoch: 011/100 | Batch 100/110 | Loss: 0.20\n",
            "Epoch: 012/100 | Batch 000/110 | Loss: 0.25\n",
            "Epoch: 012/100 | Batch 020/110 | Loss: 0.17\n",
            "Epoch: 012/100 | Batch 040/110 | Loss: 0.21\n",
            "Epoch: 012/100 | Batch 060/110 | Loss: 0.18\n",
            "Epoch: 012/100 | Batch 080/110 | Loss: 0.25\n",
            "Epoch: 012/100 | Batch 100/110 | Loss: 0.17\n",
            "Epoch: 013/100 | Batch 000/110 | Loss: 0.20\n",
            "Epoch: 013/100 | Batch 020/110 | Loss: 0.21\n",
            "Epoch: 013/100 | Batch 040/110 | Loss: 0.25\n",
            "Epoch: 013/100 | Batch 060/110 | Loss: 0.22\n",
            "Epoch: 013/100 | Batch 080/110 | Loss: 0.33\n",
            "Epoch: 013/100 | Batch 100/110 | Loss: 0.42\n",
            "Epoch: 014/100 | Batch 000/110 | Loss: 0.23\n",
            "Epoch: 014/100 | Batch 020/110 | Loss: 0.23\n",
            "Epoch: 014/100 | Batch 040/110 | Loss: 0.16\n",
            "Epoch: 014/100 | Batch 060/110 | Loss: 0.29\n",
            "Epoch: 014/100 | Batch 080/110 | Loss: 0.16\n",
            "Epoch: 014/100 | Batch 100/110 | Loss: 0.23\n",
            "Epoch: 015/100 | Batch 000/110 | Loss: 0.24\n",
            "Epoch: 015/100 | Batch 020/110 | Loss: 0.13\n",
            "Epoch: 015/100 | Batch 040/110 | Loss: 0.15\n",
            "Epoch: 015/100 | Batch 060/110 | Loss: 0.11\n",
            "Epoch: 015/100 | Batch 080/110 | Loss: 0.20\n",
            "Epoch: 015/100 | Batch 100/110 | Loss: 0.38\n",
            "Epoch: 016/100 | Batch 000/110 | Loss: 0.14\n",
            "Epoch: 016/100 | Batch 020/110 | Loss: 0.16\n",
            "Epoch: 016/100 | Batch 040/110 | Loss: 0.20\n",
            "Epoch: 016/100 | Batch 060/110 | Loss: 0.23\n",
            "Epoch: 016/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 016/100 | Batch 100/110 | Loss: 0.20\n",
            "Epoch: 017/100 | Batch 000/110 | Loss: 0.12\n",
            "Epoch: 017/100 | Batch 020/110 | Loss: 0.34\n",
            "Epoch: 017/100 | Batch 040/110 | Loss: 0.10\n",
            "Epoch: 017/100 | Batch 060/110 | Loss: 0.27\n",
            "Epoch: 017/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 017/100 | Batch 100/110 | Loss: 0.21\n",
            "Epoch: 018/100 | Batch 000/110 | Loss: 0.19\n",
            "Epoch: 018/100 | Batch 020/110 | Loss: 0.19\n",
            "Epoch: 018/100 | Batch 040/110 | Loss: 0.14\n",
            "Epoch: 018/100 | Batch 060/110 | Loss: 0.22\n",
            "Epoch: 018/100 | Batch 080/110 | Loss: 0.21\n",
            "Epoch: 018/100 | Batch 100/110 | Loss: 0.11\n",
            "Epoch: 019/100 | Batch 000/110 | Loss: 0.16\n",
            "Epoch: 019/100 | Batch 020/110 | Loss: 0.24\n",
            "Epoch: 019/100 | Batch 040/110 | Loss: 0.13\n",
            "Epoch: 019/100 | Batch 060/110 | Loss: 0.16\n",
            "Epoch: 019/100 | Batch 080/110 | Loss: 0.12\n",
            "Epoch: 019/100 | Batch 100/110 | Loss: 0.21\n",
            "Epoch: 020/100 | Batch 000/110 | Loss: 0.22\n",
            "Epoch: 020/100 | Batch 020/110 | Loss: 0.16\n",
            "Epoch: 020/100 | Batch 040/110 | Loss: 0.19\n",
            "Epoch: 020/100 | Batch 060/110 | Loss: 0.16\n",
            "Epoch: 020/100 | Batch 080/110 | Loss: 0.12\n",
            "Epoch: 020/100 | Batch 100/110 | Loss: 0.21\n",
            "Epoch: 021/100 | Batch 000/110 | Loss: 0.29\n",
            "Epoch: 021/100 | Batch 020/110 | Loss: 0.13\n",
            "Epoch: 021/100 | Batch 040/110 | Loss: 0.21\n",
            "Epoch: 021/100 | Batch 060/110 | Loss: 0.11\n",
            "Epoch: 021/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 021/100 | Batch 100/110 | Loss: 0.23\n",
            "Epoch: 022/100 | Batch 000/110 | Loss: 0.18\n",
            "Epoch: 022/100 | Batch 020/110 | Loss: 0.24\n",
            "Epoch: 022/100 | Batch 040/110 | Loss: 0.14\n",
            "Epoch: 022/100 | Batch 060/110 | Loss: 0.23\n",
            "Epoch: 022/100 | Batch 080/110 | Loss: 0.26\n",
            "Epoch: 022/100 | Batch 100/110 | Loss: 0.12\n",
            "Epoch: 023/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 023/100 | Batch 020/110 | Loss: 0.25\n",
            "Epoch: 023/100 | Batch 040/110 | Loss: 0.14\n",
            "Epoch: 023/100 | Batch 060/110 | Loss: 0.18\n",
            "Epoch: 023/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 023/100 | Batch 100/110 | Loss: 0.19\n",
            "Epoch: 024/100 | Batch 000/110 | Loss: 0.22\n",
            "Epoch: 024/100 | Batch 020/110 | Loss: 0.30\n",
            "Epoch: 024/100 | Batch 040/110 | Loss: 0.09\n",
            "Epoch: 024/100 | Batch 060/110 | Loss: 0.16\n",
            "Epoch: 024/100 | Batch 080/110 | Loss: 0.21\n",
            "Epoch: 024/100 | Batch 100/110 | Loss: 0.16\n",
            "Epoch: 025/100 | Batch 000/110 | Loss: 0.10\n",
            "Epoch: 025/100 | Batch 020/110 | Loss: 0.12\n",
            "Epoch: 025/100 | Batch 040/110 | Loss: 0.16\n",
            "Epoch: 025/100 | Batch 060/110 | Loss: 0.18\n",
            "Epoch: 025/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 025/100 | Batch 100/110 | Loss: 0.11\n",
            "Epoch: 026/100 | Batch 000/110 | Loss: 0.18\n",
            "Epoch: 026/100 | Batch 020/110 | Loss: 0.15\n",
            "Epoch: 026/100 | Batch 040/110 | Loss: 0.23\n",
            "Epoch: 026/100 | Batch 060/110 | Loss: 0.18\n",
            "Epoch: 026/100 | Batch 080/110 | Loss: 0.21\n",
            "Epoch: 026/100 | Batch 100/110 | Loss: 0.15\n",
            "Epoch: 027/100 | Batch 000/110 | Loss: 0.08\n",
            "Epoch: 027/100 | Batch 020/110 | Loss: 0.08\n",
            "Epoch: 027/100 | Batch 040/110 | Loss: 0.13\n",
            "Epoch: 027/100 | Batch 060/110 | Loss: 0.22\n",
            "Epoch: 027/100 | Batch 080/110 | Loss: 0.10\n",
            "Epoch: 027/100 | Batch 100/110 | Loss: 0.08\n",
            "Epoch: 028/100 | Batch 000/110 | Loss: 0.28\n",
            "Epoch: 028/100 | Batch 020/110 | Loss: 0.14\n",
            "Epoch: 028/100 | Batch 040/110 | Loss: 0.19\n",
            "Epoch: 028/100 | Batch 060/110 | Loss: 0.19\n",
            "Epoch: 028/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 028/100 | Batch 100/110 | Loss: 0.20\n",
            "Epoch: 029/100 | Batch 000/110 | Loss: 0.23\n",
            "Epoch: 029/100 | Batch 020/110 | Loss: 0.13\n",
            "Epoch: 029/100 | Batch 040/110 | Loss: 0.18\n",
            "Epoch: 029/100 | Batch 060/110 | Loss: 0.12\n",
            "Epoch: 029/100 | Batch 080/110 | Loss: 0.16\n",
            "Epoch: 029/100 | Batch 100/110 | Loss: 0.08\n",
            "Epoch: 030/100 | Batch 000/110 | Loss: 0.28\n",
            "Epoch: 030/100 | Batch 020/110 | Loss: 0.08\n",
            "Epoch: 030/100 | Batch 040/110 | Loss: 0.10\n",
            "Epoch: 030/100 | Batch 060/110 | Loss: 0.17\n",
            "Epoch: 030/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 030/100 | Batch 100/110 | Loss: 0.09\n",
            "Epoch: 031/100 | Batch 000/110 | Loss: 0.17\n",
            "Epoch: 031/100 | Batch 020/110 | Loss: 0.16\n",
            "Epoch: 031/100 | Batch 040/110 | Loss: 0.19\n",
            "Epoch: 031/100 | Batch 060/110 | Loss: 0.15\n",
            "Epoch: 031/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 031/100 | Batch 100/110 | Loss: 0.18\n",
            "Epoch: 032/100 | Batch 000/110 | Loss: 0.04\n",
            "Epoch: 032/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 032/100 | Batch 040/110 | Loss: 0.21\n",
            "Epoch: 032/100 | Batch 060/110 | Loss: 0.07\n",
            "Epoch: 032/100 | Batch 080/110 | Loss: 0.10\n",
            "Epoch: 032/100 | Batch 100/110 | Loss: 0.24\n",
            "Epoch: 033/100 | Batch 000/110 | Loss: 0.15\n",
            "Epoch: 033/100 | Batch 020/110 | Loss: 0.21\n",
            "Epoch: 033/100 | Batch 040/110 | Loss: 0.27\n",
            "Epoch: 033/100 | Batch 060/110 | Loss: 0.10\n",
            "Epoch: 033/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 033/100 | Batch 100/110 | Loss: 0.19\n",
            "Epoch: 034/100 | Batch 000/110 | Loss: 0.13\n",
            "Epoch: 034/100 | Batch 020/110 | Loss: 0.09\n",
            "Epoch: 034/100 | Batch 040/110 | Loss: 0.11\n",
            "Epoch: 034/100 | Batch 060/110 | Loss: 0.16\n",
            "Epoch: 034/100 | Batch 080/110 | Loss: 0.11\n",
            "Epoch: 034/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 035/100 | Batch 000/110 | Loss: 0.14\n",
            "Epoch: 035/100 | Batch 020/110 | Loss: 0.14\n",
            "Epoch: 035/100 | Batch 040/110 | Loss: 0.06\n",
            "Epoch: 035/100 | Batch 060/110 | Loss: 0.12\n",
            "Epoch: 035/100 | Batch 080/110 | Loss: 0.13\n",
            "Epoch: 035/100 | Batch 100/110 | Loss: 0.09\n",
            "Epoch: 036/100 | Batch 000/110 | Loss: 0.15\n",
            "Epoch: 036/100 | Batch 020/110 | Loss: 0.09\n",
            "Epoch: 036/100 | Batch 040/110 | Loss: 0.09\n",
            "Epoch: 036/100 | Batch 060/110 | Loss: 0.08\n",
            "Epoch: 036/100 | Batch 080/110 | Loss: 0.19\n",
            "Epoch: 036/100 | Batch 100/110 | Loss: 0.16\n",
            "Epoch: 037/100 | Batch 000/110 | Loss: 0.14\n",
            "Epoch: 037/100 | Batch 020/110 | Loss: 0.06\n",
            "Epoch: 037/100 | Batch 040/110 | Loss: 0.12\n",
            "Epoch: 037/100 | Batch 060/110 | Loss: 0.06\n",
            "Epoch: 037/100 | Batch 080/110 | Loss: 0.09\n",
            "Epoch: 037/100 | Batch 100/110 | Loss: 0.31\n",
            "Epoch: 038/100 | Batch 000/110 | Loss: 0.18\n",
            "Epoch: 038/100 | Batch 020/110 | Loss: 0.05\n",
            "Epoch: 038/100 | Batch 040/110 | Loss: 0.14\n",
            "Epoch: 038/100 | Batch 060/110 | Loss: 0.12\n",
            "Epoch: 038/100 | Batch 080/110 | Loss: 0.17\n",
            "Epoch: 038/100 | Batch 100/110 | Loss: 0.15\n",
            "Epoch: 039/100 | Batch 000/110 | Loss: 0.25\n",
            "Epoch: 039/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 039/100 | Batch 040/110 | Loss: 0.29\n",
            "Epoch: 039/100 | Batch 060/110 | Loss: 0.06\n",
            "Epoch: 039/100 | Batch 080/110 | Loss: 0.12\n",
            "Epoch: 039/100 | Batch 100/110 | Loss: 0.17\n",
            "Epoch: 040/100 | Batch 000/110 | Loss: 0.28\n",
            "Epoch: 040/100 | Batch 020/110 | Loss: 0.11\n",
            "Epoch: 040/100 | Batch 040/110 | Loss: 0.23\n",
            "Epoch: 040/100 | Batch 060/110 | Loss: 0.09\n",
            "Epoch: 040/100 | Batch 080/110 | Loss: 0.16\n",
            "Epoch: 040/100 | Batch 100/110 | Loss: 0.14\n",
            "Epoch: 041/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 041/100 | Batch 020/110 | Loss: 0.17\n",
            "Epoch: 041/100 | Batch 040/110 | Loss: 0.14\n",
            "Epoch: 041/100 | Batch 060/110 | Loss: 0.12\n",
            "Epoch: 041/100 | Batch 080/110 | Loss: 0.36\n",
            "Epoch: 041/100 | Batch 100/110 | Loss: 0.14\n",
            "Epoch: 042/100 | Batch 000/110 | Loss: 0.17\n",
            "Epoch: 042/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 042/100 | Batch 040/110 | Loss: 0.07\n",
            "Epoch: 042/100 | Batch 060/110 | Loss: 0.09\n",
            "Epoch: 042/100 | Batch 080/110 | Loss: 0.22\n",
            "Epoch: 042/100 | Batch 100/110 | Loss: 0.13\n",
            "Epoch: 043/100 | Batch 000/110 | Loss: 0.06\n",
            "Epoch: 043/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 043/100 | Batch 040/110 | Loss: 0.22\n",
            "Epoch: 043/100 | Batch 060/110 | Loss: 0.08\n",
            "Epoch: 043/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 043/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 044/100 | Batch 000/110 | Loss: 0.12\n",
            "Epoch: 044/100 | Batch 020/110 | Loss: 0.16\n",
            "Epoch: 044/100 | Batch 040/110 | Loss: 0.07\n",
            "Epoch: 044/100 | Batch 060/110 | Loss: 0.11\n",
            "Epoch: 044/100 | Batch 080/110 | Loss: 0.21\n",
            "Epoch: 044/100 | Batch 100/110 | Loss: 0.06\n",
            "Epoch: 045/100 | Batch 000/110 | Loss: 0.12\n",
            "Epoch: 045/100 | Batch 020/110 | Loss: 0.22\n",
            "Epoch: 045/100 | Batch 040/110 | Loss: 0.16\n",
            "Epoch: 045/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 045/100 | Batch 080/110 | Loss: 0.20\n",
            "Epoch: 045/100 | Batch 100/110 | Loss: 0.13\n",
            "Epoch: 046/100 | Batch 000/110 | Loss: 0.07\n",
            "Epoch: 046/100 | Batch 020/110 | Loss: 0.11\n",
            "Epoch: 046/100 | Batch 040/110 | Loss: 0.12\n",
            "Epoch: 046/100 | Batch 060/110 | Loss: 0.05\n",
            "Epoch: 046/100 | Batch 080/110 | Loss: 0.09\n",
            "Epoch: 046/100 | Batch 100/110 | Loss: 0.09\n",
            "Epoch: 047/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 047/100 | Batch 020/110 | Loss: 0.12\n",
            "Epoch: 047/100 | Batch 040/110 | Loss: 0.28\n",
            "Epoch: 047/100 | Batch 060/110 | Loss: 0.06\n",
            "Epoch: 047/100 | Batch 080/110 | Loss: 0.10\n",
            "Epoch: 047/100 | Batch 100/110 | Loss: 0.20\n",
            "Epoch: 048/100 | Batch 000/110 | Loss: 0.07\n",
            "Epoch: 048/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 048/100 | Batch 040/110 | Loss: 0.20\n",
            "Epoch: 048/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 048/100 | Batch 080/110 | Loss: 0.09\n",
            "Epoch: 048/100 | Batch 100/110 | Loss: 0.14\n",
            "Epoch: 049/100 | Batch 000/110 | Loss: 0.07\n",
            "Epoch: 049/100 | Batch 020/110 | Loss: 0.07\n",
            "Epoch: 049/100 | Batch 040/110 | Loss: 0.22\n",
            "Epoch: 049/100 | Batch 060/110 | Loss: 0.15\n",
            "Epoch: 049/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 049/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 050/100 | Batch 000/110 | Loss: 0.16\n",
            "Epoch: 050/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 050/100 | Batch 040/110 | Loss: 0.22\n",
            "Epoch: 050/100 | Batch 060/110 | Loss: 0.06\n",
            "Epoch: 050/100 | Batch 080/110 | Loss: 0.10\n",
            "Epoch: 050/100 | Batch 100/110 | Loss: 0.08\n",
            "Epoch: 051/100 | Batch 000/110 | Loss: 0.08\n",
            "Epoch: 051/100 | Batch 020/110 | Loss: 0.04\n",
            "Epoch: 051/100 | Batch 040/110 | Loss: 0.18\n",
            "Epoch: 051/100 | Batch 060/110 | Loss: 0.07\n",
            "Epoch: 051/100 | Batch 080/110 | Loss: 0.06\n",
            "Epoch: 051/100 | Batch 100/110 | Loss: 0.28\n",
            "Epoch: 052/100 | Batch 000/110 | Loss: 0.16\n",
            "Epoch: 052/100 | Batch 020/110 | Loss: 0.17\n",
            "Epoch: 052/100 | Batch 040/110 | Loss: 0.06\n",
            "Epoch: 052/100 | Batch 060/110 | Loss: 0.07\n",
            "Epoch: 052/100 | Batch 080/110 | Loss: 0.30\n",
            "Epoch: 052/100 | Batch 100/110 | Loss: 0.10\n",
            "Epoch: 053/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 053/100 | Batch 020/110 | Loss: 0.18\n",
            "Epoch: 053/100 | Batch 040/110 | Loss: 0.10\n",
            "Epoch: 053/100 | Batch 060/110 | Loss: 0.12\n",
            "Epoch: 053/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 053/100 | Batch 100/110 | Loss: 0.09\n",
            "Epoch: 054/100 | Batch 000/110 | Loss: 0.08\n",
            "Epoch: 054/100 | Batch 020/110 | Loss: 0.07\n",
            "Epoch: 054/100 | Batch 040/110 | Loss: 0.17\n",
            "Epoch: 054/100 | Batch 060/110 | Loss: 0.15\n",
            "Epoch: 054/100 | Batch 080/110 | Loss: 0.09\n",
            "Epoch: 054/100 | Batch 100/110 | Loss: 0.10\n",
            "Epoch: 055/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 055/100 | Batch 020/110 | Loss: 0.08\n",
            "Epoch: 055/100 | Batch 040/110 | Loss: 0.06\n",
            "Epoch: 055/100 | Batch 060/110 | Loss: 0.09\n",
            "Epoch: 055/100 | Batch 080/110 | Loss: 0.13\n",
            "Epoch: 055/100 | Batch 100/110 | Loss: 0.18\n",
            "Epoch: 056/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 056/100 | Batch 020/110 | Loss: 0.18\n",
            "Epoch: 056/100 | Batch 040/110 | Loss: 0.11\n",
            "Epoch: 056/100 | Batch 060/110 | Loss: 0.20\n",
            "Epoch: 056/100 | Batch 080/110 | Loss: 0.06\n",
            "Epoch: 056/100 | Batch 100/110 | Loss: 0.04\n",
            "Epoch: 057/100 | Batch 000/110 | Loss: 0.07\n",
            "Epoch: 057/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 057/100 | Batch 040/110 | Loss: 0.25\n",
            "Epoch: 057/100 | Batch 060/110 | Loss: 0.16\n",
            "Epoch: 057/100 | Batch 080/110 | Loss: 0.16\n",
            "Epoch: 057/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 058/100 | Batch 000/110 | Loss: 0.16\n",
            "Epoch: 058/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 058/100 | Batch 040/110 | Loss: 0.09\n",
            "Epoch: 058/100 | Batch 060/110 | Loss: 0.08\n",
            "Epoch: 058/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 058/100 | Batch 100/110 | Loss: 0.08\n",
            "Epoch: 059/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 059/100 | Batch 020/110 | Loss: 0.08\n",
            "Epoch: 059/100 | Batch 040/110 | Loss: 0.09\n",
            "Epoch: 059/100 | Batch 060/110 | Loss: 0.21\n",
            "Epoch: 059/100 | Batch 080/110 | Loss: 0.09\n",
            "Epoch: 059/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 060/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 060/100 | Batch 020/110 | Loss: 0.29\n",
            "Epoch: 060/100 | Batch 040/110 | Loss: 0.15\n",
            "Epoch: 060/100 | Batch 060/110 | Loss: 0.12\n",
            "Epoch: 060/100 | Batch 080/110 | Loss: 0.10\n",
            "Epoch: 060/100 | Batch 100/110 | Loss: 0.10\n",
            "Epoch: 061/100 | Batch 000/110 | Loss: 0.11\n",
            "Epoch: 061/100 | Batch 020/110 | Loss: 0.04\n",
            "Epoch: 061/100 | Batch 040/110 | Loss: 0.17\n",
            "Epoch: 061/100 | Batch 060/110 | Loss: 0.20\n",
            "Epoch: 061/100 | Batch 080/110 | Loss: 0.11\n",
            "Epoch: 061/100 | Batch 100/110 | Loss: 0.09\n",
            "Epoch: 062/100 | Batch 000/110 | Loss: 0.08\n",
            "Epoch: 062/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 062/100 | Batch 040/110 | Loss: 0.18\n",
            "Epoch: 062/100 | Batch 060/110 | Loss: 0.06\n",
            "Epoch: 062/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 062/100 | Batch 100/110 | Loss: 0.15\n",
            "Epoch: 063/100 | Batch 000/110 | Loss: 0.03\n",
            "Epoch: 063/100 | Batch 020/110 | Loss: 0.15\n",
            "Epoch: 063/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 063/100 | Batch 060/110 | Loss: 0.30\n",
            "Epoch: 063/100 | Batch 080/110 | Loss: 0.11\n",
            "Epoch: 063/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 064/100 | Batch 000/110 | Loss: 0.10\n",
            "Epoch: 064/100 | Batch 020/110 | Loss: 0.08\n",
            "Epoch: 064/100 | Batch 040/110 | Loss: 0.08\n",
            "Epoch: 064/100 | Batch 060/110 | Loss: 0.08\n",
            "Epoch: 064/100 | Batch 080/110 | Loss: 0.04\n",
            "Epoch: 064/100 | Batch 100/110 | Loss: 0.20\n",
            "Epoch: 065/100 | Batch 000/110 | Loss: 0.11\n",
            "Epoch: 065/100 | Batch 020/110 | Loss: 0.05\n",
            "Epoch: 065/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 065/100 | Batch 060/110 | Loss: 0.10\n",
            "Epoch: 065/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 065/100 | Batch 100/110 | Loss: 0.08\n",
            "Epoch: 066/100 | Batch 000/110 | Loss: 0.15\n",
            "Epoch: 066/100 | Batch 020/110 | Loss: 0.11\n",
            "Epoch: 066/100 | Batch 040/110 | Loss: 0.14\n",
            "Epoch: 066/100 | Batch 060/110 | Loss: 0.11\n",
            "Epoch: 066/100 | Batch 080/110 | Loss: 0.06\n",
            "Epoch: 066/100 | Batch 100/110 | Loss: 0.10\n",
            "Epoch: 067/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 067/100 | Batch 020/110 | Loss: 0.05\n",
            "Epoch: 067/100 | Batch 040/110 | Loss: 0.18\n",
            "Epoch: 067/100 | Batch 060/110 | Loss: 0.05\n",
            "Epoch: 067/100 | Batch 080/110 | Loss: 0.17\n",
            "Epoch: 067/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 068/100 | Batch 000/110 | Loss: 0.17\n",
            "Epoch: 068/100 | Batch 020/110 | Loss: 0.24\n",
            "Epoch: 068/100 | Batch 040/110 | Loss: 0.25\n",
            "Epoch: 068/100 | Batch 060/110 | Loss: 0.16\n",
            "Epoch: 068/100 | Batch 080/110 | Loss: 0.07\n",
            "Epoch: 068/100 | Batch 100/110 | Loss: 0.11\n",
            "Epoch: 069/100 | Batch 000/110 | Loss: 0.06\n",
            "Epoch: 069/100 | Batch 020/110 | Loss: 0.07\n",
            "Epoch: 069/100 | Batch 040/110 | Loss: 0.06\n",
            "Epoch: 069/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 069/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 069/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 070/100 | Batch 000/110 | Loss: 0.04\n",
            "Epoch: 070/100 | Batch 020/110 | Loss: 0.05\n",
            "Epoch: 070/100 | Batch 040/110 | Loss: 0.06\n",
            "Epoch: 070/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 070/100 | Batch 080/110 | Loss: 0.08\n",
            "Epoch: 070/100 | Batch 100/110 | Loss: 0.15\n",
            "Epoch: 071/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 071/100 | Batch 020/110 | Loss: 0.15\n",
            "Epoch: 071/100 | Batch 040/110 | Loss: 0.13\n",
            "Epoch: 071/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 071/100 | Batch 080/110 | Loss: 0.07\n",
            "Epoch: 071/100 | Batch 100/110 | Loss: 0.06\n",
            "Epoch: 072/100 | Batch 000/110 | Loss: 0.10\n",
            "Epoch: 072/100 | Batch 020/110 | Loss: 0.09\n",
            "Epoch: 072/100 | Batch 040/110 | Loss: 0.05\n",
            "Epoch: 072/100 | Batch 060/110 | Loss: 0.03\n",
            "Epoch: 072/100 | Batch 080/110 | Loss: 0.18\n",
            "Epoch: 072/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 073/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 073/100 | Batch 020/110 | Loss: 0.07\n",
            "Epoch: 073/100 | Batch 040/110 | Loss: 0.19\n",
            "Epoch: 073/100 | Batch 060/110 | Loss: 0.06\n",
            "Epoch: 073/100 | Batch 080/110 | Loss: 0.26\n",
            "Epoch: 073/100 | Batch 100/110 | Loss: 0.04\n",
            "Epoch: 074/100 | Batch 000/110 | Loss: 0.04\n",
            "Epoch: 074/100 | Batch 020/110 | Loss: 0.17\n",
            "Epoch: 074/100 | Batch 040/110 | Loss: 0.08\n",
            "Epoch: 074/100 | Batch 060/110 | Loss: 0.11\n",
            "Epoch: 074/100 | Batch 080/110 | Loss: 0.08\n",
            "Epoch: 074/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 075/100 | Batch 000/110 | Loss: 0.08\n",
            "Epoch: 075/100 | Batch 020/110 | Loss: 0.05\n",
            "Epoch: 075/100 | Batch 040/110 | Loss: 0.12\n",
            "Epoch: 075/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 075/100 | Batch 080/110 | Loss: 0.03\n",
            "Epoch: 075/100 | Batch 100/110 | Loss: 0.06\n",
            "Epoch: 076/100 | Batch 000/110 | Loss: 0.13\n",
            "Epoch: 076/100 | Batch 020/110 | Loss: 0.09\n",
            "Epoch: 076/100 | Batch 040/110 | Loss: 0.08\n",
            "Epoch: 076/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 076/100 | Batch 080/110 | Loss: 0.26\n",
            "Epoch: 076/100 | Batch 100/110 | Loss: 0.14\n",
            "Epoch: 077/100 | Batch 000/110 | Loss: 0.14\n",
            "Epoch: 077/100 | Batch 020/110 | Loss: 0.05\n",
            "Epoch: 077/100 | Batch 040/110 | Loss: 0.22\n",
            "Epoch: 077/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 077/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 077/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 078/100 | Batch 000/110 | Loss: 0.04\n",
            "Epoch: 078/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 078/100 | Batch 040/110 | Loss: 0.15\n",
            "Epoch: 078/100 | Batch 060/110 | Loss: 0.25\n",
            "Epoch: 078/100 | Batch 080/110 | Loss: 0.18\n",
            "Epoch: 078/100 | Batch 100/110 | Loss: 0.04\n",
            "Epoch: 079/100 | Batch 000/110 | Loss: 0.03\n",
            "Epoch: 079/100 | Batch 020/110 | Loss: 0.18\n",
            "Epoch: 079/100 | Batch 040/110 | Loss: 0.03\n",
            "Epoch: 079/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 079/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 079/100 | Batch 100/110 | Loss: 0.04\n",
            "Epoch: 080/100 | Batch 000/110 | Loss: 0.07\n",
            "Epoch: 080/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 080/100 | Batch 040/110 | Loss: 0.05\n",
            "Epoch: 080/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 080/100 | Batch 080/110 | Loss: 0.08\n",
            "Epoch: 080/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 081/100 | Batch 000/110 | Loss: 0.04\n",
            "Epoch: 081/100 | Batch 020/110 | Loss: 0.13\n",
            "Epoch: 081/100 | Batch 040/110 | Loss: 0.15\n",
            "Epoch: 081/100 | Batch 060/110 | Loss: 0.05\n",
            "Epoch: 081/100 | Batch 080/110 | Loss: 0.11\n",
            "Epoch: 081/100 | Batch 100/110 | Loss: 0.06\n",
            "Epoch: 082/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 082/100 | Batch 020/110 | Loss: 0.06\n",
            "Epoch: 082/100 | Batch 040/110 | Loss: 0.09\n",
            "Epoch: 082/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 082/100 | Batch 080/110 | Loss: 0.16\n",
            "Epoch: 082/100 | Batch 100/110 | Loss: 0.16\n",
            "Epoch: 083/100 | Batch 000/110 | Loss: 0.04\n",
            "Epoch: 083/100 | Batch 020/110 | Loss: 0.11\n",
            "Epoch: 083/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 083/100 | Batch 060/110 | Loss: 0.09\n",
            "Epoch: 083/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 083/100 | Batch 100/110 | Loss: 0.04\n",
            "Epoch: 084/100 | Batch 000/110 | Loss: 0.13\n",
            "Epoch: 084/100 | Batch 020/110 | Loss: 0.03\n",
            "Epoch: 084/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 084/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 084/100 | Batch 080/110 | Loss: 0.05\n",
            "Epoch: 084/100 | Batch 100/110 | Loss: 0.14\n",
            "Epoch: 085/100 | Batch 000/110 | Loss: 0.03\n",
            "Epoch: 085/100 | Batch 020/110 | Loss: 0.04\n",
            "Epoch: 085/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 085/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 085/100 | Batch 080/110 | Loss: 0.16\n",
            "Epoch: 085/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 086/100 | Batch 000/110 | Loss: 0.07\n",
            "Epoch: 086/100 | Batch 020/110 | Loss: 0.17\n",
            "Epoch: 086/100 | Batch 040/110 | Loss: 0.02\n",
            "Epoch: 086/100 | Batch 060/110 | Loss: 0.11\n",
            "Epoch: 086/100 | Batch 080/110 | Loss: 0.03\n",
            "Epoch: 086/100 | Batch 100/110 | Loss: 0.11\n",
            "Epoch: 087/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 087/100 | Batch 020/110 | Loss: 0.04\n",
            "Epoch: 087/100 | Batch 040/110 | Loss: 0.11\n",
            "Epoch: 087/100 | Batch 060/110 | Loss: 0.09\n",
            "Epoch: 087/100 | Batch 080/110 | Loss: 0.09\n",
            "Epoch: 087/100 | Batch 100/110 | Loss: 0.18\n",
            "Epoch: 088/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 088/100 | Batch 020/110 | Loss: 0.06\n",
            "Epoch: 088/100 | Batch 040/110 | Loss: 0.05\n",
            "Epoch: 088/100 | Batch 060/110 | Loss: 0.03\n",
            "Epoch: 088/100 | Batch 080/110 | Loss: 0.06\n",
            "Epoch: 088/100 | Batch 100/110 | Loss: 0.11\n",
            "Epoch: 089/100 | Batch 000/110 | Loss: 0.08\n",
            "Epoch: 089/100 | Batch 020/110 | Loss: 0.10\n",
            "Epoch: 089/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 089/100 | Batch 060/110 | Loss: 0.13\n",
            "Epoch: 089/100 | Batch 080/110 | Loss: 0.11\n",
            "Epoch: 089/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 090/100 | Batch 000/110 | Loss: 0.11\n",
            "Epoch: 090/100 | Batch 020/110 | Loss: 0.16\n",
            "Epoch: 090/100 | Batch 040/110 | Loss: 0.11\n",
            "Epoch: 090/100 | Batch 060/110 | Loss: 0.21\n",
            "Epoch: 090/100 | Batch 080/110 | Loss: 0.04\n",
            "Epoch: 090/100 | Batch 100/110 | Loss: 0.08\n",
            "Epoch: 091/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 091/100 | Batch 020/110 | Loss: 0.09\n",
            "Epoch: 091/100 | Batch 040/110 | Loss: 0.15\n",
            "Epoch: 091/100 | Batch 060/110 | Loss: 0.03\n",
            "Epoch: 091/100 | Batch 080/110 | Loss: 0.12\n",
            "Epoch: 091/100 | Batch 100/110 | Loss: 0.36\n",
            "Epoch: 092/100 | Batch 000/110 | Loss: 0.06\n",
            "Epoch: 092/100 | Batch 020/110 | Loss: 0.06\n",
            "Epoch: 092/100 | Batch 040/110 | Loss: 0.10\n",
            "Epoch: 092/100 | Batch 060/110 | Loss: 0.08\n",
            "Epoch: 092/100 | Batch 080/110 | Loss: 0.03\n",
            "Epoch: 092/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 093/100 | Batch 000/110 | Loss: 0.06\n",
            "Epoch: 093/100 | Batch 020/110 | Loss: 0.09\n",
            "Epoch: 093/100 | Batch 040/110 | Loss: 0.05\n",
            "Epoch: 093/100 | Batch 060/110 | Loss: 0.05\n",
            "Epoch: 093/100 | Batch 080/110 | Loss: 0.02\n",
            "Epoch: 093/100 | Batch 100/110 | Loss: 0.10\n",
            "Epoch: 094/100 | Batch 000/110 | Loss: 0.12\n",
            "Epoch: 094/100 | Batch 020/110 | Loss: 0.04\n",
            "Epoch: 094/100 | Batch 040/110 | Loss: 0.10\n",
            "Epoch: 094/100 | Batch 060/110 | Loss: 0.05\n",
            "Epoch: 094/100 | Batch 080/110 | Loss: 0.06\n",
            "Epoch: 094/100 | Batch 100/110 | Loss: 0.01\n",
            "Epoch: 095/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 095/100 | Batch 020/110 | Loss: 0.12\n",
            "Epoch: 095/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 095/100 | Batch 060/110 | Loss: 0.08\n",
            "Epoch: 095/100 | Batch 080/110 | Loss: 0.14\n",
            "Epoch: 095/100 | Batch 100/110 | Loss: 0.05\n",
            "Epoch: 096/100 | Batch 000/110 | Loss: 0.09\n",
            "Epoch: 096/100 | Batch 020/110 | Loss: 0.06\n",
            "Epoch: 096/100 | Batch 040/110 | Loss: 0.05\n",
            "Epoch: 096/100 | Batch 060/110 | Loss: 0.10\n",
            "Epoch: 096/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 096/100 | Batch 100/110 | Loss: 0.03\n",
            "Epoch: 097/100 | Batch 000/110 | Loss: 0.16\n",
            "Epoch: 097/100 | Batch 020/110 | Loss: 0.15\n",
            "Epoch: 097/100 | Batch 040/110 | Loss: 0.07\n",
            "Epoch: 097/100 | Batch 060/110 | Loss: 0.04\n",
            "Epoch: 097/100 | Batch 080/110 | Loss: 0.15\n",
            "Epoch: 097/100 | Batch 100/110 | Loss: 0.04\n",
            "Epoch: 098/100 | Batch 000/110 | Loss: 0.05\n",
            "Epoch: 098/100 | Batch 020/110 | Loss: 0.12\n",
            "Epoch: 098/100 | Batch 040/110 | Loss: 0.03\n",
            "Epoch: 098/100 | Batch 060/110 | Loss: 0.07\n",
            "Epoch: 098/100 | Batch 080/110 | Loss: 0.03\n",
            "Epoch: 098/100 | Batch 100/110 | Loss: 0.07\n",
            "Epoch: 099/100 | Batch 000/110 | Loss: 0.03\n",
            "Epoch: 099/100 | Batch 020/110 | Loss: 0.07\n",
            "Epoch: 099/100 | Batch 040/110 | Loss: 0.26\n",
            "Epoch: 099/100 | Batch 060/110 | Loss: 0.09\n",
            "Epoch: 099/100 | Batch 080/110 | Loss: 0.07\n",
            "Epoch: 099/100 | Batch 100/110 | Loss: 0.19\n",
            "Epoch: 100/100 | Batch 000/110 | Loss: 0.17\n",
            "Epoch: 100/100 | Batch 020/110 | Loss: 0.03\n",
            "Epoch: 100/100 | Batch 040/110 | Loss: 0.04\n",
            "Epoch: 100/100 | Batch 060/110 | Loss: 0.27\n",
            "Epoch: 100/100 | Batch 080/110 | Loss: 0.04\n",
            "Epoch: 100/100 | Batch 100/110 | Loss: 0.17\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "model = LogisticRegression(num_features=4)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) ## FILL IN VALUE\n",
        "\n",
        "num_epochs = 100  ## FILL IN VALUE\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model = model.train()\n",
        "  for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
        "    features = standardize(features, train_mean, train_std) ## SOLUTION\n",
        "    probas = model(features)\n",
        "    \n",
        "    loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    ### LOGGING\n",
        "    if not batch_idx % 20: # log every 20th batch\n",
        "        print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n",
        "                f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n",
        "                f' | Loss: {loss:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41396cca-8920-4edd-9075-588c03d81f01",
      "metadata": {
        "id": "41396cca-8920-4edd-9075-588c03d81f01"
      },
      "source": [
        "## 7) Evaluating the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f458e2cc-011c-48e5-b66a-5ef568114242",
      "metadata": {
        "id": "f458e2cc-011c-48e5-b66a-5ef568114242"
      },
      "source": [
        "Again, reusing the code from Unit 3.6, we will calculate the training and validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4b6473aa-98ac-4ffe-84b5-cb5a2d511018",
      "metadata": {
        "id": "4b6473aa-98ac-4ffe-84b5-cb5a2d511018"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "\n",
        "  model = model.eval()\n",
        "  \n",
        "  correct = 0.0\n",
        "  total_examples = 0\n",
        "  \n",
        "  for idx, (features, class_labels) in enumerate(dataloader):\n",
        "    with torch.no_grad():\n",
        "      probas = model(features)\n",
        "    \n",
        "    pred = torch.where(probas > 0.5, 1, 0)\n",
        "    lab = class_labels.view(pred.shape).to(pred.dtype)\n",
        "\n",
        "    compare = lab == pred\n",
        "    correct += torch.sum(compare)\n",
        "    total_examples += len(compare)\n",
        "\n",
        "  return correct / total_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "70de800b-4138-49ac-b4cc-e89605b78a3d",
      "metadata": {
        "id": "70de800b-4138-49ac-b4cc-e89605b78a3d",
        "outputId": "4ca624ca-e9a4-40c7-b9fc-8a8f2e2cf7c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 86.69%\n"
          ]
        }
      ],
      "source": [
        "train_acc = compute_accuracy(model, train_loader)\n",
        "print(f\"Accuracy: {train_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01e464b-cc38-41b7-9d7f-6baafba73f56",
      "metadata": {
        "id": "c01e464b-cc38-41b7-9d7f-6baafba73f56"
      },
      "source": [
        "<font color='red'>Notice that the code validation accuracy is not shown? It's part of the exercise to implement it :)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7edead56-db64-4667-8007-937ab1974ac0",
      "metadata": {
        "id": "7edead56-db64-4667-8007-937ab1974ac0",
        "outputId": "1fa86c6b-1c7b-4f64-ebf8-6ce96065a795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 82.18%\n"
          ]
        }
      ],
      "source": [
        "val_acc = compute_accuracy(model, val_loader)\n",
        "print(f\"Accuracy: {val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Now, add the standardization to the `compute_accuracy` function above and recompute the training and validation accuracy. What do you observe?</font>"
      ],
      "metadata": {
        "id": "lDilWbjMDA-d"
      },
      "id": "lDilWbjMDA-d"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "\n",
        "  model = model.eval()\n",
        "  \n",
        "  correct = 0.0\n",
        "  total_examples = 0\n",
        "  \n",
        "  for idx, (features, class_labels) in enumerate(dataloader):\n",
        "    with torch.no_grad():\n",
        "      features = standardize(features, train_mean, train_std) ## SOLUTION\n",
        "      probas = model(features)\n",
        "    \n",
        "    pred = torch.where(probas > 0.5, 1, 0)\n",
        "    lab = class_labels.view(pred.shape).to(pred.dtype)\n",
        "\n",
        "    compare = lab == pred\n",
        "    correct += torch.sum(compare)\n",
        "    total_examples += len(compare)\n",
        "\n",
        "  return correct / total_examples"
      ],
      "metadata": {
        "id": "_KT36o-jDBm2"
      },
      "id": "_KT36o-jDBm2",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc = compute_accuracy(model, train_loader)\n",
        "print(f\"Accuracy: {train_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "8b7xYcqHDMlc",
        "outputId": "188444f5-1e9d-4bba-b46a-ec62800bc143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8b7xYcqHDMlc",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc = compute_accuracy(model, val_loader)\n",
        "print(f\"Accuracy: {val_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "wGjZKt19DPNV",
        "outputId": "7088f55e-694e-4195-8270-a363ad4cf4ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wGjZKt19DPNV",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.18%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}