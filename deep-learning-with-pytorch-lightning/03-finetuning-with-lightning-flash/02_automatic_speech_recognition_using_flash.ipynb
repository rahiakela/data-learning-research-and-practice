{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPR64pKBYgZLOUaY5gD/8lD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a95aa8ae1a854ece8670c4e377064d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_561619d2e0a54ae9accf89177c4152d5",
              "IPY_MODEL_6ead40f680e440f084530b3f19f26ccd",
              "IPY_MODEL_996a76887e4c4302a9ec633ba4f9abb9"
            ],
            "layout": "IPY_MODEL_90329988b9534b13a4dd3ad0cc31023d"
          }
        },
        "561619d2e0a54ae9accf89177c4152d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65b7fc62a96e41bb9ca427434a1e6cf0",
            "placeholder": "​",
            "style": "IPY_MODEL_6cb8879b5f8b4fd3b2b86c9fae11b9ee",
            "value": "Validation sanity check:   0%"
          }
        },
        "6ead40f680e440f084530b3f19f26ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f2b7d7e4e242bb91751ec3ae37623f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a736d1fb034446f281960cb36c72457f",
            "value": 0
          }
        },
        "996a76887e4c4302a9ec633ba4f9abb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93d48ae89a540bda323559fc2fac1cd",
            "placeholder": "​",
            "style": "IPY_MODEL_6a28ac3abb04427382d34dd91de44987",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "90329988b9534b13a4dd3ad0cc31023d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "65b7fc62a96e41bb9ca427434a1e6cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb8879b5f8b4fd3b2b86c9fae11b9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61f2b7d7e4e242bb91751ec3ae37623f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a736d1fb034446f281960cb36c72457f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a93d48ae89a540bda323559fc2fac1cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a28ac3abb04427382d34dd91de44987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-lightning/03-finetuning-with-lightning-flash/02_automatic_speech_recognition_using_flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Automatic speech recognition using Flash"
      ],
      "metadata": {
        "id": "ydFRNrn6lZNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recognizing speech from an audio file is perhaps one of the most widely used applications\n",
        "of AI. It's part of smartphone speakers such as Alexa, as well as automatically generated\n",
        "captions for video streaming platforms such as YouTube, and also many music platforms.\n",
        "\n",
        "It can detect speech in an audio file and convert it into text. Detection of speech involves\n",
        "various challenges such as speaker modalities, pitch, and pronunciation, as well as dialect\n",
        "and language itself.\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABO0AAAEeCAYAAADW5eEOAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAEFkSURBVHhe7d0JlGRXfef5f2Tse25VWXtlrSrVppIKJIQACQM2R3jaGrslM4gZCxuEDdjGjDG0GTQCNR5o2Ry6LXAb4wZzwOOBti2PBzVGtC0ZaKEVbVWqfd9zi4zM2Jec+L+8UZVVuWdGZLwX8f3UiZP3vohc4mVWZsQv/vf+XWMVAgAAAAAAAMA22sxbAAAAAAAAADZBaAcAAAAAAADYDKEdAAAAAAAAYDOEdgAAAAAAAIDNENoBAAAAAAAANkNoBwAAAAAAANgMoR0AAAAAAABgM4R2AAAAAAAAgM0Q2gEAAAAAAAA2Q2gHAAAAAAAA2AyhHQAAAAAAAGAzhHYAAAAAAACAzRDaAQAAAAAAADZDaAcAAAAAAADYDKEdAAAAAAAAYDOEdgAAAAAAAIDNENoBAAAAAAAANkNoBwAAAAAAANiMa6zCjAEANnJitCg/G8zL4WRRLmVLki7x6xrz421zyYqAW9aF3XJTl0+uj3vNNQAAAADsjtAOAGzo60dG5bGzWTMDamN7u1f+YEdUwh4K7QEAAAC7I7QDABs5my7KVw+lZP9wwRwBamttyC2/sSUiO9qpugMAAADsjNAOAGzk488NyYlUycyA+vncjXHZGiO4AwAAAOyK9TEAYBM/uZQjsMOS+eqhUcmXed0OAAAAsCtCOwCwiUdPpc0IqL+TqVLlZy5jZgAAAADshtAOAGzgULIgp9JU2WFpffdk2upMDAAAAMB+CO0AwAYePZ0RViqiEV4cpOkJAAAAYEeEdgBgA/sSBCdojNOpohkBAAAAsBNCOwCwgXSRMjs0ximanwAAAAC2RGgHAAAAAAAA2AyhHQAAAAAAAGAzhHYAAAAAAACAzRDaAQAAAAAAADZDaAcAAAAAAADYDKEdAAAAAAAAYDOEdgAAAAAAAIDNENoBAAAAAAAANkNoBwAAAAAAANgMoR0AAAAAAABgM4R2AAAAAAAAgM0Q2gEAAAAAAAA2Q2gHAAAAAAAA2AyhHQAAAAAAAGAzrrEKMwawSMViUcqlkuTyOSkWClKu/Pey3pbLV64342t5vV5xuVzWxevziUuPVd6629zi8/vF4/GM3xBN6e4n+80IWFrb4175zJ64mQEAAACwC0I7YBHy+ZyMJEdkeHhIhhMJc7R+IpGoRKJRCYVCEm/vMEfRDAjt0CiEdgAAAIA9EdoBc1AqFSWdTkumcslmM5LL5iRTeVsqFs0tGkMr8Pz+gAQC45dgMCTBUEja2lj57jSEdmgUQjsAAADAngjtgGnoMtaR5LBcunhRRkdHzFH7c7vd0tnVLV2ViwZ4cAZCOzQKoR0AAABgT4R2wAS5nC53HZZUalSGh4cbXkm3WB6P11pOGw6HK5eIhCMRcw3shtAOjUJoBwAAANgToR1Qoctez58/uyT70jWSVuH1rFhpXWAvhHZoFEI7AAAAwJ4I7dCyNKgbHk5IYmhIMpm0OdoaNLzTphad3d0Si8XZA88GCO3QKIR2AAAAgD0R2qGlaDOJ4cSQDA4OSD6XM0cRjcUlFotZHWn9fr85iqVEaIdGIbQDAAAA7InyGrSMgYF+OfjaPrlw/hyB3TV0H7+zZ07La/tekZMnjlvdcgEAAAAAQOMQ2qGpaQfYvksX5cD+fXLqxHFzFNPRwtvBgX7Z/+qrVng3MpI01wAAAAAAgKVEaIempYHTqy+/KGdOn2q5PesWq1gsWOHdkUMH5fChA5LNZsw1AAAAAABgKRDaoemkUqNy4thROXr4kJRKJXMUCzU6MmJVKp47e4ZlxQAAAAAALBFCOzSV8+fOyqEDr8nQ0KC11BO1oefy4oXzsu/Vl61zDAAAAAAA6ovQDk1Bl8IePnjAajKB+tJzrJV3/X2XrD0DAQCtKVsak0vZkiQLS/+3QD+vXkq8QAcAAJqYa4xyJDicVn4R1jWG2+2WLVu3STAUMkewUHc/2W9GwNLaHvfKZ/bEzQyYmycuZOXrR1OSLo4/jLyjxy8f2BoRX5vLmtfLvkRB/sO+5OXP2xt2y4N72iXsqe/nBQAAaARCOziWVnlpYHfp4gVzBI2gwV1HR6cs6+mRQCBojmK+CO3QKIR2mK/vnEjJd09OblC0q90rD9xQv58lraz71AsJSRSufuja7muTT+6MyaaoxxwBlkaxWJISqw4Ax3O5XOJua6s8r2EhIuyH0A6OVCgU5LV9r9Bowkb0j92GjZsk3t5hjmA+CO3QKIR2mK9f+8nA5Uq3az28t116I/UJz/7+VFr++vjU3eDfsTIg92+NmBlQXyOjGRlNZ80MQLPQ5zPRSFDCQb85AjQeUTIcRTPmRGJIDh18jcDOZvR7c+zoETl65JDk83lzFADQTA4lC9MGdupsun5/m8/N8LHr+XmBqlQ6Kxf6EgR2QJPS5zPJkbT0DQxLNlcwR4HGIrSDY2h13bGjh+X40SOSz+XMUdhNcnhYDux/Vc6dPU14BwBNZraeE/kyCzjQfMqVn+v+oaQkRzPWk3oAza1YKsvQ8KiMpgjo0XiEdnCEbDZjLYfVQAj2p1WQFy9ckMNURAIAAIfL5vJSKPB4Bmg1I6lM5bkM+1aisQjtYHuZdNpadkn44zxaaffKSz+zuvtq4xAAAACnIbADWle+UDQjoDEI7WBrun/dgdf2SS5LabJT6TIS7fJ76MBr1hJnAAAAJxmr/APQmlgSj0YjtINtaWB3+uRJM4PTZTJpOXLooPV95Y8fAAAAAAAzI7SDLWmwow0nikUqs5qJ7k2o39e+S5fMEQAAAAAAMBVCO9iO7oNGhV1zO3/ujAwM9JsZmt13b++2Lp/eHTdHAAAAAACzIbSD7Zw5fZIKuyanTSlOnThuVd7BXqoB21t6/OZIc3r7yoC8b1PYChK/cFP75fv9yM0d1rF7N4Rkd4fX3BoAAAAAlh6hHWxDu8OeOHZUhhMJcwTNbmhw0IyApXXrMr/cuSZoBXMbox7pz5bkYqYkYY/LOnbXupAV3v3ZLR1WuLcy6DbvCQAAAABLg9AOtnHxwjkZGiLEaSWjIyNmBDTGo6fScveT/fJbTw/JR54Zkvf9j0G57ycD8vUjKUkVx6Q74LbCvc/cEBcvfzEBAAAALCGegsAWdH8zmhO0nlwua0ZAY+TLZjCBhnWPnc3IB54akK8dHq3cZkw6/G3ywS0RcwsAAAAAqD/XWIUZAw1x6eIFOXvmtJmh1dy49/Vm1Nq02ssOdF839acHRuRfL+as8WJVP+bLQwV56OVha9xouvRVl8F+50RavnsybY5OrcPXJg/tiUtP0D2n2zvN9rhXPlO5f3CeRL4s/+/pjPy0f/z/6urKz+g7VgXk5u767Um5L1GQB1+a/v/xh66LyFtXBMxsas9Uvt7Hz2XlbKZkzV/f5ZP/eV1I2iv/12by5crvpSem+b00l5/joyNF63wdHS1IufLot6vy+d7cE5C3rfSL2+UytwImSyRTksnmzWxxXh4Zk7NZnn4B9bQ64JLd0dr8Xo9HQxIKNvdez7A3Qjs0VD6fk/2vviL8GLYmj8cru27YY2atzUmh3caIxwoGdrV7rSDr+YG8/P2ptBxMFs0trnZtaKfvf9e6oGyLeWUoX5YnL2blsbNTV13evzUi71gZsG7zyIFRa4nqu1YH5Q3L/LIp6pGn+nLyw/NZ62PPx3xCO3X3+pDc0xuyxrp8VqvxrjXxvPjaXPL8YF6euJCd9rxMpF/LnZX7tbfLZ1X2HRguWkt3X0nUvykPoZ0z6c/JR58dkr7s5HLRj++I1i24W2xop4Hdw/smb40Q8rjkL27ttP7vTGexod2Hnh6c8nzdtyks71oTNDNgslqGdt86V5ZnhnncC9TTzXGXvHdVbRYVEtqh0Vgei4bJ5XJy+OABArsW5vXRndNJtBnDJ3bG5I9uilvdV7VpgzZv0KBJnyx/bHt01oYN+n4P3Ri3GkHoklNtAvG+zRGra+t1MY+51RVDuStPsPXz/PHeDrl3Y9gK7JR+HA3gfndbVGZ4rr9orwxdebK2xXzuqqnOi4aLOp7tvATcLvnDXTHrPuzq8Frnc7QwZoV4n9ods95XK/2Aa/3oYm7KAEppFZtdTfe1pYtj1n2ql28fS017vn5g4/MFAABaG88E0DAXzp+TfL42r1rCmQL+mZdQwV4+en1UXtflk0xpTL64f8Rq2qDNGz7xfEIqh6wA7f1bwubWk/UE2+R9m8Py2JmsVa32kacHrSq58evc8t6N07/v2pBHPrkzJm6XWJ/7PT/qt6r2jo2MV7G9qccvO9rrFwKfTo8v41Oxa0K0qc6LXv780Ois5+VX1oXkxk6fXMyW5GPPjjfD+OBPB62PU33fu02FHzDR6dT0FZzVZad2NNPXpstX6+UH5nfNVM7Z+HwBAIDWRmiHhkgkhmRocMDM0Kra2vgV5BS6TFWr4nRJ3sOvjljLUquOjRatgErt7vBZS0mn0hNwyz+dzcq3j493Zr2YLVvvp8tA1ZaYx1piOhX93BpSfPblYetzF8rjy20//2rSqk5Tt9ZxH6+Jy2H1flTNdF40kJzpvPQE2uTtK/1yYLggv/fskHU+qvTjaAdbdWu3z6reAyaaaom20xV0o7k60Uo+AAAAp+EZM5ZcPpeT40ePsCwW4vX5zAh2tibktvaVU7r33P7hyfus6f531ao33fttupDpb01AN9FTfeMVt7oRvC4Lnc4XXh2RS9csb9M98b59fPxjaiXfUlrsedFlvhFvmzx+PmuFkNfSffyU3qb6eQAAAAC0DkI7LDldFgvAOdaGxyvLSmNj8tiZjDWeyqOnrwRy68OTK+a0omyq6qDT6StL4nZ1TB3k9mdL1hLSqbxs9pubWAFXa7r3XFXSJGyLOS8a3unSV6XnZSr6aarXaeMNAAAAAK2F0A5LKpNOy8CAPbpkApibNaHxoKk/W7Yq26ZTrZhT3YHJf16me9epqsyupfu7TWcplgl2+6/cn37THGMx52ViwPjlWzqtDrtTXbbFxysPq403AAAAALQOQjssmWKxIMeOHjYzAHCO9RP22js3oSnFQk3sZaF78s12qS6xBQAAANA6CO2wZAb6++kWCzhQtYqsY0K12VS0IUPVpUxzdWN80/Lx5ana/OK8uW+1Oi/aMXa2yydeSJhbAwAAAGgVhHZYEtp0or+/z8yAcQVCXFu5vefKvmkTwyWt9FK+Npe8ZcJtrnXX2qAZifSZJaTN4NZlPqtBxmihLF8/Mt4NVi3mvJybcH6n65gLAAAAoLUR2mFJ9PddsrrGAhPRP9g+lgfa5L7NYWs8lCvL0dEryzH3DRfk+YHxgPW3t0Vlu9lnbaK3rwxcbqzwyIERGWiC0K6nck5+a2tEPrY9ZgVzXz+akjMTlsYu5rwkC2Py7WMpa/z+LWHpmLheFgAAAAAqeJaAJaFLY4FrlUrs09Uod64OWNVjetFg6YHdcYl4xv8kPHJw5KrmEOUxkc+/mpTTqfHv16d2x6z3qdrV7pUPbo1Y4+cG8vLkRWcH9Lqc9SPXReSRWzrl58z91Pv0r9fcr5nOi7dyKrXCbqbz8ujpjNX5dkvMK1+9tXPKaj39/sxUxQcAAACgeRHaoe60W2wmkzYz4IpS8UrVEpaWhkuf3h23Lhos9QTHu5n+7cm0tW/bVP7s4KgVMrldYr3P19/YKV99Q6c8cEPcWjr65MWsfPXQleWjTnBPb0geubnj8uXbb+6SL9zULrevCEi+PCbP9Ofki/uT8ueHRsx7TDbVefnLW7vk3o3hWc/L1w6n5KhpMqHVetoxtvq1/M1buqzvzzsmBKQAAAAAWgehHeqqXC7L2dOnzAyAXRwYLkqqeGWB8uPnsvKRp4fkb05MH7AfHinKQy8n5XOVy1N9ORGXiM/tsirIPvlCQh45MHq5OYPdHRstWuGk7kungWX1oudFK+A0qLv3RwPy8L6Ryn3NX1V5eK2pzkt/rmx9nNnOiza10Nt8cf+IdR4vZse/Hv3ePNOft5bQ6nUAAAAAWo9rTDsEAHUyONAvJ08cNzPgaoFAUK7fsdPMWtvdT7KEHI2he/F9Zk/czOAUXz4wIk9cs+S6almgTb5yS6eZTS1bGrNC5v2JglzKlqU34pabu/2yo33y3owT7avc/sGXhs1ssg9dF5G3rpi+OvRDTw9KX+XzTeWOHr98eFvUzCab6T7P9nM82+9YrXKdSanycFn3sNRLIj8mK4Nt1vL5dWEaybSCRDIlmWxtmmd961xZnhnm6RdQTzfHXfLeVbWpT4pHQxIKslUJGodKO9RVIjFkRsBk5TLLYwGgEf7v4yn5ysFRKwTbP1yQx85mrTBOl4Rjsu9Xzo9W3v7zhZy8MJiX71XmWmE7sWIZAACg1gjtUDfaLXYkmTQzAABgB4l8WR4/nzWzq33z6HhXY1yRKpblb09N3jpAz+Mf/ixh7X8JAABQD4R2qJvz589Ze9oBAAD7OJQsTLtP48VsWU6M0tl7oicu5GSkMHUwdy5d4nwBAIC6IbRDXeTzeWs/O2Ambvd4x1IAwNKZbUknSz6vNtv5mKlRDQAAwGIQ2qEuksMJMwKm53azgTcAAAAAAFMhtENdjI6OmhEwvTY3v4IAAAAAAJgKz5hRc6VSUYbpGos58Hl9ZgQAAAAAACYitEPNDQwM0IACc+Lxes0IAAAAAABMRGiHmhobG5OL58+bGTAzL6EdAAAAAABTIrRDTWUzGSkWC2YGzMzn85sRAAAAAACYiNAONZVOp80ImB3LYwEAAAAAmBqhHWoqkyG0w9z5/TSiAAAAAABgKoR2qKkMlXaYI5/PJ263x8wAAAAAAMBEhHaoKSrtMFeRaMyMAAAAAADAtQjtUDPZbFZKpZKZATOLxeJmBACt7bXhgvz9qbR1eaY/J/nymLkGAAAArYzQDjUzNDhgRsDsYnFCOwD4+pGUPPDisPz18bR1eXjfiHxp/4i5FgAAAK2M0A41k0wOmxEwM7fHI26328wAoDX94FxGHjubMbMrnh3Iy1cPjZoZAAAAWhWhHWomnUqZETAzv99vRgDQup7tz5vRZI+fz0oiXzYzAAAAtCJCO9RENju5UgCYjs/rMyMAaF2zZXKXsuwTCwAA0MoI7VATuWzOjIDZ+QMBMwIATKdAoR0AAEBLI7RDTRRLRTMCZkdoBwAAAADAzAjtUBOF/PT78gDX8rg9ZgQAAAAAAKZCaIeaKJdZw4O583gJ7QAAAAAAmAmhHWqiUCiYETA7r8drRgAAAAAAYCqEdqiJEnvaYR7cHrcZAQAAAACAqRDaoSZKxZIZAbNzs6cdAAdI5MtyKVua9ZIqskUE5m+qn6WpLqWxMfMeAACg1RDaAQAATJAvj8mXD4zIB54alA8/PTTr5UOVy9k0L15h7v7Dq8kpf5amunzy+YSkigR3AAC0IkI71ESpzJMVzI3bzdJYAPb2J/uS8sTFnJnNLl0ck8++NGxVRQEzSRbK8vHnhuTZgbl33T+RKslHnx2SoyNsRQIAQKshtENNlIo8kMTc+Hx+MwIA+9Hg7YXB+TdXGsyX5RtHU2YGTO2nfXkrhJsvXar9389nzQwAALQKQjsASyqTSZsRANhPX3bh+9M92z/36im0ptOphb/IeSFDJScAAK2G0A4AAMDwLuKRUbuPh1WYWdTrMqP5C3sW/r4AAMCZeHQJYMkVCvNfegYAS2FrzCuhBYYj924ImREwtR3tPjOav10dC39fAADgTIR2qA0Xr/5i7kol9kAEYF/3bQrPO7j7n9YE5Y4VATMDpraj3Wv9fM2n4k6rP+9cHZC39LAnLAAArYbQDjXh8/LqL+aOSjsAdvbWFQH5q9u65ANbInL3+tCsl0/tisn/tils3huY2bvWBOVPb+6Y8mdpqsvDezvkfZsjEnDzAikAAK2G0A7AkstmMmaEqlUhtxkBS2s1P3vT+vlVAbmnNzTrZU8nL1xhfsKetil/lqa68H8UAIDWRWgHYMkR2k328ytZVofG6I14zAgAAPu6d2NIvnt79+XLI7d0mGsAoHkR2qEmvF6vGQGzS2fSZoSqO1b4pX0RXQWBhdrVwe9vAGhFK4Puy8uwt8XGX8Dp8LVdtTx7Lh2114evfJyZLhsX+SLRrd3O3NdxtvOj+1Xq98Lubl3mu/w1b4mOfy+7/Vd+Xu5aG5Q2HsoCNUdoh5pwtfGjhLlLp1JSKpXMDEqXSt22nE3GsbT0QbYTnigAAGovWShfXob99lXjFf9D+bLcvsJ/+fimOQRt794Qvnz7mS5f2Ntu3mNhnurPmZGz6J6nU52P6uW3t0XlP93cIR/bHp1TSNoI+nV9cGv08tdcGhs/rttDVI+psjkOoHZIWlATbjdP+jA/Z06fNCNU3bkmyEbjWDLb4x65a13QzAAArSZVHJNHDoxY422xK1XXX9w3InmTvmxvn70a+0yqaEYze24gb0YL8+1jabn7yX558mLWHHGGk6Nze6H61mV++dSuuC2Duy1Rr4RNV/VHT6Xl2Oj493y3qdY/ly7J986y/Q1QD4R2qAlCO8zX4MCA5HPOfMW0XpYH3PJu80olUG/ajdLHOhYAaGlPXszJy0N56Qm6LwcwGsi8PDTe6X/HHEK7bx8fD9MmXqq+c+LKdV94NWmOtpZvHktddU6q56N6+Xbl+ouZ8WBPz7dW3tnNjZ3jPwf6der3u6q6xcY/nctIoWwNAdQYoR1qwuNhI3PMX39/nxmh6p2rA/Kh6yKyzM+vZ9THTZUH3l++pYMGFAAAiwZ3SrdMqPrH0+NVUxMrrGpN98/75XVB+fTumHzztq7LDSZ0rl+L7pc2m7evDFi3r77vn1X+vuky06n2z/v9HdHLt9PLfZvDcv/WiDxyc8flY1+4qd16AXUpX9N6tHKuP/dKUk6bikWtuNs7RVdyXY2h+9/p/fvqGzovf816n+/fErm8L+FU9FzrnnMTz5VedP4blfMw236D2+Lj4dw/X7hS5ahfS8TTJqPFsjzVt7gqSgDT41khasJDIwoswKWLFySbpZR+IrfLJW9dEZAvVR5Afrzy4PIdlQejvWG3hOr0gBnNb1mgTfZ0eOU9G0JWWPfvdsWtqk7YX9SBzWkKhYIkk0nr0j/QL8dPnpaz5y9Yl8FEQsplSjEAu/nxpZwVGGkwUw1+9g8XrCq5YOXxx/s2ha1jtfSLa4Ly1Vs75X/ZEJbdHT7r81TpXPdI++LrOuTO1YFpA7Seyt+yD26NWLev6q4c09DrwRviM4ZY6l2rg9bjLK0yrNoY9civrA/Jr22s/X2eyflMST794rAM58d/R+q+ghNpqKaBolbh6f3rmBBo6n1+x6qAPHRju3zs+sn74uk5/OLr2uXeyn2aeK6Uzt9ZOQ+636AGf1PZ1V75uaj8bBwbKco/nhl/3K5B7vs2h2W0UJaHXkpaeyECqA/XWIUZAws2MpKUI4cOmhkwd51d3bK+d4OZoR4+9PSg9GXt/2BKKww1sARaxTeOjMr3zk69N5MG9X91W5eZTfblAyPyhKmOuZYGtV+5pdPMJvuXC1n5ysFRM5tMn+xOtyRuX6IgD740bGaT/YLvouz0jO+RpfoGU1KasDP5o8HtknJNriBRd/T45cMzLAub6T5vrzyh/MyeuJlNNnFp2lS04mQ6Glx89+T0Xc9nOl9oDolkSjLZ2lQSfetcWZ4Ztt/TLw12dNsEXaqplV9VWoWmodb9Tw3OK5ip/p+a6v+Phm2P3NJhZiJP9eXk0VMZa1muXnf/1qvDpc+/mpTnJ+yH95FtEbm9xzTOyJXlO5WP/8Pz479LtXJOgzilIdMnXkhY4ypdAvzp3Vd+V+ht/vzQqPncbfKx7TEruNOPe/9PB82taqN6Tmb6faSVb3rfdcnyQy+PLyfWgOwbE/4eaMD6WOVvR/U+a1XeJ3fFrLHS43qflC5ffWDC/dXz+Cf7k9ZS1jUht7xrTdCqVqzS96t+3Co9X3rePvL0kFzMji/jrX5c3V/wkQPT/z1plJvjLnnvqtrUJ8WjIQkFaRaHxqHSDjXhpdIOCzQ40C+XLl40M7QiffVYn/QS2KHV/FzliVK7b/JDsXavSz414QkYANSbLm/UoCp2TYWvVlZpU4qJ1VsaIulSS10eOV9aNafLO6u+eTQlX9w/crmxgYZCulT060dGLzfDuH2az6PLMv/wZ4mrQqa/mxAQrgnPXFX+/bMZK9S78rnLVkCoJlayNYIuO6160/Ir91/DPH3hZOJ9fn4wL/9H5Tzo+VAawmkgp24375spjsmfHhix7l9177kz6ZIV0n1x/5W9Bu/dGJr0M9Dhc1nhXDWwUxFTGfn4uasDPgC1R2iHmiC0w2KcPXNKEokhM0Mr0X3VHrghRpUKWtK6sEf+4tZOeX3XlYoSrZL71O64bJ3QydFOUunpK84Wa7SOHxvAzLSKTivLvnns6v+H/3QuK/f+aEAuTajY31v5naVLLXWp5nz3u1sVdFuVbEpDoOpyy4k0q9NKsmozjImdbSfS7rf9uaur/zKlK1WMszVb0iWp17LjMs/qYyQN5bT6LlmYXKl5MFm0uutWVfegqy791eXH+v2q7mU38aLVhVUaFur3aKKPPZeYVE2nIa9WDOrnBVBfhHaoCbfbIx4PT7qxcGdOnZQMT9hayt3rg/K5G+OVB/z8KUJr+4OdMfnGbZ3yl2/stJa12rFJiO5R99rB1+TkqRPmSO0NDg3LYz98Qg4dPW6OALCjapWVmlgRNhcTgzRdmjqTlKkc885cMNdUtGGEGp7QitVrzplWQs5kYiVctYjbPb9MFYAN8UwJNRMIsrQNC6eblx94bR/BXQvQJTYP722Xe3rDs74KDrQKDa9j1+4ebhOnz56W/Qf3y3By+r3saiVfKMr+Q0flv/3wCclmWXYFONFMv8omVrJpQwXdR246u9vHq5BnC6uahZ6PteHxF23Op68EcNVzptfp3nLTee+GK80zqu9TrTzUgFQr4+ZyOUD1HGArhHaomWhs+g2ggbk6duwIS2WbWKevTR7a027LSiIAk505e0bOnjtrZksnVyjKj55+3uo6C8C+fmld0OpQXqUdW9+/+UoX0pNmv7gqDZMem7AkVpdm3rrMd3kfNV1uq3u4aQOM6r5yE5tQNCMNLnWPQG2wUfU/+q403XlmwvgTO2Pyy5VzXt2zTq0Muq19AqvLjjXk1KZB6pn+8XOn1z20J26d62tDVf2eaWOJu9eHrP3wbPr6EdCy6B6LmhkdGZHDhw6YGbA4GzZtlvb2K53FsHB26R6r3TD/6MZ2WT3hgSaAhal399jrY245fPSwDF3zIsrpUlC+k1ttZpMtpnvsxuKg3Jo/ZWZX7NmxTXrXraF7LBqmFbrHzke10+xsdA827To7YaXnZRogVfddm4nuW/fhyuMYfauNq76wt91cM04Dqt97bsi6XqvQNNSaWMWvX8PHnk1YYeHErrNVWoH22ZeHrfef6n5N1U11vrRhh+4nN1fXdvBVuoegBnuz0eYden+rS2U1gPv3e9ovB3pz8emfJRxfbUf3WDQTcnTUTCh8pSQbWKzTJ09aXWV5XWHx7BDYvXm5X/7j6zsI7AAHKI+V5cixI5MCu0Z59cBhOXHqjJkBaDSt3tIw+3Rq+mBHr/uPr41MGdgp7WSqHUm1q+l0Hj+fld97djyQU9oA41pajffejePPQe7fGpm07YbuuadVZOrawE5pmFUND6f6+FqZtljTdb+9lgaQ2jX32sBOaZCnnXZnWip8YLhgdYeduLednn8NJbVT7mw08HtuIF95/8Y/bgRwBaEdaqatrU18vsX/YQNUsViwusr2XbpkjsCp7tsUlt+5Pirt1V2RAdjahYsXZHBo0Mwar1gqyYv7DsiFS33mCIBG0o6tWn2qXUU/8UJCnrxwpRJNA6PPv5K0rntxcHyJ5lS0G612JP3480NWIKUVcVWPnkpb1bFfPTR61R54Uy2T1RDrW5X3V3p7DZ4m0o/7iulCqyHhtbTSTsMuNdXH1y6pi/XkNBXCVRpKakWfdu/VrrnT0U67eptHDoxcFczpfdDquE+/OHz5vk6kH/8vj6Ssc6rn9loawOr3QLsEf+HV5FXnHEDjsTwWNXX0yCFJDtd/o2q0Dg2Du7qXycpVq8XtpkprIWZbFlYvGtL9+uawtbEygNqq5/LYe/xnZa176qqMRiyPrXrKt06Oeaa+XyyPRT2xPBZwFpbHoplQ9oCaCgZm32sBmI9yuSx9ly7K0cOHrA6zsD/dP+Xu9UH5i1s7CewAAAAAYIEI7VBT/iChHeojlRqV1/a9QnBnc9VmE/f0ssclAAAAACwGoR1qiko71FOpVJKTJ47RnMKmdHnaA7vj0huZe4cyAAAAAMDUCO1QUwEq7VBnI8mkDA4OmBnsQjuj6X5Sm6IEdgAAAABQC4R2qCltGhCPt5sZUB/nz56RbHb21vVYGnf0+OX9WyJmBsDu3C4zwJzoPp0AAACNwMMQ1FxX9/Qd2IBa0H3tDh14zWpSgca6b1NYPrwtKgFSAMAxVgRn7sQ9XefYVnVj59TdbqtWh+hsDgAA6oPQDjUXjcWtijugnnR/uwvnz5kZllq7r00+uTMm71rDknjAaXQZ+9ppgqYd7qQZoUr36byhw2tmV7S5tFN2yPp9CAAAUA88ykDNaWDH3nZYCpcuXrDCOzsol8ckOZqufD3NX/23pfKE/y9u7ZS9XTNXnwCwJ7fLJf/7jpjV7XmiNW0Zebuvz8ww0Ue2RWVZ4OqHzf9mTVDu6Q2ZGQAAQO0R2qEuIpGoGQH1o11kjx870tBustlcQRLJlPQNDEsqnZNMLm+uaT66r9OdqwPyBztj5ggAp9IlnV95XVTeEzgjv+Q/L78WOCW/GjgrHhfduaei1XRfuaVTPn9Tu1Vl/KXXd8i9G8PmWgAAgPogtENdhEK88oylod1ks5ml338pmy/IpYFhGRoelUw2L2UTHOq4WX1gS0TetznCUjCgSSQTA7KyLSub3Snpbmve3121pEuLtcqYfewAAMBS4JkX6iJMpR2WULFUNKP60sYX6UxOBhOjMlS5TLUUtlgs2Sq4y5cXXzWzMeKRB2+Iy1tXBMwRAM1gYGDAjAAAAGBHhHaoC5/PJ8uW95gZUF/681YvuledBnVaUXexf1iGR9KSyxfMtVPT5bJFm+y1dzi58ECz3euyusM+dGNcdrRP3oQdgHPpfqCjqVEzAwAAgB0R2qFulvcQ2qH+ItGo+P31qQBLpbNysT9hBXW6d918ZLPzu70dfWp33OoO69MWiQCaSrG4NBXKAAAAWDhCO9SNz+eXcCRiZkDthcMR2bhps5ktnja00Cq65EhaLvUPS3J04XvlObkhhXZI1I3WeyMecwRAs1mqbQUAAACwcIR2qCu6yKJetMJu05Yt4nbXJljSZbADQyPWfnWpTE5K5cn71c2H7m1XqFycZkvUY3VI1I3WATSvkgN/PwEAALQaQjvUVSweNyNgcXx+v/SsWCm9GzbK9p27ZMvWbTUJ7LK5vLUHXd/AcE1CNo/HLdFwUJZ1xsRbGTtFwO2SO1cH5JO7YuYIAAAAAKCRCO1QV1ppV88mAWgN8fZ2uX77Tlm1eo10dHbVZA87Desu9CVkaDhldXstjy2uy6rH3SZd7VErrIuEA1Z45yS/vjks79sckZiXPwsAAAAAYAc8O0PdrV67zoyA+YvGYrJx0xZpa1v8rytdsjqSykjfYNIK63QPu8VwuVwSDPiksz0i3Z0x8fmctwfcxohHPndjXN66oj7NPAAAAAAAC0Noh7prb+8Qf4BAAAvT1dVtRgunVXQa0mlYN5rKWuHdYoVDAVmxrF3aY2Hx+7xWgOc02mjigRtisjXmNUcAAAAAAHZBaIcl0R5vNyNgfgLBkBnNXz5flKHhUbnYl7CWwy6W291mhXVdHVGJRYLmqPPoCti71wfl4b3tEvbwZwBwojbnvU4AAACAeeLZGpaE7kMGLIR7ActitaFE/2BSBhIjks0VzNGFa6s8O9aKuuVdcSus83mdtwy2KuRxyR/d2C739IbNEQBO1OWfft9Mr40TPffY9J25w2OLf3FlOtvbp68oZitPAABgVzxMwZIIhkJW909gvsozPMG7li6DHRgasQK7WnSCDfi90hEPy7LOuLV3ndMt87fJA7vj1rJYAM52c7dv2rBpb6d9f1+tLiXNaLJVM1y3WPesDznyfAEAgNZGaIcls2LFKjMC5q5QmHulXJvLJd5FVMFVG0toUKf71XXEIxLw+6xKOycLuF1y5+qAfH5vu2yKEtgBzUDDd62afcOE8G5V0C33bQrLvRsXvq1Ave0pnJO9+bMSK2etuWtsTNYWE3Jn9qB0l9PWsXrY0e6Vj++Iyfa49/L56vS1yV1rg/LhbdHxAwAAADZTeay0yPaJwByVy2V59eUXpVRafAUUWsemzVslFo+b2dxc6EvMuzOs1+OWzvao4wO6a+1LFCSRL8tty6l0BXBFMpmU/Qf3m9ncnS4F5Tu51WY22S/4LspOz4iZifQNpqRUrs1Dzad86+SYp9PMrqZh3Gf2zO9vBTBXiWRKMtnaLN/+1rmyPDPM0y+gnm6Ou+S9q2pTnxSPhiQU5HE0GodKOyyZtrY26d2wycyAuSkU578nnS5rnQvdLy9c+SOsjSX00myBndLqEgI7AAAAAHAeQjssKa2YikRYhoK5K85jeWzVXEK7WDQky7vj1lttLKFLYwEAAAAAsAtCOyy5zq5uMwJmt5Dl1H6fd1LXWc3kNMyLx0LWfnVaYQcAAAAAgF0R2mHJdXR2itvtNjNgZvNpRFGlVXNaQVelYZ12gNXGEqGAn6o6AAAAAIDtEdphybG3HZaCBnWRcEC6O2NWWOd28+sOAAAAAOAcPItFQ7C3HZZCNBy0usICAAAAAOA0hHZoGPa2w1xEo4S7AAAAAIDWQ2iHhmFvO8wmGAwR7gIAAAAAWhKhHRqGve0wk47OLtm67XozAwAAAACgtRDaoaF0b7t4e7uZASLtHZ1WWNe7YaMV7AIAAAAA0Ip4RoyGY/kjlM/nk23X75ANGzdJOBwxRwEAAAAAaE2Edmi4WCxu7V2G1tTe3iEbNm22ArtgiJ8DALCzzra8uGTMzCYLu0pmVHsemf5jB9wuMwIAAGgehHZoOF0Cub53g5mhVUSi0fHKuk2breDO7fGYawAAdqWh3Oq2rJldzS1jsrYtY2a1t6I0akaT7e7wmhEAAEDzILSDLWiFVSQSNTM0I5fLJdFoTFavWSvbtu+QLVu3UVkHAA70Zm+/rLwmuPNLWd7m6xOPa/oqvMVaXRqW3uKgmY3zjJXkzfGSvG1lwBwBAABoHq6xCjMGGmpgoF9OnThuZmgmPStWVC6rxO12myP1kcsXZDQ1dQWIUwUDPgkF/WYGALWRTCZl/8H9ZrYwx0shOV8OSNhVlBs8SXP0an2DKSmVa/tQM+XyyQl3u5RcbbKl2C9v3LlZ1q9ZY64Fai+RTEkmmzezxfnWubI8M8zTL6Cebo675L2ralOfFI+GeCyOhiK0g20UiwXZ/+orUirVbz8cLA2f3y/xeLu1BFb3LFyqLrDpTE6GR9Jm1hwi4YBEw0EzA4DaqEVoNxf1CO2udeOubYR2qCtCO8BZCO3QTFgeC9vweLyyZu06M4NTrVi5Snbs3G19L3WvuqUK7AAAAAAAaCZU2sF2Dux/VTKZ+m1kjfrQPeu6ly23quuikWhDGktQaQcAc0OlHTB3tay0e7y/LK+lePoF1NP1YZe8o5tKOzQHQjvYzsUL5+Xc2TNmBifSAC8Wj8vy5T0SicbM0fojtAOAuSG0A+aulqEdAGchtEOjsW4NtqNLKuFs+lrAcCIhhw8dlFdeelFOHD9mNRrJ53nACwAAAADAXBDawXb8gYB0dnWZGZxOG4wMDQ5YnYH3vfKSHNi/T86fOyuZdHNVxAEAAAAAUEuEdrClzq5uM0KzyWTScuH8OTnw2j45eeKYOQoAAAAAACZiTzvYkv5YHj1ySEaSSXMEzSoYDMqynhXWsmi3222OLkyhUJRsvmBmzcHn9Yjf5zUzAKgN9rQD5o497YDWxZ52aDRCO9hWqVSSV176mRXgoflFIlHZct02MwMA1BOhHTB3hHZA6yK0Q6OxPBa2pVVXHZ2dZoZmNzo6Ivv3vWK9BQAAAACg1RHawdbY26615LJZOXvmtJkBAAAAANC6CO1ga7pkMhqLmRlaQS6bMyMAAAAAAFoXoR1szeVyyYaNmxfdoADOoY0pAAAAAABodYR2sD0N7Fgm2/y0qnLtuvXSu3GTOQIAAAAAQOsitIMj0JCieWk15eq166zOsd3LlovX6zXXAAAAAADQugjt4AihUNiqxELz6d2wUZYv7zEzAAAAAACgCO3gCFqNtX7DBjNDs1i2vEfaO6iiBAAAAADgWoR2cAyfzy+hcNjM4HT6vVyxcqWZ1UcimZJcvmBmAAAAAAA4B6EdHKWre5kZwckCgaBs2bpNPJ767V+XHElLJpuXwcSoXOhLWAFeNpeXsbExcwsAAAAAAOyL0A6OEolEzAhO1tXdLW1t9fv1UyqXJZXJmZlYQZ0GeEPDKekfTEo+XzTXAAAAAABgT4R2cBSt0GKJrPPF2zvMqD6yuemXxBZLZRlIjEjfYFJGUhkpFkvmGgBwrudfPCSlyu83zO7lfcckMTxqZgAAAPZFaAfHWbGivvugob46u7rF7/ebWX3kZgjtqjSsG01lrfBOl87yZBeAk/2Xbz0mA0NJM8NM/u4f/1Ue/5fnzAwAAMC+CO3gONFYXNxut5nBaXpWrDCj+tDwbb7NJ3TprIZ3ug+evi/73gEAAAAAGo3QDo6je6EFgyEzg5P4fD5riXM96ZLXhdCgTvfB08YV/UMjVN4BAAAAABqK0A6OFAwR2jmRPxAwo/ool8ekUFh8kwldOntpYNgK71LpLAEeAAAAAGDJEdrBkQjtnMnr9ZlRfbS1uWRZV1zaY2FxuVzm6MJpAJgczVgBXnI0bYWCAAAAAAAsBUI7OBLLY53J6/WYUX0FAz7p6R4P77ye2ux/mErnpG9g2GpaMVN3WgAAAAAAaoHQDo4UqPMyS9SHy7V0v3K00k7Du+7OmHXx1CC8K4+NWU0rhoZHZSDBvncAAAAAgPohtIMjaTOKUDhsZnAK/b41glbbdXdEpbM9IqGgvyZLZ/P5orVsVhtXpDM5KZUJ8AAAAAAAtUNoB8fq6uo2IziFx+s1o6WnQZ3f55V4NCQrlrVbAV446Lf2wVuMXL4gwyNpudQ/HuBpB1r2vgMAoHks1fYeAOzHV3n+ADQSoR0cKxyOmBEwfxrgxaIhWd4Vl0g4UJPqOw3wkiNpudifkJHRjDkKAACcLBTw1eRxAgBn0ecIHjeRCRqLn0A4lnaQbdRySzQPfRAeDQdlWVdMIqHKH+ZaNa7IZFkyCwBAE9DHCvo4wUfFnS2MpnMyPJJp2kuxVDL3FI0UDvmt5whAo5F4wNE8Hh48oTbcbW0SjQRlWWfM6jq72FfUtZJPPyYAAHA+/ZuuW2to5Q0aK18oSiZXaNoL26w0lv5f1yZ2sUjIHAEayzVWYcaA4xw+dFBGR5JmBrtb17vBMXsR6q9GXe6ayRast/P9VanBn3avBYCl8OHf/5L8n5+8T5Z3t5sjM0smk7L/4H4zq5++wZSU6vwE9MZd22T9mjVmNrsHP/8N2bGtV+6+6w5zBJgf7R6vwZG+Hav8w9I6e2HICrea1eqedh5DNkCby2WtuNGKWpbDw04I7eBoJ08cl8GBfjOD3TkptJtIX/FMjqYlk82bIzPTP/Q93XH+4ANYMoR2hHZAqzh84oK1RLZZbVnfQ0UngMtYuwVHY3ksloJ2mNXKOd3PRve28M6y7512qCWwAwAAAAAsBqEdHI1GFFhKHrfbeuVT97mYbmNaLatnSQMAAAAAYLFIPOBoXh/hiJM0U+2Zhnc93e1WVZ02nagKTBgDwFIql+besdrVRjUwAACA3RHawdFYgugszRay6rLZUNBvdZPT6jvduJYqOwCN0B6PyMDQ3Bsz+bz8rgIAALA7Qjs4mo9KO0dxt828F5yT6T53XR1Ra3ksACw1r3d+e7y2uZvnIWBP9/waHGVzeQmF2OQdAADYH6EdHK2ZQ6Bm5A/wJAkA6sGqtBuce6Wd1+MVv89vZs7lcbdJYJ5/W/Q89a5bYWYAAAD2RWgHR6N7rHNEolFxuwlZAaAeNLQbnMfyWNXR0WFGzhVcwItBw8mU7NjWa2YAAAD2RWgHYEl0dy8zIwBArXV2xOa1p53q6uwyI+fq6pxf8PjiK0ckGpm6+zcAAIDdENoBLaYR1W76OTua4MkhANjVip5OOX3mkpnNTTQSFa/X2R2vY9GwGc3N4aNnZMXyTjMDAACwN0I7OJrH4U82GmF970ZZ17tBorG4OVJ/XVTZAUBdaRB14dKgmc1dfAn/FtTDsq75BXBP/PhFWbVyfo0rAAAAGoXQDo7W1saP8HyVyyXp6uqWzVu2WuFdvWmV3fIeNvwGgHpat2a5DA6NSF9/whyZGyeHdtFwUKKRiJnN7lLl3PQNDNOEAgAAOAaJB9BiymNjZiRWeHfDjXuld+Mm6ayMa71MKhAIypat2xy//AoAnGBj70r53g9+amZzE4/HHdkkyFW57N2za3wyR8eOn7PeEtoBAACnILQDWpxWK3Z0dMr63g2yfedu661/Ad34rhUMhmTz1uskGAqZIwCAetrUu0oee/xpyReK5sjsfF6f9K5zXifVjvaYtMdiZjY3Bw6fst5u2bTGegsAAGB3hHYALtMATyvutu/YJdfv2GkFeMuW90hMKzE8HnOr6Wm1RjQakw2bNsu27TuosAOAJXTdlnXW2wOHxsOpuVrWvUxCQWe9wLKpd/y+zse+Ayfk9ttuEJ939r9nAAAAdkBoB2BKurRVA7w1a9fJps1bZfcNN8ruPTdZy11XrFx11UXDvet37LKu1+q69vYO81EAAEtl756t1ttX9h+z3s7HyhUrzcj+dC+71Svnt8RV9/o7efqi3Peed5ojAAAA9kdoB7SY4jyWTV1LK+ki0aisXLX6qouGe4EaLKkFACxcOBSQt9+xV1546ZA5MnedHZ0Si85vuWkjBAN+ue3mvWY2N6VSWb78tUdl+3XrrXMEAADgFIR2QIvR7rEAgOaklWTZbF6+/d0fmiNzoy/KbN+2XTpsXCntcrnkdTfsnPeLRM+/dEj2Hzwp73zbzeYIAACAMxDawdHyuZwZAQAA3a/t9Tdtk3947CdyqT9hjs7dht4Ntu0mu3plj3R1zj9UfPLHL0oo6JebzPJhAAAApyC0AwAAaCJ3vGmP9faJH79ovZ0P7Sa78/r5V7PV25pVK2T39fMP3U6cuiAv7z8mH/2tf0sDCgAA4DiEdgAAAE2kd90KufMdt8hjP/ipOTI/wWBQtmzaYmaNp4GdLov1+XzmyNzkC0V58PPfkE29q2TPrs3mKAAAgHO4xirMGHAcXR6779WXzQxz0d29TNau7zUzAEAz0sDqDx74z7JqZfeCq8xylb+xR44fkZGREXNk/voGU1IqL+yhpu5ht37NKtm+ddO8Azv1/R8+I//l2/9Nfv+3f1VuvmmbOQqgXhLJlBnVVy5flHK5bGbNx+/zSluby8zqJxTwV363UoEM2B2hHRyN0G7+tNPr+t4NZgYAaFY/+Jfn5Gvf/J787m/+itx2y05zdH70ifHL+16WbDZrjszPYkK7nddtls0bF/4i0wd+949l9cpuefCT95kjAOrp/KUhM4ITxKMha79PAPbG8lgAAIAm9JZbd8v269bL3/3jv0oqvbDQra2tTa6/7npZtWKVeDxLU5GxvLtT3vyGvYsK7LTKLhwKyMd/593mCAAAgPMQ2gEAADShQMBnVZklR9LW3m4L5ff5Zd3adbJr+y6JRWPmaO15PW65cdc2eePrb5Kujvl3ia3SgPJv/u6f5a53vckK7gAAAJyK0A4AAKCJ/a+/+vNy8vRF+c6jT5gjC+P3+2X7tu2y/brtsnzZcqvTbC0s6+qQXdu2yNve8kZZv2aNObowl/oT8qmHviYrejrlzbfuNkcBAACcidAOaDG6DyAAoHW85Y27rW6y//UfnpR9B06YowsXi8VkY+9GuWnPTbJ189YFVd+1uVzSEYtYy2Bvu3mvbNqwXgL+xe+t9Fd//X05d2FAfv29d4rbzcNcAADgbDSigKPRiGL+IpGobLmOLnoA0Gr+05//nTz93H757ft/Wd7w+u3maG0Ui0VJZ9KSyWSkUChYDSwKxYIMJkbF6/VKwB+wQjTtYhsMBqSzvb2me+Rls3lrSeyPnnpZPvi+f0O3WKABaEThLDSiAJyB0A6Op08Ucrms5PN5K8TTt7nK24L1Niv8iF+N0A4AWlOpVJbv//dnrHDrw79xV82Du0ZJDI/KnzzyHblwaVA+9uF75Pqt68w1AJYSoZ2zENoBzkBoh6aXzWYlmRyW1OioDCeGWj7EI7QDgName9v9w/d+LJ/79Puld90Kc9SZNLD76L97RLxej3zp//oIjSeABiK0cxZCO8AZCO3QUvTH3arAy49X5FUr88bfjo+bXSgcluu2NUd1BQBg/rTi7kt/9l/lhZcOyXvufru86+ffYK5xFt2f7y/+6v+TQqEoH7n/l6mwAxqM0M5ZCO0AZyC0AybQPXjSqZSMjCRlaGhQctmsuaZ5+Hw+2bHrBjMDALSifKFoBV5P/uQluf22G+QDv/aL1n5zTvHEj1+Ur/zlP8j6tT3y8Gd/0xwF0EiEds5CaAc4A6EdMAMN8XR5rVWFd1VVXuVt5VipVDK3dA5COwBA1fd+8FP56+/+ULZsWiN333WH7NjWa66xp0v9Cfnuo0/Isy8ckNtu2Wl9ze3xiLkWQCMR2jkLoR3gDIR2wCJocDeSHJHh4SEZTiTMUXsjtAMATJRKZ+UPHvjP0jcwLP/2l26Xe+66w1xjLxrYfeqhr8lwMiUf+o1fkjvetMdcA8AOCO2chdAOcAZCO6CGCoWCZLMZKVpvs5ffFooF2yy1JbQDAFxrYDBpdZZ9/F+ek9Uru+UX3nazvOWNu821jXXh4qD861MvW1/bti3r5N2/8nPW1wjAXgjtnIXQDnAGQjtgCWXSaUkkhiSTSTesMo/QDgAwnbPn++WP/uRbVtXdsq64fPx33t2wDrO6754Gdbp8t1AsyZ3vuEXue887zbUA7IbQzlkI7QBnILQDGkT3y9NKvEwmI4VCXnK5nNXZNpvLVt4WpFgsmFvW3o17X29GAABMpo0e9HL0xDl5w+u2y+tv2mbtdxcOBcwt6ufAoVPy/EuH5KfP7ZeRkbS1DFYvjQoPAcwNoZ2zENoBzkBoB9iULrVNp0ZleHjYqsqrZYhHaAcAmIsTpy7Ig5//hqQzOevJnYZnd73rTXVp/qAhoTaZ0Co/9fY79sr9v/aL1hiA/RHaOQuhHeAMhHaAQ2iIp5V4uZx2sy1IXt9OqNDTyr25IrQDAMyVNqrYd+CEvPjKEdlfeTswlJQVyzutyje96HjtmuUSi4QkEPCZ95peYnjU2kPvwqVBOXe+3/rYh4+ekWjlCaTuWbdr+0bZu2crXWEBhyG0cxZCO8AZCO2AJlEN9XS5rXa1tS658ZBPr5uI0A4AsFAa4mnIdurMJTl99pIVvg0OJq0wT5fPBvw+K7yLRkKSrfwdcre1WUGd0tvofEVPp3R2xKyGEtu39Vrh3/Ludus2AJyJ0M5ZCO0AZyC0A1rEcGJI0mltgDEk27bvNEcBAKidQ0fPSKFQtCrofvL0q9LXn7CW1GowpzScW4p98QAsPUI7ZyG0A5yB0A4AAAA1p/vT6dLXBz95nzkCAACA+WgzbwEAAAAAAADYBKEdAAAAAAAAYDOEdgAAAAAAAIDNENoBAAAAAAAANkNoBwAAAAAAANgMoR0AAAAAAABgM4R2AAAAAAAAgM0Q2gEAAAAAAAA2Q2gHAAAAAAAA2AyhHQAAAAAAAGAzhHYAAAAAAACAzRDaAQAAAAAAADZDaAcAAAAAAADYDKEdAAAAAAAAYDOusQozBgAAAC4rlcpy4PApM5ufJ378opw4dUHue887zZH56eqIyYqeTjMDAABoPYR2AAAAmNIzLxyQP/7T/8fMltb6tT3y8Gd/08wAAABaD6EdAAAAprSYSjuVzeYlEPCZ2fxQaQcAAFodoR0AAAAAAABgMzSiAAAAAAAAAGyG0A4AAAAAAACwGUI7AAAAAAAAwGYI7QAAAAAAAACbIbQDAAAAAAAAbIbQDgAAAAAAALAZQjsAAAAAAADAZgjtAAAAAAAAAJshtAMAAAAAAABshtAOAAAAAAAAsBlCOwAAAAAAAMBmCO0AAAAAAAAAmyG0AwAAAAAAAGxF5P8HWZztKFU41JkAAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "MvuycO87Elo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model for Automatic Speech Recognition (ASR), we need a training dataset\n",
        "that is a collection of audio files along with the corresponding text transcription that\n",
        "describes that audio. The more diverse the set of audio files with people from different age\n",
        "groups, ethnicities, dialects, and so on is, the more robust the ASR model will be for the\n",
        "unseen audio files.\n",
        "\n",
        "While there are many architectures available for ASR, wav2vec is a really good\n",
        "cross-lingual architecture developed by Facebook AI. \n",
        "\n",
        "It can arguably work on any\n",
        "language and is extremely scalable. It has recently outperformed the LibriSpeech\n",
        "benchmark. It was first published in the paper wav2sec: Unsupervised Pre-training\n",
        "for Speech Recognition.\n",
        "\n",
        "The wav2vec model is an improvement over the BERT Transformer model, which\n",
        "we saw in Chapter 3, Using Pre-Trained Models. This model uses much less labeled\n",
        "data than other models since it relies on \"self-supervised learning.\" It learns the latent\n",
        "representation of audio files in a set of small speech units (25 ms), which are shorter than\n",
        "the phenomes. \n",
        "\n",
        "These small latent representations are fed into the transformer in masked\n",
        "form along with knowledge from the entire sequence. A contrastive loss function is used\n",
        "to find the converge of masked positions and speech units. It uses the concepts of\n",
        "self-supervision and contrastive loss.\n",
        "\n",
        "The pre-trained model is trained on nearly 960 hours of audio. The most interesting part\n",
        "of the architecture is that it is cross-lingual and has been tried in various languages.\n",
        "We will try this model for a Scottish language dataset."
      ],
      "metadata": {
        "id": "P1aYfUp5xsx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "QUYNoxZDlwbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install correct version as defined in the Technical Req\n",
        "!pip install pytorch-lightning --quiet\n",
        "!pip install lightning-flash --quiet\n",
        "!pip install 'lightning-flash[audio,image, video, text]' --quiet\n",
        "!pip install Pillow==9.0.0"
      ],
      "metadata": {
        "id": "oDEJVSarlx3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import flash\n",
        "\n",
        "from flash import Trainer\n",
        "from flash.audio import SpeechRecognitionData, SpeechRecognition\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "qrzUx1T1lair"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "urllib3.disable_warnings()"
      ],
      "metadata": {
        "id": "38bX1kDH66sy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the Scottish language dataset."
      ],
      "metadata": {
        "id": "agUQlsGx7qjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.openslr.org/resources/83/scottish_english_female.zip\n",
        "!unzip scottish_english_female.zip"
      ],
      "metadata": {
        "id": "xBkW0SZVZYZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "SXR_D4n5mPbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are collecting the dataset and down-sampling it to use\n",
        "only 6% of the dataset. This is done because of compute resource limitation. \n",
        "\n",
        "If you have\n",
        "more compute available, you can try a higher number or the entire dataset.\n",
        "\n",
        "Let's load the dataset."
      ],
      "metadata": {
        "id": "JlDSL6ww3ZMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "\n",
        "scottish_df = pd.read_csv(\"line_index.csv\", header=None, names=[\"not_required\", \"speech_files\", \"targets\"])\n",
        "scottish_df = scottish_df.sample(frac=0.06)\n",
        "print(scottish_df.shape)\n",
        "scottish_df.head()"
      ],
      "metadata": {
        "id": "YUuBTQgemQ3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "6532ae33-8810-4e9e-9a51-a94eb43100d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    not_required  ...                                            targets\n",
              "434       EN0514  ...   Choose which sounds to hear while you're fall...\n",
              "159       EN0771  ...   He is characterized by his frequent impatienc...\n",
              "675       EN0228  ...   It isn't snowing in Warsaw it is minus eleven...\n",
              "62        EN0006  ...   When a man looks for something beyond his rea...\n",
              "759       EN0137  ...   There are four cities with nonstop flights to...\n",
              "\n",
              "[5 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d096c6e7-d452-44e9-9a06-249e2d8f1448\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>not_required</th>\n",
              "      <th>speech_files</th>\n",
              "      <th>targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>EN0514</td>\n",
              "      <td>scf_05223_01907513691</td>\n",
              "      <td>Choose which sounds to hear while you're fall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>EN0771</td>\n",
              "      <td>scf_02484_01779494101</td>\n",
              "      <td>He is characterized by his frequent impatienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>EN0228</td>\n",
              "      <td>scf_06136_01207317484</td>\n",
              "      <td>It isn't snowing in Warsaw it is minus eleven...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>EN0006</td>\n",
              "      <td>scf_05223_00305075751</td>\n",
              "      <td>When a man looks for something beyond his rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>EN0137</td>\n",
              "      <td>scf_05223_00757482258</td>\n",
              "      <td>There are four cities with nonstop flights to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d096c6e7-d452-44e9-9a06-249e2d8f1448')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d096c6e7-d452-44e9-9a06-249e2d8f1448 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d096c6e7-d452-44e9-9a06-249e2d8f1448');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will create a train-test split for the dataset."
      ],
      "metadata": {
        "id": "IT24ZKbUna5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scottish_df = scottish_df[[\"speech_files\", \"targets\"]]\n",
        "scottish_df[\"speech_files\"] = scottish_df[\"speech_files\"].str.lstrip()\n",
        "\n",
        "scottish_df[\"speech_files\"] = scottish_df[\"speech_files\"].astype(str) + \".wav\"\n",
        "scottish_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iKXXWNLRnbjJ",
        "outputId": "f97559dc-d242-4705-f0ee-b66f4c78d2d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      speech_files                                            targets\n",
              "434  scf_05223_01907513691.wav.wav   Choose which sounds to hear while you're fall...\n",
              "159  scf_02484_01779494101.wav.wav   He is characterized by his frequent impatienc...\n",
              "675  scf_06136_01207317484.wav.wav   It isn't snowing in Warsaw it is minus eleven...\n",
              "62   scf_05223_00305075751.wav.wav   When a man looks for something beyond his rea...\n",
              "759  scf_05223_00757482258.wav.wav   There are four cities with nonstop flights to..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efa8cbdc-cd74-4a65-bb12-e563121a158a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speech_files</th>\n",
              "      <th>targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>scf_05223_01907513691.wav.wav</td>\n",
              "      <td>Choose which sounds to hear while you're fall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>scf_02484_01779494101.wav.wav</td>\n",
              "      <td>He is characterized by his frequent impatienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>scf_06136_01207317484.wav.wav</td>\n",
              "      <td>It isn't snowing in Warsaw it is minus eleven...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>scf_05223_00305075751.wav.wav</td>\n",
              "      <td>When a man looks for something beyond his rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>scf_05223_00757482258.wav.wav</td>\n",
              "      <td>There are four cities with nonstop flights to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efa8cbdc-cd74-4a65-bb12-e563121a158a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efa8cbdc-cd74-4a65-bb12-e563121a158a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efa8cbdc-cd74-4a65-bb12-e563121a158a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "\n",
        "train_scottish, test_scottish_raw = train_test_split(scottish_df, test_size=0.2)\n",
        "test_scottish = test_scottish_raw[\"speech_files\"]\n",
        "test_scottish.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC2rWakkoRfv",
        "outputId": "92bf0a5c-a50d-4b70-8ee7-540d0022cdb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166    scf_06136_01953870538.wav.wav\n",
              "758    scf_03397_00368900806.wav.wav\n",
              "590    scf_06136_00954042844.wav.wav\n",
              "745    scf_02484_00771531634.wav.wav\n",
              "788    scf_04310_01650377019.wav.wav\n",
              "Name: speech_files, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_scottish.to_csv(\"train_scottish.csv\")\n",
        "test_scottish.to_csv(\"test_scottish.csv\")"
      ],
      "metadata": {
        "id": "caDPy7urov8_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to create a data module for this dataset."
      ],
      "metadata": {
        "id": "XvTpf5RKpEwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_datamodule = SpeechRecognitionData.from_csv(\"speech_files\", \"targets\", \n",
        "                                                train_file=\"train_scottish.csv\", \n",
        "                                                predict_file=\"test_scottish.csv\",\n",
        "                                                batch_size=10)"
      ],
      "metadata": {
        "id": "binV-z4LpFbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuring the backbone"
      ],
      "metadata": {
        "id": "-stblTzQu3ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step for us is now to select the pre-trained model architecture, also called the\n",
        "\"backbone,\" in Flash."
      ],
      "metadata": {
        "id": "9elyG9s480Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's list the available model architecture options\n",
        "print(VideoClassifier.available_backbones())"
      ],
      "metadata": {
        "id": "BTVdd8ghnkx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8d20f8-c145-43f4-f67f-b485ef81776d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c2d_r50', 'csn_r101', 'efficient_x3d_s', 'efficient_x3d_xs', 'i3d_r50', 'r2plus1d_r50', 'slow_r50', 'slow_r50_detection', 'slowfast_16x8_r101_50_50', 'slowfast_r101', 'slowfast_r50', 'slowfast_r50_detection', 'x3d_l', 'x3d_m', 'x3d_s', 'x3d_xs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's choose the slowfast_r50 model architecture\n",
        "print(VideoClassifier.get_backbone_details(\"slowfast_r50\"))"
      ],
      "metadata": {
        "id": "-ByGuYoqwr_6",
        "outputId": "a6ce8190-41af-4adf-a35c-a9c9bda3e365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('pretrained', <Parameter \"pretrained: bool = False\">), ('progress', <Parameter \"progress: bool = True\">), ('kwargs', <Parameter \"**kwargs: Any\">)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it's time for us to create a task using a SlowFast architecture"
      ],
      "metadata": {
        "id": "OQYzNUDr8gzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slowr50_model = VideoClassifier(backbone=\"slowfast_r50\", labels=kinetics_videodatamodule.labels, pretrained=True)"
      ],
      "metadata": {
        "id": "kA4J7Upq8lKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning the model"
      ],
      "metadata": {
        "id": "R0oJXW1Oq50Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can fine-tune the model\n",
        "trainer = flash.Trainer(max_epochs=25, gpus=torch.cuda.device_count())\n",
        "trainer.finetune(slowr50_model, datamodule=kinetics_videodatamodule, strategy=\"freeze\")"
      ],
      "metadata": {
        "id": "phYJjRDyRHfh",
        "outputId": "40cb43b3-3a98-47c8-bfda-59da6ab7c129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673,
          "referenced_widgets": [
            "a95aa8ae1a854ece8670c4e377064d50",
            "561619d2e0a54ae9accf89177c4152d5",
            "6ead40f680e440f084530b3f19f26ccd",
            "996a76887e4c4302a9ec633ba4f9abb9",
            "90329988b9534b13a4dd3ad0cc31023d",
            "65b7fc62a96e41bb9ca427434a1e6cf0",
            "6cb8879b5f8b4fd3b2b86c9fae11b9ee",
            "61f2b7d7e4e242bb91751ec3ae37623f",
            "a736d1fb034446f281960cb36c72457f",
            "a93d48ae89a540bda323559fc2fac1cd",
            "6a28ac3abb04427382d34dd91de44987"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.distributed:GPU available: True, used: True\n",
            "INFO:pytorch_lightning.utilities.distributed:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.distributed:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name          | Type       | Params\n",
            "---------------------------------------------\n",
            "0 | train_metrics | ModuleDict | 0     \n",
            "1 | val_metrics   | ModuleDict | 0     \n",
            "2 | test_metrics  | ModuleDict | 0     \n",
            "3 | backbone      | Net        | 34.6 M\n",
            "4 | head          | Sequential | 2.0 K \n",
            "---------------------------------------------\n",
            "62.7 K    Trainable params\n",
            "34.5 M    Non-trainable params\n",
            "34.6 M    Total params\n",
            "138.274   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a95aa8ae1a854ece8670c4e377064d50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7b539993f749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now, we can fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslowr50_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkinetics_videodatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"freeze\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flash/core/trainer.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule, strategy, train_bn)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \"\"\"\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_bn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     def predict(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdl_max_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataloader_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flash/core/model.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         self.log_dict(\n\u001b[1;32m    415\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34mf\"val_{k}\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutputKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOGS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flash/video/classification/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, batch_idx, metrics)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flash/core/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, batch_idx, metrics)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mOutputKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flash/video/classification/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorchvideo/models/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorchvideo/models/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m         assert isinstance(\n\u001b[1;32m    109\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         ), \"input for MultiPathWayWithFuse needs to be a list of tensors\"\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: input for MultiPathWayWithFuse needs to be a list of tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Making predictions"
      ],
      "metadata": {
        "id": "rJGUaF3lHSjw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOGzkjBxHRa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's the model and check the accuracy on the test dataset\n",
        "trainer.test()"
      ],
      "metadata": {
        "id": "bAEG0XqMphBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that we have been able to achieve 60% accuracy on the testing dataset\n",
        "with just 10 epochs and without any hyperparameter tuning. \n",
        "\n",
        "A model improvement\n",
        "exercise on HealthClaimClassifier might even help us in getting much better\n",
        "accuracy with the pre-trained BERT model."
      ],
      "metadata": {
        "id": "VJ2KH2z4Am1r"
      }
    }
  ]
}