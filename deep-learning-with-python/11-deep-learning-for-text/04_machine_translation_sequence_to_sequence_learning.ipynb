{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-machine-translation--sequence-to-sequence-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP37iNeEDt0GRUgpy7E5gNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/04_machine_translation_sequence_to_sequence_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Machine translation: A sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "Pka5_bIxkQdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you’ll deepen your expertise by learning about\n",
        "sequence-to-sequence models.\n",
        "\n",
        "A sequence-to-sequence model takes a sequence as input (often a sentence or\n",
        "paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n",
        "- **Machine translation**—Convert a paragraph in a source language to its equivalent in a target language.\n",
        "- **Text summarization**—Convert a long document to a shorter version that retains the most important information.\n",
        "- **Question answering**—Convert an input question into its answer.\n",
        "- **Chatbots**—Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
        "- **Text generation**—Convert a text prompt into a paragraph that completes the prompt.\n",
        "\n",
        "The general template behind sequence-to-sequence models is described in figure.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/3.png?raw=1' width='600'/>\n",
        "\n",
        "During training:-\n",
        "- An `encoder` model turns the source sequence into an intermediate representation.\n",
        "- A `decoder` is trained to predict the next token i in the target sequence by looking at both previous tokens `(0 to i - 1)` and the encoded source sequence.\n",
        "\n",
        "**During inference, we don’t have access to the target sequence**—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
        "\n",
        "- We obtain the encoded source sequence from the encoder.\n",
        "- The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string `[start]`), and uses them to predict the first real token in the sequence.\n",
        "- The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string\n",
        "`[end]`).\n",
        "\n",
        "Everything you’ve learned so far can be repurposed to build this new kind of model.\n",
        "\n",
        "Let’s dive in.\n"
      ],
      "metadata": {
        "id": "BHePHVWuk-6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "KkxNMWG1mnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XUDQfXo9mpJp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll be working with an English-to-Spanish translation dataset available at\n",
        "www.manythings.org/anki/. \n",
        "\n",
        "Let’s download it:"
      ],
      "metadata": {
        "id": "NGB_nEcMnipW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "MjRO78CmnnH7",
        "outputId": "e72e0460-c681-4c61-e5db-35754e872c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 04:36:25--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 74.125.140.128, 108.177.15.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-02-03 04:36:26 (174 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "DeMzuQbmo4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. \n",
        "\n",
        "Let’s parse this file."
      ],
      "metadata": {
        "id": "qWjNE2oLo7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "  # Each line contains an English phrase and its Spanish translation, tab-separated.\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  # We prepend \"[start]\" and append \"[end]\" to the Spanish sentence, to match the template\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "QcKV-E2rpKfx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `text_pairs` look like this:"
      ],
      "metadata": {
        "id": "6PYcBUf5rBzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ts-AXBMzrEeq",
        "outputId": "bb539e58-a35f-40c4-b11f-aeded67a866b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"In my opinion, it's better to change the policy.\", '[start]A mi entender es mejor cambiar de procedimiento.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "ZKHJ29-JrVaI",
        "outputId": "ece8d731-90ca-4b11-8e35-99e9a263ad04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Mary was arrested for shoplifting.', '[start]Mary fue arrestada por ratera.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s shuffle them and split them into the usual training, validation, and test sets:"
      ],
      "metadata": {
        "id": "TgG4WIZ1rTIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "_xCJvrN7rT0k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s prepare two separate TextVectorization layers: one for English and one for Spanish. \n",
        "\n",
        "We’re going to need to customize the way strings are preprocessed:"
      ],
      "metadata": {
        "id": "ZScoIPTZuKY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a custom string standardization function for the Spanish TextVectorization layer: it preserves [ and ] \n",
        "# but strips ¿ (as well as all other characters from strings.punctuation).\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "# Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.\n",
        "target_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", \n",
        "                                                output_sequence_length=sequence_length + 1,\n",
        "                                                standardize=custom_standardization)\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# Learn the vocabulary of each language\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "Wh7IlyfVuNSl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can turn our data into a tf.data pipeline. \n",
        "\n",
        "We want it to return a tuple `(inputs, target)` where `inputs` is a dict with two keys,`encoder_inputs` (the English sentence) and `decoder_inputs` (the Spanish sentence), and `target` is the Spanish sentence offset by one step ahead."
      ],
      "metadata": {
        "id": "flQpu1pcwxpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return (\n",
        "      {\n",
        "      \"english\": eng,\n",
        "      \"spanish\": spa[:, :-1],  # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length\n",
        "      },\n",
        "      spa[:, 1:]   # The target Spanish sentence is one step ahead. Both are still the same length (20 words)\n",
        "  )"
      ],
      "metadata": {
        "id": "lRd8xb_uw68P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  # Use in-memory caching to speed up preprocessing\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "metadata": {
        "id": "mQNGKIVKzAF7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "FatVJpipzx8c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what our dataset outputs look like:"
      ],
      "metadata": {
        "id": "B_Bd8zTL0EpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "TkPwTeeH0Gnu",
        "outputId": "d26099be-fa9e-4ae3-f250-5d8d32cda2d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now ready—time to build some models. We’ll start with a recurrent\n",
        "sequence-to-sequence model before moving on to a Transformer."
      ],
      "metadata": {
        "id": "3sz5nJvQ0sBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "wc2FZshD0spZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step. \n",
        "\n",
        "In Keras, it would look like this:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "However, there are two major issues with this approach:\n",
        "\n",
        "* The target sequence must always be the same length as the source sequence.\n",
        "* Due to the step-by-step nature of RNNs, the model will only be looking at\n",
        "tokens `0…N` in the source sequence in order to predict token N in the target\n",
        "sequence. This constraint makes this setup unsuitable for most tasks, and\n",
        "particularly translation.\n",
        "\n",
        "If you’re a human translator, you’d start by reading the entire source sentence before\n",
        "starting to translate it. This is especially important if you’re dealing with languages\n",
        "that have wildly different word ordering, like English and Japanese. And that’s exactly\n",
        "what standard sequence-to-sequence models do.\n",
        "\n",
        "In a proper sequence-to-sequence setup, you would first use an\n",
        "RNN (the encoder) to turn the entire source sequence into a single vector (or set of\n",
        "vectors). \n",
        "\n",
        "This could be the last output of the RNN, or alternatively, its final internal\n",
        "state vectors. \n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/4.png?raw=1' width='600'/>\n",
        "\n",
        "Then you would use this vector (or vectors) as the `initial state` of another RNN (the decoder), which would look at elements `0…N` in the target sequence, and\n",
        "try to predict step `N+1` in the target sequence.\n",
        "\n",
        "Let’s implement this in Keras with GRU-based encoders and decoders. The choice\n",
        "of GRU rather than LSTM makes things a bit simpler, since GRU only has a single\n",
        "state vector, whereas LSTM has multiple. \n",
        "\n",
        "Let’s start with the encoder."
      ],
      "metadata": {
        "id": "64zlIq2T0wkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024"
      ],
      "metadata": {
        "id": "kfQrePqY6SEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The English source sentence goes here.\n",
        "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\n",
        "# Don’t forget masking: it’s critical in this setup\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "RYOcpMqK4Fpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s add the decoder—a simple GRU layer that takes as its initial state the encoded source sentence. \n",
        "\n",
        "On top of it, we add a Dense layer that produces for each\n",
        "output step a probability distribution over the Spanish vocabulary."
      ],
      "metadata": {
        "id": "6yPp5_kW-TFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Spanish target sentence goes here\n",
        "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "# The encoded source sentence serves as the initial state of the decoder GRU\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predicts the next token\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future\n",
        "seq2seq_rnn = keras.Model(inputs=[source, past_target], outputs=target_next_step)"
      ],
      "metadata": {
        "id": "kaP45JT8-bFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, the decoder takes as input the entire target sequence, but thanks to\n",
        "the step-by-step nature of RNNs, it only looks at tokens `0…N` in the input to predict token N in the output (which corresponds to the next token in the sequence, since\n",
        "the output is intended to be offset by one step). \n",
        "\n",
        "This means we only use information\n",
        "from the past to predict the future, as we should; otherwise we’d be cheating, and our\n",
        "model would not work at inference time.\n",
        "\n",
        "Let’s start training."
      ],
      "metadata": {
        "id": "8Hhq-ul-Bk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "XxMp-usaB2WW",
        "outputId": "82f5133b-41de-44ae-e6ee-5e6b9d4cf0d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 132s 91ms/step - loss: 1.4797 - accuracy: 0.3238 - val_loss: 1.2083 - val_accuracy: 0.4242\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 1.2150 - accuracy: 0.4467 - val_loss: 1.0688 - val_accuracy: 0.4953\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 121s 93ms/step - loss: 1.0937 - accuracy: 0.5035 - val_loss: 0.9998 - val_accuracy: 0.5309\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 115s 89ms/step - loss: 1.0118 - accuracy: 0.5391 - val_loss: 0.9691 - val_accuracy: 0.5504\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 117s 90ms/step - loss: 0.9689 - accuracy: 0.5658 - val_loss: 0.9580 - val_accuracy: 0.5601\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 121s 93ms/step - loss: 0.9418 - accuracy: 0.5889 - val_loss: 0.9576 - val_accuracy: 0.5663\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 123s 94ms/step - loss: 0.9254 - accuracy: 0.6050 - val_loss: 0.9611 - val_accuracy: 0.5696\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 122s 94ms/step - loss: 0.9133 - accuracy: 0.6179 - val_loss: 0.9655 - val_accuracy: 0.5706\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 118s 91ms/step - loss: 0.9049 - accuracy: 0.6275 - val_loss: 0.9687 - val_accuracy: 0.5731\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8990 - accuracy: 0.6346 - val_loss: 0.9723 - val_accuracy: 0.5741\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 116s 89ms/step - loss: 0.8948 - accuracy: 0.6397 - val_loss: 0.9763 - val_accuracy: 0.5748\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8915 - accuracy: 0.6437 - val_loss: 0.9810 - val_accuracy: 0.5734\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8899 - accuracy: 0.6462 - val_loss: 0.9836 - val_accuracy: 0.5744\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8885 - accuracy: 0.6480 - val_loss: 0.9847 - val_accuracy: 0.5745\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 115s 88ms/step - loss: 0.8881 - accuracy: 0.6491 - val_loss: 0.9870 - val_accuracy: 0.5750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4cd5975d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We picked accuracy as a crude way to monitor validation-set performance during\n",
        "training. We get to 64% accuracy: on average, the model predicts the next word in the\n",
        "Spanish sentence correctly 64% of the time. However, in practice, next-token accuracy\n",
        "isn’t a great metric for machine translation models.\n",
        "\n",
        "If you work on a real-world machine translation system, you will likely use `BLEU scores` to evaluate your models—a metric that looks at entire generated sequences\n",
        "and that seems to correlate well with human perception of translation quality.\n",
        "\n",
        "At last, let’s use our model for inference.\n",
        "\n",
        "We’ll pick a few sentences in the test set\n",
        "and check how our model translates them. We’ll start from the seed token, `[start]`,\n",
        "and feed it into the decoder model, together with the encoded English source sentence.\n",
        "\n",
        "We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoder\n",
        "repeatedly, sampling one new target token at each iteration, until we get to `[end]`\n",
        "or reach the maximum sentence length."
      ],
      "metadata": {
        "id": "4-_XLXPLC40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a dict to convert token index predictions to string tokens\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  # seed token\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    # sample the next token\n",
        "    next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    # Convert the next token prediction to a string and append it to the generated sentence.\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "    # Exit condition: either hit max length or sample a stop character\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "GP4YWv07DkXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "M-zD2DzwDyhs",
        "outputId": "a0143ba0-1d86-48bf-daec-ad765cd0fa4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "How big you are!\n",
            "[start]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Why don't you leave, Tom?\n",
            "[start] no te por tom[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom wants to stay here.\n",
            "[start] tom quiere aquí[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We love you.\n",
            "[start]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "There were a lot of boats on the lake.\n",
            "[start] muchos en el mary del error[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Let's play tennis in the afternoon.\n",
            "[start] al tenis por la tarde[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "This is the worst of all.\n",
            "[start] es el mejor de la para nada[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I'll be in trouble if the story gets out.\n",
            "[start] en problemas si la historia se [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I jumped up and down.\n",
            "[start] y [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I need you here in case something else happens.\n",
            "[start] que estás aquí en otra [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom winked.\n",
            "[start] un [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "She sent her children off to school.\n",
            "[start] a sus niños a la escuela[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Here's some water.\n",
            "[start] algo de agua[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "That short woman over there is my mother.\n",
            "[start] de allí es [UNK] de mi madre[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "My father has lived in Nagoya for 30 years.\n",
            "[start] ha padre ha [UNK] en hace tres años[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom's response was immediate.\n",
            "[start] la fue de tom fue de gran no al muy [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "When does the performance begin?\n",
            "[start] la [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We're out of time.\n",
            "[start] de tiempo[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We went to the beach.\n",
            "[start] a la playa[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I wrote a letter last night.\n",
            "[start] una carta anoche[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this inference setup, while very simple, is rather inefficient, since we reprocess\n",
        "the entire source sentence and the entire generated target sentence every time\n",
        "we sample a new word.\n",
        "\n",
        "In a practical application, you’d factor the encoder and the\n",
        "decoder as two separate models, and your decoder would only run a single step at\n",
        "each token-sampling iteration, reusing its previous internal state.\n",
        "\n",
        "There are many ways this toy model could be improved: \n",
        "\n",
        "* We could use a deep stack of\n",
        "recurrent layers for both the encoder and the decoder (note that for the decoder, this makes state management a bit more involved). \n",
        "* We could use an LSTM instead of a GRU. And so on. \n",
        "\n",
        "Beyond such tweaks, however, the RNN approach to sequence-to-sequence\n",
        "learning has a few fundamental limitations:\n",
        "\n",
        "* The source sequence representation has to be held entirely in the encoder state\n",
        "vector(s), which puts significant limitations on the size and complexity of the\n",
        "sentences you can translate. It’s a bit as if a human were translating a sentence\n",
        "entirely from memory, without looking twice at the source sentence while producing\n",
        "the translation.\n",
        "\n",
        "* RNNs have trouble dealing with very long sequences, since they tend to progressively\n",
        "forget about the past—by the time you’ve reached the 100th token in\n",
        "either sequence, little information remains about the start of the sequence.That means RNN-based models can’t hold onto long-term context, which can\n",
        "be essential for translating long documents.\n",
        "\n",
        "These limitations are what has led the machine learning community to embrace the\n",
        "Transformer architecture for sequence-to-sequence problems."
      ],
      "metadata": {
        "id": "508No_6uEMBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-to-sequence learning with Transformer"
      ],
      "metadata": {
        "id": "EGRK2tdiFJWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence-to-sequence learning is the task where Transformer really shines. Neural\n",
        "attention enables Transformer models to successfully process sequences that are con\n",
        "siderably\n",
        "longer and more complex than those RNNs can handle.\n",
        "\n",
        "Look at the decoder\n",
        "internals: you’ll recognize that it looks very similar to the Transformer encoder, except\n",
        "that an extra attention block is inserted between the self-attention block applied to\n",
        "the target sequence and the dense layers of the exit block.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/5.png?raw=1' width='600'/>\n",
        "\n",
        "\n",
        "Let’s implement it. Like for the TransformerEncoder, we’ll use a Layer subclass."
      ],
      "metadata": {
        "id": "4XZJdFnQFLUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Positional embedding"
      ],
      "metadata": {
        "id": "oUXZqJ6KBYzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "\n",
        "  # A downside of position embeddings is that the sequence length needs to be known in advance\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    # Prepare an Embedding layer for the token indices.\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "    # And another one for the token positions\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    # add both embedding vectors together\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    \"\"\"\n",
        "    Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. \n",
        "    The compute_mask method will called automatically by the framework, and the mask will get propagated to the next layer.\n",
        "    \"\"\"\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "KQ8eY7jXBgAA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer encoder"
      ],
      "metadata": {
        "id": "z4EwXfAdBMRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  \n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim    # Size of the input token vectors\n",
        "    self.dense_dim = dense_dim    # Size of the inner dense layer\n",
        "    self.num_heads = num_heads  # Number of attention heads\n",
        "\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_projection = keras.Sequential([\n",
        "         layers.Dense(dense_dim, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim)                                     \n",
        "    ])\n",
        "\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    # The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    projection_input = self.layernorm_1(inputs + attention_output)\n",
        "    projection_output = self.dense_projection(projection_input)\n",
        "    return self.layernorm_2(projection_input + projection_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "TU0I0kAdBOJ7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer decoder"
      ],
      "metadata": {
        "id": "2F08S5JdBPkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "\n",
        "      self.multi_head_attention_layer_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.multi_head_attention_layer_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "      \n",
        "      self.dense_projection = keras.Sequential([\n",
        "           layers.Dense(dense_dim, activation=\"relu\"),\n",
        "           layers.Dense(embed_dim, )                                     \n",
        "      ])\n",
        "\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "      self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "      # This attribute ensures that the layer will propagate its input mask to its outputs\n",
        "      self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          \"embed_dim\": self.embed_dim,\n",
        "          \"num_heads\": self.num_heads,\n",
        "          \"dense_dim\": self.dense_dim\n",
        "      })\n",
        "      return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    \"\"\"\n",
        "    Causal padding is absolutely critical to successfully training a sequence-to-sequence Transformer.\n",
        "    we’ll mask the upper half of the pairwise attention matrix to prevent the model from paying any attention \n",
        "    to information from the future only information from tokens 0...N in the target sequence should be used \n",
        "    when generating target token N+1.\n",
        "    \"\"\"\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    # Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    # Replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length)\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    # Retrieve the causal mask\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    # Prepare the input mask (that describes padding locations in the target sequence)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      # Merge the two masks together\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    # Pass the causal mask to the first attention layer, which performs self-attention over the target sequence\n",
        "    attention_output_1 = self.multi_head_attention_layer_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
        "    # Pass the combined mask to the second attention layer, which relates the source sequence to the target sequence\n",
        "    attention_output_2 = self.multi_head_attention_layer_2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
        "    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "    projection_output = self.dense_projection(attention_output_2)\n",
        "\n",
        "    return self.layernorm_3(attention_output_2 + projection_output)"
      ],
      "metadata": {
        "id": "72mYZvEnihqh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The end-to-end Transformer is the model we’ll be training. It maps the source\n",
        "sequence and the target sequence to the target sequence one step in the future. It\n",
        "straightforwardly combines the pieces we’ve built so far: \n",
        "\n",
        "- `PositionalEmbedding` layers\n",
        "- `TransformerEncoder` layers \n",
        "- `TransformerDecoder` layers\n",
        "\n",
        "Note that both the `TransformerEncoder`\n",
        "and the `TransformerDecoder` are shape-invariant, so you could be\n",
        "stacking many of them to create a more powerful encoder or decoder."
      ],
      "metadata": {
        "id": "unW7Je3g-ZBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8"
      ],
      "metadata": {
        "id": "waF24iKH-wbg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "# Encode the source sentence\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "# Encode the target sentence and combine it with the encoded source sentence\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predict a word for each output position\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "ms66vW94-1Ox"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re now ready to train our model—we get to 67% accuracy, a good deal above the GRU-based model."
      ],
      "metadata": {
        "id": "8-QECaHrEGsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "5lQ0bpSHEIOo",
        "outputId": "1879f84a-9044-469a-e26a-3876638f3a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 177s 130ms/step - loss: 1.7545 - accuracy: 0.2146 - val_loss: 1.5486 - val_accuracy: 0.2681\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 168s 129ms/step - loss: 1.5504 - accuracy: 0.2933 - val_loss: 1.4416 - val_accuracy: 0.3306\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.4473 - accuracy: 0.3396 - val_loss: 1.3481 - val_accuracy: 0.3610\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.3719 - accuracy: 0.3727 - val_loss: 1.2876 - val_accuracy: 0.3918\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.3030 - accuracy: 0.4013 - val_loss: 1.2285 - val_accuracy: 0.4159\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.2481 - accuracy: 0.4250 - val_loss: 1.1764 - val_accuracy: 0.4391\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.2123 - accuracy: 0.4449 - val_loss: 1.1509 - val_accuracy: 0.4551\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.2107 - accuracy: 0.4607 - val_loss: 1.1679 - val_accuracy: 0.4660\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.2033 - accuracy: 0.4751 - val_loss: 1.1568 - val_accuracy: 0.4760\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.1860 - accuracy: 0.4882 - val_loss: 1.1437 - val_accuracy: 0.4845\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.1750 - accuracy: 0.4962 - val_loss: 1.1359 - val_accuracy: 0.4918\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.1627 - accuracy: 0.5041 - val_loss: 1.1252 - val_accuracy: 0.4985\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.1509 - accuracy: 0.5114 - val_loss: 1.1127 - val_accuracy: 0.5062\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.1397 - accuracy: 0.5186 - val_loss: 1.1126 - val_accuracy: 0.5033\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.1272 - accuracy: 0.5251 - val_loss: 1.1068 - val_accuracy: 0.5088\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.1182 - accuracy: 0.5299 - val_loss: 1.1023 - val_accuracy: 0.5109\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.1112 - accuracy: 0.5340 - val_loss: 1.1034 - val_accuracy: 0.5093\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.1031 - accuracy: 0.5377 - val_loss: 1.0955 - val_accuracy: 0.5157\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0946 - accuracy: 0.5418 - val_loss: 1.0952 - val_accuracy: 0.5140\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0855 - accuracy: 0.5467 - val_loss: 1.0893 - val_accuracy: 0.5185\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0802 - accuracy: 0.5489 - val_loss: 1.0878 - val_accuracy: 0.5191\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0759 - accuracy: 0.5518 - val_loss: 1.0928 - val_accuracy: 0.5165\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0689 - accuracy: 0.5569 - val_loss: 1.0848 - val_accuracy: 0.5229\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0604 - accuracy: 0.5606 - val_loss: 1.0854 - val_accuracy: 0.5231\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0507 - accuracy: 0.5657 - val_loss: 1.0820 - val_accuracy: 0.5257\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 165s 126ms/step - loss: 1.0420 - accuracy: 0.5698 - val_loss: 1.0826 - val_accuracy: 0.5255\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0388 - accuracy: 0.5708 - val_loss: 1.0834 - val_accuracy: 0.5247\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0326 - accuracy: 0.5732 - val_loss: 1.0798 - val_accuracy: 0.5293\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0237 - accuracy: 0.5778 - val_loss: 1.0762 - val_accuracy: 0.5288\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 1.0150 - accuracy: 0.5815 - val_loss: 1.0797 - val_accuracy: 0.5287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff9704eb950>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Translation evaluation"
      ],
      "metadata": {
        "id": "_a8jEXwUJChp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let’s try using our model to translate never-seen-before English sentences from\n",
        "the test set. \n",
        "\n",
        "The setup is identical to what we used for the sequence-to-sequence RNN\n",
        "model."
      ],
      "metadata": {
        "id": "dHPUYeQ1Ixvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a dict to convert token index predictions to string tokens\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  # seed token\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "    # sample the next token\n",
        "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "    # Convert the next token prediction to a string and append it to the generated sentence.\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "    # Exit condition: either hit max length or sample a stop character\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "uNzOcQZFIy6J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "Za3ejV1FI6wu",
        "outputId": "d6122036-cdad-4b71-cc88-0c8b334118b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "We enjoyed the party very much.\n",
            "[start] la fiesta muy te [UNK]               \n",
            "-\n",
            "The neck of the bottle was broken.\n",
            "[start] el [UNK] de la situación fue [UNK]  [UNK]           la\n",
            "-\n",
            "Tom appreciated Mary's kindness.\n",
            "[start] la que mary [UNK] la [UNK] de mary[end]            \n",
            "-\n",
            "Tom is banned from entering this building.\n",
            "[start] de [UNK] de este [UNK]  en de esta en de [UNK] en       en\n",
            "-\n",
            "The other day, I bought a camera.\n",
            "[start] una vez un cámara de cámara[end]              \n",
            "-\n",
            "She is very free with her money.\n",
            "[start] es muy viejo con tu dinero[end]              \n",
            "-\n",
            "Would you like another apple?\n",
            "[start] una [UNK]                  \n",
            "-\n",
            "What are you going to say?\n",
            "[start] lo que dice eso[end]                \n",
            "-\n",
            "We can pay 200 dollars at most.\n",
            "[start] palabras para los [UNK] de la gente más [UNK]  que del a el      de\n",
            "-\n",
            "The more you have, the more you want.\n",
            "[start] más que menos tienen dinero que quieras[end]             \n",
            "-\n",
            "I am as sad and lonely as can be.\n",
            "[start] tan rápido y puede estar tan rápido así[end]            \n",
            "-\n",
            "How was today's game?\n",
            "[start] fue el fútbol[end]                 \n",
            "-\n",
            "She became happy.\n",
            "[start] se hizo feliz[end]                 \n",
            "-\n",
            "Do you want to send a message?\n",
            "[start] bien es un mensaje para [UNK]              \n",
            "-\n",
            "He's waiting for you at home.\n",
            "[start]  aquí[end]                  \n",
            "-\n",
            "Tom applauded.\n",
            "[start]  a                  \n",
            "-\n",
            "I have to win.\n",
            "[start] tengo que                  \n",
            "-\n",
            "There's no more salt.\n",
            "[start] no hay nada de la leche en la leche[end]           \n",
            "-\n",
            "I ate three hot dogs.\n",
            "[start] tres [UNK] y los perros[end]               \n",
            "-\n",
            "It wasn't a pleasant job.\n",
            "[start] no era un trabajo[end]                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subjectively, the Transformer seems to perform significantly better than the GRUbased\n",
        "translation model. \n",
        "\n",
        "It’s still a toy model, but it’s a better toy model."
      ],
      "metadata": {
        "id": "BP1-i5K9LGPD"
      }
    }
  ]
}