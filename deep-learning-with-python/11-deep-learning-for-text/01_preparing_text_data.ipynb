{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-preparing-text-data.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNKeXBaTSIpQgpgXDfHLQqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/01_preparing_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing text data"
      ],
      "metadata": {
        "id": "MDJbbxiRu98c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. \n",
        "\n",
        "**Vectorizing text is the process of transforming\n",
        "text into numeric tensors.**\n",
        "\n",
        " Text vectorization processes come in many shapes and\n",
        "forms, but they all follow the same template.\n",
        "\n",
        "- First, you standardize the text to make it easier to process, such as by converting\n",
        "it to lowercase or removing punctuation.\n",
        "- You split the text into units (called tokens), such as characters, words, or groups\n",
        "of words. This is called tokenization.\n",
        "- You convert each such token into a numerical vector. This will usually involve\n",
        "first indexing all tokens present in the data.\n",
        "\n",
        "<img src='https://github.com/rahiakela/data-learning-research-and-practice/blob/main/deep-learning-with-python-by-francois-chollet/11-deep-learning-for-text/images/1.png?raw=1' width='800'/>"
      ],
      "metadata": {
        "id": "GryfCpykvAQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text standardization"
      ],
      "metadata": {
        "id": "jh8RoIkowA17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text standardization is a basic form of feature engineering that aims to erase\n",
        "encoding differences that you don’t want your model to have to deal with. It’s not\n",
        "exclusive to machine learning, either—you’d have to do the same thing if you were\n",
        "building a search engine.\n",
        "\n",
        "One of the simplest and most widespread standardization schemes is “convert to\n",
        "lowercase and remove punctuation characters.”\n",
        "\n",
        "Of course, standardization may\n",
        "also erase some amount of information, so always keep the context in mind: for\n",
        "instance, if you’re writing a model that extracts questions from interview articles, it\n",
        "should definitely treat “?” as a separate token instead of dropping it, because it’s a useful\n",
        "signal for this specific task."
      ],
      "metadata": {
        "id": "iN8seLaqwBg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text splitting (tokenization)"
      ],
      "metadata": {
        "id": "GNH-ai5Axqhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your text is standardized, you need to break it up into units to be vectorized\n",
        "(tokens), a step called tokenization. You could do this in three different ways:\n",
        "\n",
        "- Word-level tokenization\n",
        "- N-gram tokenization\n",
        "- Character-level tokenization\n",
        "\n",
        "In general, you’ll always use either word-level or N-gram tokenization. There are two kinds of text-processing models: \n",
        "\n",
        "- those that care about word order, called **sequence models**,\n",
        "- those that treat input words as a set, discarding their original order, \n",
        "called **bag-of-words models**\n",
        "\n",
        "If you’re building a sequence model, you’ll use word-level tokeni\n",
        "zation, and if you’re building a bag-of-words model, you’ll use N-gram tokenization.\n",
        "\n",
        "N-grams are a way to artificially inject a small amount of local word order information into the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LMLrEad4xs63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vocabulary indexing"
      ],
      "metadata": {
        "id": "EB5HbxKQyaTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your text is split into tokens, you need to encode each token into a numerical\n",
        "representation. \n",
        "\n",
        "You could potentially do this in a stateless way, such as by hashing each\n",
        "token into a fixed binary vector, but in practice, the way you’d go about it is to build\n",
        "an index of all terms found in the training data (the “vocabulary”), and assign a\n",
        "unique integer to each entry in the vocabulary."
      ],
      "metadata": {
        "id": "Ed5TuRoMya66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {}\n",
        "for text in dataset:\n",
        "  text = standardize(text)\n",
        "  tokens = tokenize(text)\n",
        "  for token in tokens:\n",
        "    if token not in vocabulary:\n",
        "      vocabulary[token] = len(vocabulary)"
      ],
      "metadata": {
        "id": "HtyQypdiyt_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then convert that integer into a vector encoding that can be processed by a\n",
        "neural network, like a one-hot vector:"
      ],
      "metadata": {
        "id": "oCCOnRrczEyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_token(token):\n",
        "  vector = np.zeros((len(vocabulary), ))\n",
        "  token_index = vocabulary[token]\n",
        "  vector[token_index] = 1\n",
        "  return vector"
      ],
      "metadata": {
        "id": "-GaKlR6yzIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the TextVectorization layer"
      ],
      "metadata": {
        "id": "Kt_wILLKzguQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a Text Vectorization class."
      ],
      "metadata": {
        "id": "9uctOBzMzhfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "B70gBx6154sm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer:\n",
        "\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_seq):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_seq)"
      ],
      "metadata": {
        "id": "6035PEsN56Iu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = Vectorizer()"
      ],
      "metadata": {
        "id": "DdS4NWyG8jW3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "   \"I write, erase, rewrite\",\n",
        "   \"Erase again, and then\",\n",
        "   \"A poppy blooms.\"        \n",
        "]\n",
        "\n",
        "vectorizer.make_vocabulary(dataset)"
      ],
      "metadata": {
        "id": "IsMaIdJL8o-Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does the job:"
      ],
      "metadata": {
        "id": "w_YGRI4085dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "id": "wd1VAR7u8526",
        "outputId": "6f6785ca-b3e0-482a-80e3-e6b4b5145977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "id": "9-Sm-Xiv9aIZ",
        "outputId": "0930fbf2-15b4-475c-c1ab-3c9e52350607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, you’ll\n",
        "work with the Keras `TextVectorization` layer, which is fast and efficient and can be\n",
        "dropped directly into a `tf.data` pipeline or a Keras model.\n",
        "\n",
        "This is what the TextVectorization layer looks like:"
      ],
      "metadata": {
        "id": "lIcH2oLa-CCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "GxgUjKJV-Cod"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configures the layer to return sequences of words encoded as integer indices.\n",
        "text_vectorization = TextVectorization(output_mode=\"int\")"
      ],
      "metadata": {
        "id": "9Y9ZEmcS-J2y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the `TextVectorization` layer will use the setting “convert to lowercase and\n",
        "remove punctuation” for text standardization, and “split on whitespace” for tokenization.\n",
        "\n",
        "But importantly, you can provide custom functions for standardization and tokenization,\n",
        "which means the layer is flexible enough to handle any use case. \n",
        "\n",
        "Note that\n",
        "such custom functions should operate on tf.string tensors, not regular Python\n",
        "strings!"
      ],
      "metadata": {
        "id": "xFFa7-qg-rs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "QLdfe1Pf-zOm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization_fn(string_tensor): \n",
        "  # Convert strings to lowercase\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  # Replace punctuation characters with the empty string\n",
        "  return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  # Split strings on whitespace.\n",
        "  return tf.strings.split(string_tensor)"
      ],
      "metadata": {
        "id": "4U1MCph7-5c1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(output_mode=\"int\",\n",
        "                                       standardize=custom_standardization_fn,\n",
        "                                       split=custom_split_fn)"
      ],
      "metadata": {
        "id": "32xg0Qzm_q5I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To index the vocabulary of a text corpus, just call the\n",
        " `adapt()` method of the layer with a Dataset object that yields strings, or just with a list of Python strings:"
      ],
      "metadata": {
        "id": "qQJeNsoQBLC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "   \"I write, erase, rewrite\",\n",
        "   \"Erase again, and then\",\n",
        "   \"A poppy blooms.\"        \n",
        "]\n",
        "\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "a7q2kvKlBfgW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you can retrieve the computed vocabulary via `get_vocabulary()`—this can be useful if you need to convert text encoded as integer sequences back into words.\n",
        "\n",
        "The first two entries in the vocabulary are the mask token (index 0) and the OOV\n",
        "token (index 1). \n",
        "\n",
        "Entries in the vocabulary list are sorted by frequency, so with a realworld\n",
        "dataset, very common words like “the” or “a” would come first."
      ],
      "metadata": {
        "id": "ZdBksZrMB45b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "id": "Io5ypBHCB_sZ",
        "outputId": "0519175e-938e-441e-fc05-875317a71571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a demonstration, let’s try to encode and then decode an example sentence:"
      ],
      "metadata": {
        "id": "qF8ZB1lNCJR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "id": "hbvJb20uCJw7",
        "outputId": "137e7951-b1f4-4cdf-d741-b03a007ef6bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "id": "JO5-EbbSCgwD",
        "outputId": "3ecda4f4-2863-4c46-83ff-b26411b70ff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, because `TextVectorization` is mostly a dictionary lookup operation, it\n",
        "can’t be executed on a GPU (or TPU)—only on a CPU. \n",
        "\n",
        "So if you’re training your model\n",
        "on a GPU, your `TextVectorization` layer will run on the CPU before sending its output\n",
        "to the GPU. This has important performance implications."
      ],
      "metadata": {
        "id": "clr1P6RyC_AE"
      }
    }
  ]
}