{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxv9+BwKNwTOIWUS47GTnv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/11_fine_tuning_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-Tuning GPT-2"
      ],
      "metadata": {
        "id": "I_fJ4_FUlY0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the next element in a sequence is exactly what a Transformer decoder\n",
        "does, so it should be no surprise that GPT-2 is actually a Transformer decoder.\n",
        "\n",
        "\n",
        "\n",
        "We’ll start our NLP journey by following the steps of Alice and Dorothy, from\n",
        "[Alice’s Adventures in Wonderland](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/1476) by Lewis Carroll and [The Wonderful Wizard of Oz](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/1740) by L. Frank Baum.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/alice_dorothy.png?raw=1)\n",
        "\n",
        "*Left: \"Alice and the Baby Pig\" illustration by John Tenniel's, from \"Alice's Adventure's in Wonderland\" (1865).*\n",
        "\n",
        "*Right: \"Dorothy meets the Cowardly Lion\" illustration by W.W. Denslow, from \"The Wonderful Wizard of Oz\" (1900)*\n"
      ],
      "metadata": {
        "id": "6S3AN5ZklZkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "y_fsZxO6mSi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter11()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter11 import *"
      ],
      "metadata": {
        "id": "4KaJRDNLmTq_",
        "outputId": "bfc021cd-a471-47f2-f7de-8b4d5a0bcacb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "eotRrneKk78G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import errno\n",
        "import requests\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "from data_generation.nlp import ALICE_URL, WIZARD_URL, download_text\n",
        "from stepbystep.v4 import StepByStep\n",
        "# These are the classes we built in Chapter 10\n",
        "from seq2seq import *\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "i9pPl_87mbqd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Split\n",
        "from transformers import (\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BertModel, BertTokenizer, BertForSequenceClassification,\n",
        "    DistilBertModel, DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModel, AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, pipeline, TextClassificationPipeline\n",
        ")\n",
        "from transformers.pipelines import SUPPORTED_TASKS"
      ],
      "metadata": {
        "id": "w1HKtpREXdlF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "AIwXdiEhTmok"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "beHZ9NYf-DNZ",
        "outputId": "d21f7ddc-eda6-4c61-dc0d-1791f577be3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading Books"
      ],
      "metadata": {
        "id": "VGxz6kEmX3ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data"
      ],
      "metadata": {
        "id": "4hr1Jm9kK9DO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's download data\n",
        "HOME_DIR = \"data\"\n",
        "download_text(ALICE_URL, HOME_DIR)\n",
        "download_text(WIZARD_URL, HOME_DIR)"
      ],
      "metadata": {
        "id": "eTXbiieBX4Sz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the downloaded data\n",
        "#!cat data/alice28-1476.txt"
      ],
      "metadata": {
        "id": "45chINE_YYtz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat data/wizoz10-1740.txt"
      ],
      "metadata": {
        "id": "uM-tdJNRZcnt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to remove these additions to the original texts:"
      ],
      "metadata": {
        "id": "msne6vRcZ14Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alice_file = os.path.join(HOME_DIR, \"alice28-1476.txt\")\n",
        "with open(alice_file, \"r\") as f:\n",
        "  # The actual texts of the books are contained between lines 105 and 3703\n",
        "  alice_text = \"\".join(f.readlines()[104:3704])\n",
        "\n",
        "wizard_file = os.path.join(HOME_DIR, \"wizoz10-1740.txt\")\n",
        "with open(wizard_file, \"r\") as f:\n",
        "  # The actual texts of the books are contained between lines 309 and 5099\n",
        "  wizard_text = \"\".join(f.readlines()[310:5100])"
      ],
      "metadata": {
        "id": "8z29BFQwZx4U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alice_text[:500])\n",
        "print(\"\\n\", \"#\"*70, \"\\n\")\n",
        "print(wizard_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNZmtkHNahrp",
        "outputId": "5f1c4c3a-5e15-4c38-c7d0-621fa0966612"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ALICE'S ADVENTURES IN WONDERLAND\n",
            "\n",
            "                          Lewis Carroll\n",
            "\n",
            "               THE MILLENNIUM FULCRUM EDITION 2.8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            CHAPTER I\n",
            "\n",
            "                      Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "  Alice was beginning to get very tired of sitting by her sister\n",
            "on the bank, and of having nothing to do:  once or twice she had\n",
            "peeped into the book her sister was reading, but it had no\n",
            "pictures or conversations in it, `and what is the use of a book,'\n",
            "thought Alice `w\n",
            "\n",
            " ###################################################################### \n",
            "\n",
            "                    THE WONDERFUL WIZARD OF OZ\n",
            "\n",
            "\n",
            "                          1.  The Cyclone\n",
            "\n",
            "\n",
            "    Dorothy lived in the midst of the great Kansas prairies, with\n",
            "Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's\n",
            "wife.  Their house was small, for the lumber to build it had to be\n",
            "carried by wagon many miles.  There were four walls, a floor and a\n",
            "roof, which made one room; and this room contained a rusty looking\n",
            "cookstove, a cupboard for the dishes, a table, three or four\n",
            "chairs, and th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can partially automate the removal of the extra lines by setting the real start and end lines of each text in a configuration file."
      ],
      "metadata": {
        "id": "zS5E4dECbotZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_cfg = \"\"\"fname,start,end\n",
        "alice28-1476.txt,104,3704\n",
        "wizoz10-1740.txt,310,5100\"\"\"\n",
        "bytes_written = open(os.path.join(HOME_DIR, 'lines.cfg'), 'w').write(text_cfg)"
      ],
      "metadata": {
        "id": "ayc3piYsbutL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence Tokenization"
      ],
      "metadata": {
        "id": "Bk4m73rMb0gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A token is a piece of a text, and to tokenize a text means to split\n",
        "it into pieces; that is, into a list of tokens.\n",
        "\n",
        "The most common kind of piece is a word.\n",
        "\n",
        "So, tokenizing a text usually means to\n",
        "split it into words using the white space as a separator."
      ],
      "metadata": {
        "id": "6w5mJGOFb1Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I'm following the white rabbit\"\n",
        "tokens = sentence.split(\" \")\n",
        "tokens"
      ],
      "metadata": {
        "id": "oFsqtdFkenM-",
        "outputId": "cf742699-1c96-4df4-f278-e34d25bfb11c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'following', 'the', 'white', 'rabbit']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do sentence tokenization, which means to split a text into its sentences."
      ],
      "metadata": {
        "id": "3WGtMb-Qg2Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_alice = sent_tokenize(alice_text)\n",
        "corpus_wizard = sent_tokenize(wizard_text)\n",
        "\n",
        "len(corpus_alice), (len(corpus_wizard))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg1Kg0Ung7aq",
        "outputId": "6c0a8622-6e06-45f7-8226-d96078b996ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1612, 2240)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check one sentence from the first corpus of text."
      ],
      "metadata": {
        "id": "1qSQVckXh3aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_alice[2]"
      ],
      "metadata": {
        "id": "mWQQFsanh4GS",
        "outputId": "42e177f5-79db-4d69-e5b8-bb1e2b4d8571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There was nothing so VERY remarkable in that; nor did Alice\\nthink it so VERY much out of the way to hear the Rabbit say to\\nitself, `Oh dear!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check one sentence from the second corpus of text."
      ],
      "metadata": {
        "id": "RIGvTC9-iFwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_wizard[30]"
      ],
      "metadata": {
        "id": "ojBtgVa1iGSs",
        "outputId": "8c1490f4-d29e-4e78-85ad-35572a38b7d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"There\\'s a cyclone coming, Em,\" he called to his wife.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset is going to be a collection of CSV files, one file for each book, with each\n",
        "CSV file containing one sentence per line.\n",
        "\n",
        "Therefore, we need to:\n",
        "\n",
        "* clean the line breaks to make sure each sentence is on one line only;\n",
        "* define an appropriate quote char to \"wrap\" the sentence such that the original commas and semicolons in the original text do not get misinterpreted as separation chars of the CSV file; and\n",
        "* add a second column to the CSV file to\n",
        "identify the original source of the sentence since we’ll be concatenating, and\n",
        "shuffling the sentences before training a model on our corpora.\n",
        "\n",
        "The sentence above should end up looking like this:\n",
        "```log\n",
        "\\\"There's a cyclone coming, Em,\" he called to his wife.\\,wizoz10-1740.txt\n",
        "```\n",
        "\n",
        "The function below does the grunt work of cleaning, splitting, and saving the\n",
        "sentences to a CSV file for us:"
      ],
      "metadata": {
        "id": "6_QMReIwkcaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_tokenize(source, quote_char=\"\\\\\", sep_char=\",\", include_header=True, include_source=True, extensions=(\"txt\"), **kwargs):\n",
        "  # If source is a folder, goes through all files inside it that match the desired extensions ('txt' by default)\n",
        "  if os.path.isdir(source):\n",
        "    filenames = [f for f in os.listdir(source) if os.path.isfile(os.path.join(source, f)) and os.path.splitext(f)[1][1:] in extensions]\n",
        "  elif isinstance(source, str):\n",
        "    filenames = [source]\n",
        "\n",
        "  # If there is a configuration file, builds a dictionary with the corresponding start and end lines of each text file\n",
        "  config_file = os.path.join(source, \"lines.cfg\")\n",
        "  config = {}\n",
        "  if os.path.exists(config_file):\n",
        "    with open(config_file, \"r\") as f:\n",
        "      rows = f.readlines()\n",
        "    for r in rows[1:]:\n",
        "      fname, start, end = r.strip().split(\",\")\n",
        "      config.update({fname: (int(start), int(end))})\n",
        "\n",
        "  new_fnames = []\n",
        "  # For each file of text\n",
        "  for fname in filenames:\n",
        "    # If there's a start and end line for that file, use it\n",
        "    try:\n",
        "        start, end = config[fname]\n",
        "    except KeyError:\n",
        "        start = None\n",
        "        end = None\n",
        "\n",
        "    # Opens the file, slices the configures lines (if any)\n",
        "    # cleans line breaks and uses the sentence tokenizer\n",
        "    with open(os.path.join(source, fname), 'r') as f:\n",
        "        contents = (''.join(f.readlines()[slice(start, end, None)]).replace('\\n', ' ').replace('\\r', ''))\n",
        "    corpus = sent_tokenize(contents, **kwargs)\n",
        "\n",
        "    # Builds a CSV file containing tokenized sentences\n",
        "    base = os.path.splitext(fname)[0]\n",
        "    new_fname = f'{base}.sent.csv'\n",
        "    new_fname = os.path.join(source, new_fname)\n",
        "    with open(new_fname, 'w') as f:\n",
        "        # Header of the file\n",
        "        if include_header:\n",
        "            if include_source:\n",
        "                f.write('sentence,source\\n')\n",
        "            else:\n",
        "                f.write('sentence\\n')\n",
        "        # Writes one line for each sentence\n",
        "        for sentence in corpus:\n",
        "            if include_source:\n",
        "                f.write(f'{quote_char}{sentence}{quote_char}{sep_char}{fname}\\n')\n",
        "            else:\n",
        "                f.write(f'{quote_char}{sentence}{quote_char}\\n')\n",
        "    new_fnames.append(new_fname)\n",
        "\n",
        "  # Returns list of the newly generated CSV files\n",
        "  return sorted(new_fnames)"
      ],
      "metadata": {
        "id": "2BxVWK0Rk21Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_fnames = sentence_tokenize(HOME_DIR)\n",
        "new_fnames"
      ],
      "metadata": {
        "id": "95M-z4GlFTgy",
        "outputId": "cb669b4f-e6cc-44ce-e020-3e35e4eea729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/alice28-1476.sent.csv', 'data/wizoz10-1740.sent.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipeline"
      ],
      "metadata": {
        "id": "CuG-4mETlXam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s load the GPT-2-based text generation pipeline:\n",
        "text_generator = pipeline(\"text-generation\")"
      ],
      "metadata": {
        "id": "hqNvMBivlZtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let’s use the first two paragraphs from Alice’s Adventures in Wonderland as\n",
        "our base text:"
      ],
      "metadata": {
        "id": "ZSY-9mbEFCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_text = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on\n",
        "the bank, and of having nothing to do: once or twice she had peeped\n",
        "into the book her sister was reading, but it had no pictures or\n",
        "conversations in it, `and what is the use of a book,'thought Alice\n",
        "`without pictures or conversation?' So she was considering in her\n",
        "own mind (as well as she could, for the hot day made her feel very\n",
        "sleepy and stupid), whether the pleasure of making a daisy-chain\n",
        "would be worth the trouble of getting up and picking the daisies,\n",
        "when suddenly a White Rabbit with pink eyes ran close by her.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c0eeepwOltO4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator.model.config.task_specific_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edPRSZWGGOSg",
        "outputId": "f07a6319-c0c9-4bcc-a763-244e861292c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text-generation': {'do_sample': True, 'max_length': 50}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = text_generator(base_text, max_length=250)\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBcoBj87GyEv",
        "outputId": "bc4edb9f-d93c-4986-809c-c8f865ddee5c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on\n",
            "the bank, and of having nothing to do: once or twice she had peeped\n",
            "into the book her sister was reading, but it had no pictures or\n",
            "conversations in it, `and what is the use of a book,'thought Alice\n",
            "`without pictures or conversation?' So she was considering in her\n",
            "own mind (as well as she could, for the hot day made her feel very\n",
            "sleepy and stupid), whether the pleasure of making a daisy-chain\n",
            "would be worth the trouble of getting up and picking the daisies,\n",
            "when suddenly a White Rabbit with pink eyes ran close by her.\n",
            "The Rabbit, looking at her, asked the\n",
            "-- \"How did that White Rabbit get onto the daisies?\",' said Alice\n",
            "`and what is the use of a daisies if that white Rabbit isn't familiar with this\n",
            "--\n",
            "And what is the use of being so far down a little bit as a white dog with a\n",
            "white nose, when a White Rabbit goes so far up a daisie?\n",
            "Then the White Rabbit's eyes looked at her, and said, `You\n",
            "se\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the way, if you try using greedy decoding instead (setting `do_sample=False`), the generated text simply and annoyingly repeats the same text over and over again:"
      ],
      "metadata": {
        "id": "uWmEBo8WIe0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = text_generator(base_text, max_length=250, do_sample=False)\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmyJVrqgIhms",
        "outputId": "8154be72-6757-4335-828a-195535bc867f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on\n",
            "the bank, and of having nothing to do: once or twice she had peeped\n",
            "into the book her sister was reading, but it had no pictures or\n",
            "conversations in it, `and what is the use of a book,'thought Alice\n",
            "`without pictures or conversation?' So she was considering in her\n",
            "own mind (as well as she could, for the hot day made her feel very\n",
            "sleepy and stupid), whether the pleasure of making a daisy-chain\n",
            "would be worth the trouble of getting up and picking the daisies,\n",
            "when suddenly a White Rabbit with pink eyes ran close by her.\n",
            "'Oh, my dear, I am so glad to see you!' said Alice, 'I am so glad to see you!'\n",
            "'Oh, my dear, I am so glad to see you!' said Alice, 'I am so glad to see you!'\n",
            "'Oh, my dear, I am so glad to see you!' said Alice, 'I am so glad to see you!'\n",
            "'Oh, my dear, I am so glad to see you!' said Alice, 'I am so glad to see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait a minute! Aren’t we fine-tuning GPT-2 so it can write text in a\n",
        "given style?\n",
        "\n"
      ],
      "metadata": {
        "id": "DomhpnhpI1SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation"
      ],
      "metadata": {
        "id": "7hq9xgRMF0kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to capture the style of Lewis Carroll’s Alice’s Adventures in Wonderland, we\n",
        "need to use a dataset containing sentences from that book alone."
      ],
      "metadata": {
        "id": "8NqQYjeAJHrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(path=\"csv\", data_files=[\"data/alice28-1476.sent.csv\"], quotechar=\"\\\\\", split=Split.TRAIN)\n",
        "\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "o8LUn6F-lzaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we tokenize the dataset using GPT-2's pre-trained tokenizer\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "u7ZUQnA27N4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(row):\n",
        "  return auto_tokenizer(row[\"sentence\"])"
      ],
      "metadata": {
        "id": "-rPzhk3N7aUx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=[\"source\", \"sentence\"],\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=[\"source\", \"sentence\"],\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "JUYe9pn5-P3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without padding, the sentences have varied lengths\n",
        "list(map(len, tokenized_train_dataset[0:6][\"input_ids\"]))"
      ],
      "metadata": {
        "id": "HfQCHUgd-tUL",
        "outputId": "3ac90fde-7f37-42b3-fe30-29b7454a4f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 28, 20, 9, 34, 29]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Packing Dataset"
      ],
      "metadata": {
        "id": "6FYku3hb8wen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"packing\" is actually simpler now; it is simply concatenating the inputs together\n",
        "and then chunking them into blocks."
      ],
      "metadata": {
        "id": "CBq0ti3C_9sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples, block_size=128):\n",
        "  # Concatenate all texts\n",
        "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "  # We drop the small remainder, we could add padding\n",
        "  # if the model supported it instead of this drop, you can customize this part to your needs.\n",
        "  total_length = (total_length // block_size) * block_size\n",
        "  # Split by chunks of max_len\n",
        "  result = {\n",
        "      k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
        "      for k, t in concatenated_examples.items()\n",
        "  }\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result"
      ],
      "metadata": {
        "id": "7mi_HSfZ8ypg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can apply the function above to our datasets\n",
        "lm_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "lm_test_dataset = tokenized_test_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "lm_train_dataset.set_format(type=\"torch\")\n",
        "lm_test_dataset.set_format(type=\"torch\")"
      ],
      "metadata": {
        "id": "ghsp6KfgBo4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, the first data point actually contains the first 128 tokens of our dataset\n",
        "print(lm_train_dataset[0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "xnmUrbZmCJ4u",
        "outputId": "27c56f73-a420-4b2b-aa0d-96c4071f759b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   63,  2437,   466,   345,   760,   314,  1101,  8805,  8348,   464,\n",
            "         2677,  3114,  7296,  6819,   379,   262,  2635, 25498,    11,   508,\n",
            "          531,   287,   257,  1877,  3809,    11,  4600,  7120, 25788,  1276,\n",
            "         3272,    12,  1069,  9862, 12680,  4973,  2637,  1537,   611,   314,\n",
            "         1101,   407,   262,   976,    11,   262,  1306,  1808,   318,    11,\n",
            "         5338,   287,   262,   995,   716,   314,    30,   464,   360,   579,\n",
            "         1076,  6364,  4721,   465,  2951,    13,    63,  1026,   373,   881,\n",
            "        21289,   272,   353,   379,  1363,  4032,  1807,  3595, 14862,    11,\n",
            "         4600, 12518,   530,  2492,   470,  1464,  3957,  4025,   290,  4833,\n",
            "           11,   290,   852,  6149,   546,   416, 10693,   290, 33043,    13,\n",
            "         1870, 14862,   373,   523,   881, 24776,   326,   673,  4966,   572,\n",
            "          379,  1752,   287,   262,  4571,   340,  6235,   284,    11,  1231,\n",
            "         2111,   284,  4727,   262,  7457,   340,   550,   925])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consequently, the datasets get smaller, since they do not contain sentences\n",
        "anymore but sequences of 128 tokens instead:"
      ],
      "metadata": {
        "id": "2Upc3ILoCWBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(lm_train_dataset), len(lm_test_dataset)"
      ],
      "metadata": {
        "id": "qJVJ7RtFCWgE",
        "outputId": "4ae71d04-ad17-4ea7-b5e0-480c6326a4c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(239, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training"
      ],
      "metadata": {
        "id": "8l15Yf4Hoiie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 is a model for causal language modeling, and that’s the AutoModel we use to load it."
      ],
      "metadata": {
        "id": "l7k7vEWtClbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "print(gpt_model.__class__)"
      ],
      "metadata": {
        "id": "2qp4xqFSCp3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's override the default trainer arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=8,  # means mini-batch has size eight\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=300,\n",
        "    logging_steps=300,\n",
        "    gradient_accumulation_steps=8,\n",
        "    prediction_loss_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "aMZ7DOdFqNDm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since GPT-2 is a generative model, we won’t be running any additional metrics during training or validation, and there’s no need for anything but the loss."
      ],
      "metadata": {
        "id": "uHDlmrI2DaeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s redefine the trainer\n",
        "trainer = Trainer(\n",
        "    model=gpt_model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_train_dataset,\n",
        "    eval_dataset=lm_test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "e1o6j9bGsLAo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There we go—we’re 100% ready to call the glorious train() method\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ECOS5HPZsFai",
        "outputId": "4c3cd524-8ac7-4566-a673-27657525366b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 11:16, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=29, training_loss=3.6358197968581627, metrics={'train_runtime': 700.8899, 'train_samples_per_second': 0.341, 'train_steps_per_second': 0.041, 'total_flos': 15154937856000.0, 'train_loss': 3.6358197968581627, 'epoch': 0.97})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the final validation\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "fsxza4UGsiVW",
        "outputId": "ff2c8810-5794-4a61-af52-e31d2b1d084f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:59]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.419328451156616,\n",
              " 'eval_runtime': 69.9444,\n",
              " 'eval_samples_per_second': 0.801,\n",
              " 'eval_steps_per_second': 0.1,\n",
              " 'epoch': 0.97}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating Text"
      ],
      "metadata": {
        "id": "6smXdj6WF22k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assign our fine-tuned model and pretrained\n",
        "tokenizer to a pipeline and using most of its default values."
      ],
      "metadata": {
        "id": "cn8a5Du6GCvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_index = (gpt_model.device.index if gpt_model.device.type != \"cpu\" else -1)\n",
        "\n",
        "gpt_gen = pipeline(\"text-generation\", model=gpt_model, tokenizer=auto_tokenizer, device=device_index)"
      ],
      "metadata": {
        "id": "WH4Hg3oNamN9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The only parameter we may have to change is, once again, the max_length\n",
        "result = gpt_gen(base_text, max_length=250)\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "XUHZG6MIb22_",
        "outputId": "061f340e-807e-44f5-cd1e-e66b620b023e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on\n",
            "the bank, and of having nothing to do: once or twice she had peeped\n",
            "into the book her sister was reading, but it had no pictures or\n",
            "conversations in it, `and what is the use of a book,'thought Alice\n",
            "`without pictures or conversation?' So she was considering in her\n",
            "own mind (as well as she could, for the hot day made her feel very\n",
            "sleepy and stupid), whether the pleasure of making a daisy-chain\n",
            "would be worth the trouble of getting up and picking the daisies,\n",
            "when suddenly a White Rabbit with pink eyes ran close by her.\n",
            "`You!'thought Alice, `and I can't look at this!'\n",
            "Alice said nothing with any trepidation, and as she went back to drawing up her\n",
            "`What's more?'Alice had seen the Queen, when, as she was talking, she got a\n",
            "little confused about the meaning of this sentence.`Well,'thought Alice, and, without waiting for any more words,\n",
            "`just as soon as I was finished with this talk--`I suppose I must go to the\n",
            "'I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = gpt_gen(base_text, max_length=250)\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "LE7RlNKwcLq9",
        "outputId": "06ea899e-850e-4322-8d4c-32f5caf7e160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on\n",
            "the bank, and of having nothing to do: once or twice she had peeped\n",
            "into the book her sister was reading, but it had no pictures or\n",
            "conversations in it, `and what is the use of a book,'thought Alice\n",
            "`without pictures or conversation?' So she was considering in her\n",
            "own mind (as well as she could, for the hot day made her feel very\n",
            "sleepy and stupid), whether the pleasure of making a daisy-chain\n",
            "would be worth the trouble of getting up and picking the daisies,\n",
            "when suddenly a White Rabbit with pink eyes ran close by her.\n",
            "`Did you get a White Rabbit!'said Alice: `I've only seen one before!'\n",
            "So Alice began by making small changes with the stick, and gave her way up the steps towards the garden:\n",
            "\n",
            "\n",
            "Alice was only a little surprised to be at the first steps, and in her own way noticed that the Rabbit-in-the-Band had started all the way out,\n",
            "`I had just seen what the King and Queen said when she went upstairs the other day,'said Alice. `I wish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried it out\n",
        "several times and, in my humble opinion, the output looks more \"Alice-y\" now.\n",
        "\n",
        "What do you think?"
      ],
      "metadata": {
        "id": "Bj5mhJLeHbBW"
      }
    }
  ]
}