{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFN/3W/cbl08cEFYnjNgnf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/01_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequences"
      ],
      "metadata": {
        "id": "qKDOcAGwkGhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we’ll dive into a new kind of input: sequences!\n",
        "\n",
        "In sequence problems, an ordered sequence of data points shares a single\n",
        "label—emphasis on being ordered.\n",
        "\n",
        "Why is ordered so important?\n",
        "\n",
        "If the data points aren’t ordered, even if they share a single label, they are not a\n",
        "sequence, but rather a collection of data points.\n",
        "\n",
        "If the data structure has a single dimension, though, that’s a sequence. This\n",
        "particular structure can be exploited by recurrent neural networks and their many\n",
        "variants, as well as by 1D convolutional neural networks.\n",
        "\n",
        "There are two main types of sequence problems: time series and natural language\n",
        "processing (NLP). \n",
        "\n",
        "We’ll start by generating a synthetic dataset and then use it to\n",
        "illustrate the inner workings of:\n",
        "\n",
        "1. recurrent neural networks, \n",
        "2. encoder-decoder models, \n",
        "3. attention mechanisms, and \n",
        "4. Transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "KIj__-HokOxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "3WRqxBEvlvgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter8()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter8 import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlXOl_Holz6M",
        "outputId": "ea7793e9-e12c-45c9-a685-24f66f13e3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from torch.nn.utils import rnn as rnn_utils\n",
        "\n",
        "from data_generation.square_sequences import generate_sequences\n",
        "from stepbystep.v4 import StepByStep"
      ],
      "metadata": {
        "id": "54a8hhq8l2AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "HrCBq56i3sVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ball Dataset"
      ],
      "metadata": {
        "id": "Z9Sqyzlxl-Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use a dataset of 1,000 random points drawn from a ten-dimensional ball such that each feature has zero mean and unit standard\n",
        "deviation. \n",
        "\n",
        "In this dataset, points situated within half of the radius of the ball are\n",
        "labeled as negative cases, while the remaining points are labeled positive cases."
      ],
      "metadata": {
        "id": "NMt6wXGPl_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_data(n_points=1000, n_dims=10)"
      ],
      "metadata": {
        "id": "-04BquM1nbe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ball_dataset = TensorDataset(torch.as_tensor(X).float(), torch.as_tensor(y).float())\n",
        "ball_loader = DataLoader(ball_dataset, batch_size=len(X))"
      ],
      "metadata": {
        "id": "uLYCNOICACwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Block Model"
      ],
      "metadata": {
        "id": "MOWbL1u6BwJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate the vanishing gradients problem, we need a deeper model.\n",
        "\n",
        "Let’s call it the \"block\" model: It is a block of several hidden\n",
        "layers (and activation functions) stacked together, every layer containing the same\n",
        "number of hidden units (neurons)."
      ],
      "metadata": {
        "id": "1pWMxzI7Bw4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "\n",
        "n_layers = 5\n",
        "n_features = X.shape[1]\n",
        "hidden_units = 100\n",
        "activation_fn = nn.ReLU\n",
        "\n",
        "model = build_model(n_features, n_layers, hidden_units, activation_fn, use_bn=False)"
      ],
      "metadata": {
        "id": "QW0r77mzsKad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "fgMpzO1_Cdx1",
        "outputId": "eca560d2-38af-44cb-bdf1-bcd70a0b91ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (h1): Linear(in_features=10, out_features=100, bias=True)\n",
            "  (a1): ReLU()\n",
            "  (h2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a2): ReLU()\n",
            "  (h3): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a3): ReLU()\n",
            "  (h4): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a4): ReLU()\n",
            "  (h5): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a5): ReLU()\n",
            "  (o): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We’re only missing a loss function and an optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "v8JKnoukC_Eo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}