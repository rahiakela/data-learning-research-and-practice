{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBLIa35oxBgMSsso75Z5rM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/05_building_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building a Dataset"
      ],
      "metadata": {
        "id": "I_fJ4_FUlY0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll start our NLP journey by following the steps of Alice and Dorothy, from\n",
        "[Alice’s Adventures in Wonderland](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/1476) by Lewis Carroll and [The Wonderful Wizard of Oz](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/1740) by L. Frank Baum.\n",
        "\n",
        "![](images/alice_dorothy.png?raw=1)\n",
        "\n",
        "*Left: \"Alice and the Baby Pig\" illustration by John Tenniel's, from \"Alice's Adventure's in Wonderland\" (1865).*\n",
        "\n",
        "*Right: \"Dorothy meets the Cowardly Lion\" illustration by W.W. Denslow, from \"The Wonderful Wizard of Oz\" (1900)*"
      ],
      "metadata": {
        "id": "6S3AN5ZklZkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "y_fsZxO6mSi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# # UPDATED\n",
        "# ###########################################################\n",
        "!pip install gensim==4.3.1\n",
        "# # The library has been archived and won't be used anymore\n",
        "# # # !pip install allennlp==0.9.0\n",
        "#!pip install flair==0.12.2\n",
        "#!pip install torchvision==0.15.1\n",
        "# # # HuggingFace\n",
        "!pip install transformers==4.32.0\n",
        "!pip install datasets==2.14.4\n",
        "# ###########################################################"
      ],
      "metadata": {
        "id": "bPp9LPHbWch9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter11()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter11 import *"
      ],
      "metadata": {
        "id": "4KaJRDNLmTq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be10ba9-f360-442b-c126-faf0e23b7daf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import errno\n",
        "import requests\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "from data_generation.nlp import ALICE_URL, WIZARD_URL, download_text\n",
        "from stepbystep.v4 import StepByStep\n",
        "# These are the classes we built in Chapter 10\n",
        "from seq2seq import *\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "i9pPl_87mbqd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora, downloader\n",
        "from gensim.parsing.preprocessing import *\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "dEQBv2ddXGS_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Split\n",
        "from transformers import (\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BertModel, BertTokenizer, BertForSequenceClassification,\n",
        "    DistilBertModel, DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModel, AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, pipeline, TextClassificationPipeline\n",
        ")\n",
        "from transformers.pipelines import SUPPORTED_TASKS"
      ],
      "metadata": {
        "id": "w1HKtpREXdlF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading Books"
      ],
      "metadata": {
        "id": "VGxz6kEmX3ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's download data\n",
        "HOME_DIR = \"data\"\n",
        "download_text(ALICE_URL, HOME_DIR)\n",
        "download_text(WIZARD_URL, HOME_DIR)"
      ],
      "metadata": {
        "id": "eTXbiieBX4Sz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the downloaded data\n",
        "!cat data/alice28-1476.txt"
      ],
      "metadata": {
        "id": "45chINE_YYtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/wizoz10-1740.txt"
      ],
      "metadata": {
        "id": "uM-tdJNRZcnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to remove these additions to the original texts:"
      ],
      "metadata": {
        "id": "msne6vRcZ14Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alice_file = os.path.join(HOME_DIR, \"alice28-1476.txt\")\n",
        "with open(alice_file, \"r\") as f:\n",
        "  # The actual texts of the books are contained between lines 105 and 3703\n",
        "  alice_text = \"\".join(f.readlines()[104:3704])\n",
        "\n",
        "wizoz_file = os.path.join(HOME_DIR, \"wizoz10-1740.txt\")\n",
        "with open(wizoz_file, \"r\") as f:\n",
        "  # The actual texts of the books are contained between lines 309 and 5099\n",
        "  wizoz_text = \"\".join(f.readlines()[310:5100])"
      ],
      "metadata": {
        "id": "8z29BFQwZx4U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alice_text[:500])\n",
        "print(\"\\n\", \"#\"*70, \"\\n\")\n",
        "print(wizoz_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNZmtkHNahrp",
        "outputId": "4814a716-153d-4578-e020-d1e54f28cd07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ALICE'S ADVENTURES IN WONDERLAND\n",
            "\n",
            "                          Lewis Carroll\n",
            "\n",
            "               THE MILLENNIUM FULCRUM EDITION 2.8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            CHAPTER I\n",
            "\n",
            "                      Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "  Alice was beginning to get very tired of sitting by her sister\n",
            "on the bank, and of having nothing to do:  once or twice she had\n",
            "peeped into the book her sister was reading, but it had no\n",
            "pictures or conversations in it, `and what is the use of a book,'\n",
            "thought Alice `w\n",
            "\n",
            " ###################################################################### \n",
            "\n",
            "                    THE WONDERFUL WIZARD OF OZ\n",
            "\n",
            "\n",
            "                          1.  The Cyclone\n",
            "\n",
            "\n",
            "    Dorothy lived in the midst of the great Kansas prairies, with\n",
            "Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's\n",
            "wife.  Their house was small, for the lumber to build it had to be\n",
            "carried by wagon many miles.  There were four walls, a floor and a\n",
            "roof, which made one room; and this room contained a rusty looking\n",
            "cookstove, a cupboard for the dishes, a table, three or four\n",
            "chairs, and th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can partially automate the removal of the extra lines by setting the real start and end lines of each text in a configuration file."
      ],
      "metadata": {
        "id": "zS5E4dECbotZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_cfg = \"\"\"fname,start,end\n",
        "alice28-1476.txt,104,3704\n",
        "wizoz10-1740.txt,310,5100\"\"\"\n",
        "bytes_written = open(os.path.join(HOME_DIR, 'lines.cfg'), 'w').write(text_cfg)"
      ],
      "metadata": {
        "id": "ayc3piYsbutL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence Tokenization"
      ],
      "metadata": {
        "id": "Bk4m73rMb0gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6w5mJGOFb1Ua"
      }
    }
  ]
}