{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGiDl5hRKpFk8WbroTjyUr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/04_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer"
      ],
      "metadata": {
        "id": "I_fJ4_FUlY0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re actually quite close to developing our own version of the famous\n",
        "Transformer model. The encoder-decoder architecture with positional encoding\n",
        "is missing only a few details to effectively \"transform and roll out\" :-)\n",
        "\n",
        "First, we need to revisit the multi-headed attention mechanism to make it less\n",
        "computationally expensive by using narrow attention. Then, we’ll learn about a\n",
        "new kind of normalization: layer normalization.\n",
        "\n",
        "Finally, we’ll add some more bells\n",
        "and whistles: dropout, residual connections, and more \"layers\"."
      ],
      "metadata": {
        "id": "6S3AN5ZklZkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "y_fsZxO6mSi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter10()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter8 import *\n",
        "from plots.chapter9 import *\n",
        "from plots.chapter10 import *"
      ],
      "metadata": {
        "id": "4KaJRDNLmTq_",
        "outputId": "3876bebf-1810-464e-dba1-76a6f39c41a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from torchvision.transforms import Compose, Normalize, Pad\n",
        "\n",
        "from data_generation.square_sequences import generate_sequences\n",
        "from data_generation.image_classification import generate_dataset\n",
        "from helpers import index_splitter, make_balanced_sampler\n",
        "from stepbystep.v4 import StepByStep\n",
        "# These are the classes we built in Chapter 9\n",
        "from seq2seq import PositionalEncoding, subsequent_mask, EncoderDecoderSelfAttn"
      ],
      "metadata": {
        "id": "i9pPl_87mbqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "tWAO0Hb3mjb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Narrow Attention"
      ],
      "metadata": {
        "id": "7FS3XWpJmuaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used full attention heads to build a multi-headed attention\n",
        "and we called it wide attention. Although this mechanism works well, it gets\n",
        "prohibitively expensive as the number of dimensions grows.\n",
        "\n",
        "That’s when the narrow attention comes in: Each attention head will get a chunk of the\n",
        "transformed data points (projections) to work with."
      ],
      "metadata": {
        "id": "5WshScxG9vTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chunking"
      ],
      "metadata": {
        "id": "TVbjXRVh9tsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention heads do not\n",
        "use chunks of the original data points, but rather those of their\n",
        "projections.\n",
        "\n",
        "Why?\n",
        "\n",
        "To understand why, let’s take an example of an affine transformation, one that\n",
        "generates \"values\" ($v_0$) from the first data point ($x_0$).\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/attn_narrow_transf.png?raw=1)\n",
        "\n",
        "The transformation above takes a single data point of four dimensions (features) and turns it into a \"value\" (also with four dimensions) that’s going to be used in the attention mechanism.\n",
        "\n",
        "At first sight, it may look like we’ll get the same result whether we split the inputs\n",
        "into chunks or we split the projections into chunks. But that’s definitely not the case.\n",
        "So, let’s zoom in and look at the individual weights inside that transformation.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/multihead_chunking.png?raw=1)\n",
        "\n",
        "On the left, the correct approach: It computes the projections first and chunks them later. It is clear that each value in the projection (from $v_{00}$ to $v_{03}$) is a linear combination of all features in the data point.\n",
        "\n",
        "Since each head is working with a subset of the projected\n",
        "dimensions, these projected dimensions may end up\n",
        "representing different aspects of the underlying data. For\n",
        "natural language processing tasks, for example, some attention\n",
        "heads may correspond to linguistic notions of syntax and\n",
        "coherence. A particular head may attend to the direct objects of\n",
        "verbs, while another head may attend to objects of prepositions,\n",
        "and so on.\n",
        "\n",
        "Now, compare it to the wrong approach, on the right: By chunking it first, each value in the projection is a linear combination of a subset of the features only.\n",
        "\n",
        "Why is it so bad?\n",
        "\n",
        "First, it is a simpler model (the wrong approach has only eight weights while the correct one has sixteen), so its learning capacity is limited. Second, since each head can only look at a subset of the features, they simply cannot learn about longrange dependencies in the inputs.\n",
        "\n",
        "Now, let’s use a source sequence of length two as input, with each data point\n",
        "having four features like the chunking example above, to illustrate our new self-attention mechanism.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/narrow-attention1.png?raw=1)\n",
        "\n",
        "The flow of information goes like this:\n",
        "\n",
        "* Both data points (x0 and x1) go through distinct affine transformations to generate the corresponding \"values\" (v0 and v1) and \"keys\" (k0 and k1), which we’ll be calling projections.\n",
        "\n",
        "* Both data points also go through another affine transformation to generate the corresponding \"queries\" (q0 and q1).\n",
        "\n",
        "* Each projection has the same number of dimensions as the inputs (four).\n",
        "\n",
        "* Instead of simply using the projections, as former attention heads did, this attention head uses only a chunk of the projections to compute the context vector.\n",
        "\n",
        "* Since projections have four dimensions, let’s split them into two chunks—blue (left) and green (right)—of two dimensions each.\n",
        "\n",
        "* The first attention head uses only blue chunks to compute its context vector, which, like the projections, has only two dimensions.\n",
        "\n",
        "* The second attention head (not depicted in the figure above) uses the green chunks to compute the other half of the context vector, which, in the end, has the desired dimension.\n",
        "\n",
        "* Like the former multi-headed attention mechanism, the context vector goes through a feed-forward network to generate the \"hidden states\" (only the first one is depicted in the figure above).\n",
        "\n"
      ],
      "metadata": {
        "id": "FT5PYVOlmvkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multi-Headed Attention"
      ],
      "metadata": {
        "id": "dmsw_E3b9g0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new multi-headed attention class is more than a combination of both the\n",
        "Attention and MultiHeadedAttention classes: It implements the chunking of the projections and introduces dropout for attention scores."
      ],
      "metadata": {
        "id": "QV5-eLRZ9hto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, d_model, dropout=0.1):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.d_k = int(d_model / n_heads)\n",
        "\n",
        "    # Affine transformations for Q, K, and V\n",
        "    self.QUERY = nn.Linear(d_model, d_model)\n",
        "    self.KEY = nn.Linear(d_model, d_model)\n",
        "    self.VALUE = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.alphas = None\n",
        "\n",
        "  def make_chunks(self, x):\n",
        "    batch_size, seq_len = x.size(0), x.size(1)\n",
        "    # N, L, D -> N, L, n_heads * d_k\n",
        "    x = x.view(batch_size, seq_len, self.n_heads, self.d_k) # splits its last dimension in two\n",
        "    # N, n_heads, L, d_k\n",
        "    x = x.transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "  def init_keys(self, key):\n",
        "    # N, n_heads, L, d_k\n",
        "    # Chunking the key, and value projections\n",
        "    self.projection_key = self.make_chunks(self.KEY(key))\n",
        "    self.projection_value = self.make_chunks(self.VALUE(key))\n",
        "\n",
        "  def score_function(self, query):\n",
        "    # scaled dot product\n",
        "    # N, n_heads, L, d_k x # N, n_heads, d_k, L -> N, n_heads, L, L\n",
        "    # Chunking the query projections\n",
        "    projection_query = self.make_chunks(self.QUERY(query))\n",
        "    dot_products = torch.matmul(projection_query, self.projection_key.transpose(-2, -1))\n",
        "    scores = dot_products / np.sqrt(self.d_k)\n",
        "    return scores\n",
        "\n",
        "  def attention(self, query, mask=None):\n",
        "    # Query is batch-first: N, L, D\n",
        "    # Score function will generate scores for each head\n",
        "    scores = self.score_function(query)  # N, n_heads, L, L\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask==0, -1e9)\n",
        "    alphas = F.softmax(scores, dim=-1)  # N, n_heads, L, L\n",
        "    alphas = self.dropout(alphas)\n",
        "    self.alphas = alphas.detach()\n",
        "\n",
        "    # N, n_heads, L, L x N, n_heads, L, d_k -> N, n_heads, L, d_k\n",
        "    context = torch.matmul(alphas, self.projection_value)\n",
        "    return context\n",
        "\n",
        "  def output_function(self, contexts):\n",
        "    # N, L, D\n",
        "    output = self.linear_layer(contexts)  # N, L, D\n",
        "    return output\n",
        "\n",
        "  def forward(self, query, mask=None):\n",
        "    if mask is not None:\n",
        "      # N, 1, L, L - every head uses the same mask\n",
        "      mask = mask.unsqueeze(1)\n",
        "\n",
        "    # N, n_heads, L, d_k\n",
        "    contexts = self.attention(query, mask=mask)\n",
        "    # Concatenating the context vectors\n",
        "    # N, L, n_heads, d_k\n",
        "    contexts = contexts.transpose(1, 2).contiguous()\n",
        "    # N, L, n_heads * d_k = N, L, d_model\n",
        "    contexts = contexts.view(query.size(0), -1, self.d_model)\n",
        "    # N, L, d_model\n",
        "    output = self.output_function(contexts)\n",
        "    return output"
      ],
      "metadata": {
        "id": "cDf9HTfwCO31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can generate some dummy points corresponding to a mini-batch of 16\n",
        "sequences (N), each sequence having two data points (L), each data point having\n",
        "four features (F):"
      ],
      "metadata": {
        "id": "7UP_3J_sTdHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_points = torch.randn(16, 2, 4) # N, L, F\n",
        "multi_head_attention = MultiHeadAttention(n_heads=2, d_model=4, dropout=0.0)\n",
        "multi_head_attention.init_keys(dummy_points)\n",
        "output = multi_head_attention(dummy_points) # N, L, D\n",
        "output.shape"
      ],
      "metadata": {
        "id": "FoNt8RdgTdrK",
        "outputId": "45fb71a6-09d1-4e4e-8b61-e19f2339a85c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we’re using the data points as \"keys,\" \"values,\" and \"queries,\" this is a selfattention mechanism.\n",
        "\n",
        "The figure below depicts a multi-headed attention mechanism with its two heads,\n",
        "blue (left) and green (right), and the first data point being used as a \"query\" to\n",
        "generate the first \"hidden state\" ($h_0$).\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/narrow-attention2.png?raw=1)\n",
        "\n",
        "\n",
        "The important thing to remember here is: \"**Multi-headed attention chunks the projections, not the inputs.**\""
      ],
      "metadata": {
        "id": "ANPx_oybUjZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stacking Encoders and Decoders"
      ],
      "metadata": {
        "id": "bOL56HO-Vd93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s make our encoder-decoder architecture deeper by stacking two encoders on\n",
        "top of one another, and then do the same with two decoders.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/stacking-encoders-decoders1.png?raw=1)\n",
        "\n",
        "The output of one encoder feeds the cross-attention mechanism of all stacked\n",
        "decoders. The output of one decoder feeds the next, and the last decoder outputs predictions as usual.\n",
        "\n",
        "The figure above represents an encoder-decoder architecture with two \"layers\"\n",
        "each. But we’re not stopping there: We’re stacking six \"layers\"!\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/stacking-encoders-decoders2.png?raw=1)\n",
        "\n",
        "By the way, that’s exactly how a Transformer is built!\n",
        "\n",
        "Cool! Is this a Transformer already then?\n",
        "\n",
        "Not yet, no. We need to work further on the \"sub-layers\" to transform the architecture above into a real Transformer."
      ],
      "metadata": {
        "id": "Bd-otWCtVe8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Wrapping \"Sub-Layers\""
      ],
      "metadata": {
        "id": "h8SJKxGds_G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As our model grows deeper, with many stacked \"layers,\" we’re going to run into\n",
        "familiar issues, like the vanishing gradients problem. In computer vision models, this issue was successfully addressed by the addition of other components, like batch normalization and residual connections.\n",
        "\n",
        "But we also know that dropout works pretty well as a regularizer, so we can throw\n",
        "that in the mix as well.\n",
        "\n",
        "We’ll wrap each and every \"sub-layer\" with them! Cool, right? But that brings up another question: How to wrap them?\n",
        "\n",
        "It turns out, we can wrap a \"sub-layer\" in one of two ways: norm-last or norm-first.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/stacking-encoders-decoders3.png?raw=1)\n",
        "\n",
        "Let’s turn the diagrams above into equations:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\begin{aligned}\n",
        "&\\text{outputs}_{\\text{norm-last}}=&\\text{norm(inputs + dropout(sublayer(inputs))}\n",
        "\\\\\n",
        "&\\text{outputs}_{\\text{norm-first}}=&\\text{inputs + dropout(sublayer(norm(inputs)))}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The equations are almost the same, except for the fact that the norm-last wrapper normalizes the outputs and the norm-first wrapper normalizes the inputs. That’s a small, yet important, difference.\n",
        "\n",
        "Why?\n",
        "\n",
        "If you’re using positional encoding, you want to normalize your inputs, so norm-first is more convenient.\n",
        "\n",
        "We’ll normalize the final outputs; that is, the output of the last \"layer\".\n",
        "\n",
        "From now on, we’re sticking with norm-first, thus normalizing the inputs:\n",
        "\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\begin{aligned}\n",
        "&\\text{outputs}_{\\text{norm-first}}=&\\text{inputs + dropout(sublayer(norm(inputs)))}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "By wrapping each and every \"sub-layer\" inside both encoder \"layers\" and decoder \"layers,\" we’ll arrive at the desired Transformer architecture.\n"
      ],
      "metadata": {
        "id": "YdgnQaE8tDH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer Encoder"
      ],
      "metadata": {
        "id": "eQNpkkRK2iy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/stacking-encoders-decoders4.png?raw=1)\n",
        "\n",
        "On the left, the encoder uses a norm-last wrapper, and its output (the encoder’s states) is given by:\n",
        "\n",
        "$$\n",
        "\\large\n",
        "\\begin{aligned}\n",
        "&\\text{outputs}_{\\text{norm-last}}=&\\text{norm}(\\underbrace{\\text{norm(inputs + att(inputs))}}_{\\text{Output of SubLayer}_0} + \\text{ffn}(\\underbrace{\\text{norm(inputs + att(inputs))}}_{\\text{Output of SubLayer}_0}))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "On the right, the encoder uses a norm-first wrapper, and its output (the encoder’s states) is given by:\n",
        "\n",
        "$$\n",
        "\\large\n",
        "\\begin{aligned}\n",
        "&\\text{outputs}_{\\text{norm-first}}=&\\underbrace{\\text{inputs + att(norm(inputs))}}_{\\text{Output of SubLayer}_0}+\\text{ffn(norm(}\\underbrace{\\text{inputs + att(norm(inputs))}}_{\\text{Output of SubLayer}_0}))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The norm-first wrapper allows the inputs to flow unimpeded (the inputs aren’t\n",
        "normalized) all the way to the top while adding the results of each \"sub-layer\" along\n",
        "the way.\n",
        "\n",
        "Which one is best?\n",
        "\n",
        "There is no straight answer to this question. It actually  placing the batch normalization layer before or after the\n",
        "activation function.\n",
        "\n",
        "Let’s see it in code, starting with the \"layer,\" and all its wrapped \"sub-layers\":"
      ],
      "metadata": {
        "id": "2GnLoFGS2jvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.ff_units = ff_units\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.self_attention_heads = MultiHeadAttention(n_heads, d_model, dropout)\n",
        "    self.feed_forward_network = nn.Sequential(\n",
        "        nn.Linear(d_model, ff_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(ff_units, d_model)\n",
        "    )\n",
        "\n",
        "    # define layer normalization\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, query, mask=None):\n",
        "    # Sublayer #0\n",
        "    # Norm\n",
        "    norm_query = self.norm1(query)\n",
        "    # Multi-headed Attention\n",
        "    self.self_attention_heads.init_keys(norm_query)\n",
        "    states = self.self_attention_heads(norm_query, mask)\n",
        "    # Add\n",
        "    attention = query + self.dropout1(states)\n",
        "\n",
        "    # Sublayer #1\n",
        "    # Norm\n",
        "    norm_attention = self.norm2(attention)\n",
        "    # Feed Forward\n",
        "    output = self.feed_forward_network(norm_attention)\n",
        "    # Add\n",
        "    output = attention + self.dropout2(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Nh9IK0mk31zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can stack a bunch of \"layers\" like that to build an actual encoder.Its constructor takes an instance of an EncoderLayer, the number\n",
        "of \"layers\" we’d like to stack on top of one another, and a max length of the source\n",
        "sequence that’s going to be used for the positional encoding.\n",
        "\n",
        "The final outputs are, as usual, the states of the\n",
        "encoder that will feed the cross-attention mechanism of every \"layer\" of the\n",
        "decoder."
      ],
      "metadata": {
        "id": "QG_ZmLiDwQWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder_layer, n_layers=1, max_len=100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = encoder_layer.d_model\n",
        "    self.positional_encoding = PositionalEncoding(max_len, self.d_model)\n",
        "    self.norm_layer = nn.LayerNorm(self.d_model)\n",
        "    self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, query, mask=None):\n",
        "    # Positional Encoding\n",
        "    x = self.positional_encoding(query)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    # Norm\n",
        "    return self.norm_layer(x)"
      ],
      "metadata": {
        "id": "BoyHSBzVwts9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer Decoder"
      ],
      "metadata": {
        "id": "Hcd3ptphydX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CKf_i7WRyfCn"
      }
    }
  ]
}