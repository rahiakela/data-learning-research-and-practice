{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOpxc3FDzaxXi7wQlrjp26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/04_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer"
      ],
      "metadata": {
        "id": "I_fJ4_FUlY0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re actually quite close to developing our own version of the famous\n",
        "Transformer model. The encoder-decoder architecture with positional encoding\n",
        "is missing only a few details to effectively \"transform and roll out\" :-)\n",
        "\n",
        "First, we need to revisit the multi-headed attention mechanism to make it less\n",
        "computationally expensive by using narrow attention. Then, we’ll learn about a\n",
        "new kind of normalization: layer normalization.\n",
        "\n",
        "Finally, we’ll add some more bells\n",
        "and whistles: dropout, residual connections, and more \"layers\"."
      ],
      "metadata": {
        "id": "6S3AN5ZklZkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "y_fsZxO6mSi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter10()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter8 import *\n",
        "from plots.chapter9 import *\n",
        "from plots.chapter10 import *"
      ],
      "metadata": {
        "id": "4KaJRDNLmTq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from torchvision.transforms import Compose, Normalize, Pad\n",
        "\n",
        "from data_generation.square_sequences import generate_sequences\n",
        "from data_generation.image_classification import generate_dataset\n",
        "from helpers import index_splitter, make_balanced_sampler\n",
        "from stepbystep.v4 import StepByStep\n",
        "# These are the classes we built in Chapter 9\n",
        "from seq2seq import PositionalEncoding, subsequent_mask, EncoderDecoderSelfAttn"
      ],
      "metadata": {
        "id": "i9pPl_87mbqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "tWAO0Hb3mjb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Narrow Attention"
      ],
      "metadata": {
        "id": "7FS3XWpJmuaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used full attention heads to build a multi-headed attention\n",
        "and we called it wide attention. Although this mechanism works well, it gets\n",
        "prohibitively expensive as the number of dimensions grows.\n",
        "\n",
        "That’s when the narrow attention comes in: Each attention head will get a chunk of the\n",
        "transformed data points (projections) to work with."
      ],
      "metadata": {
        "id": "5WshScxG9vTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chunking"
      ],
      "metadata": {
        "id": "TVbjXRVh9tsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention heads do not\n",
        "use chunks of the original data points, but rather those of their\n",
        "projections.\n",
        "\n",
        "Why?\n",
        "\n",
        "To understand why, let’s take an example of an affine transformation, one that\n",
        "generates \"values\" ($v_0$) from the first data point ($x_0$).\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/attn_narrow_transf.png?raw=1)\n",
        "\n",
        "The transformation above takes a single data point of four dimensions (features) and turns it into a \"value\" (also with four dimensions) that’s going to be used in the attention mechanism.\n",
        "\n",
        "At first sight, it may look like we’ll get the same result whether we split the inputs\n",
        "into chunks or we split the projections into chunks. But that’s definitely not the case.\n",
        "So, let’s zoom in and look at the individual weights inside that transformation.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/multihead_chunking.png?raw=1)\n",
        "\n",
        "On the left, the correct approach: It computes the projections first and chunks them later. It is clear that each value in the projection (from $v_{00}$ to $v_{03}$) is a linear combination of all features in the data point.\n",
        "\n",
        "Since each head is working with a subset of the projected\n",
        "dimensions, these projected dimensions may end up\n",
        "representing different aspects of the underlying data. For\n",
        "natural language processing tasks, for example, some attention\n",
        "heads may correspond to linguistic notions of syntax and\n",
        "coherence. A particular head may attend to the direct objects of\n",
        "verbs, while another head may attend to objects of prepositions,\n",
        "and so on.\n",
        "\n",
        "Now, compare it to the wrong approach, on the right: By chunking it first, each value in the projection is a linear combination of a subset of the features only.\n",
        "\n",
        "Why is it so bad?\n",
        "\n",
        "First, it is a simpler model (the wrong approach has only eight weights while the correct one has sixteen), so its learning capacity is limited. Second, since each head can only look at a subset of the features, they simply cannot learn about longrange dependencies in the inputs.\n",
        "\n",
        "Now, let’s use a source sequence of length two as input, with each data point\n",
        "having four features like the chunking example above, to illustrate our new selfattention mechanism.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-III-NLP/images/attn_narrow_first_head.png?raw=1)\n",
        "\n",
        "The flow of information goes like this:\n",
        "\n",
        "* Both data points (x0 and x1) go through distinct affine transformations to\n",
        "generate the corresponding \"values\" (v0 and v1) and \"keys\" (k0 and k1), which\n",
        "we’ll be calling projections.\n",
        "\n",
        "* Both data points also go through another affine transformation to generate\n",
        "the corresponding \"queries\" (q0 and q1).\n",
        "\n",
        "* Each projection has the same number of dimensions as the inputs (four).\n",
        "\n",
        "* Instead of simply using the projections, as former attention heads did, this\n",
        "attention head uses only a chunk of the projections to compute the context\n",
        "vector.\n",
        "\n",
        "* Since projections have four dimensions, let’s split them into two chunks—blue (left) and green (right)—of two dimensions each.\n",
        "\n",
        "* The first attention head uses only blue chunks to compute its context vector, which, like the projections, has only two dimensions.\n",
        "\n",
        "* The second attention head (not depicted in the figure above) uses the green chunks to compute the other half of the context vector, which, in the end, has\n",
        "the desired dimension.\n",
        "\n",
        "* Like the former multi-headed attention mechanism, the context vector goes\n",
        "through a feed-forward network to generate the \"hidden states\" (only the first\n",
        "one is depicted in the figure above).\n",
        "\n"
      ],
      "metadata": {
        "id": "FT5PYVOlmvkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multi-Headed Attention"
      ],
      "metadata": {
        "id": "dmsw_E3b9g0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QV5-eLRZ9hto"
      }
    }
  ]
}