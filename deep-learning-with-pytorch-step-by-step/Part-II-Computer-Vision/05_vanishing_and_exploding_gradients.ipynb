{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhpQPzj0ZGOicroVfeYDxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/05_vanishing_and_exploding_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vanishing and Exploding Gradients"
      ],
      "metadata": {
        "id": "qKDOcAGwkGhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradients, together\n",
        "with the learning rate, are what makes the model tick, or better yet, learn.\n",
        "We always\n",
        "assumed that the gradients were well behaved, as long as our learning rate was\n",
        "sensible. \n",
        "\n",
        "Unfortunately, this is not necessarily true, and sometimes the gradients\n",
        "may go awry: They can either vanish or explode. Either way, we need to rein them\n",
        "in, so let’s see how we can accomplish that.\n",
        "\n",
        "Backpropagation works fine for\n",
        "models with a few hidden layers, but as models grow deeper, the gradients\n",
        "computed for the weights in the initial layers become smaller and smaller. That’s\n",
        "the so-called vanishing gradients problem, and it has always been a major obstacle\n",
        "for training deeper models.\n",
        "\n",
        "If gradients vanish—that is, if they are close to zero—updating the weights will\n",
        "barely change them. In other words, the model is not learning anything; it gets\n",
        "stuck.\n",
        "\n",
        "Why does it happen?\n",
        "\n"
      ],
      "metadata": {
        "id": "KIj__-HokOxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "3WRqxBEvlvgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapterextra()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapterextra import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlXOl_Holz6M",
        "outputId": "a9d388e6-21cf-4683-b980-731d292a8a9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from stepbystep.v3 import StepByStep\n",
        "\n",
        "from data_generation.ball import load_data"
      ],
      "metadata": {
        "id": "54a8hhq8l2AK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "HrCBq56i3sVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ball Dataset"
      ],
      "metadata": {
        "id": "Z9Sqyzlxl-Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use a dataset of 1,000 random points drawn from a ten-dimensional ball such that each feature has zero mean and unit standard\n",
        "deviation. \n",
        "\n",
        "In this dataset, points situated within half of the radius of the ball are\n",
        "labeled as negative cases, while the remaining points are labeled positive cases."
      ],
      "metadata": {
        "id": "NMt6wXGPl_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_data(n_points=1000, n_dims=10)"
      ],
      "metadata": {
        "id": "-04BquM1nbe4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ball_dataset = TensorDataset(torch.as_tensor(X).float(), torch.as_tensor(y).float())\n",
        "ball_loader = DataLoader(ball_dataset, batch_size=len(X))"
      ],
      "metadata": {
        "id": "uLYCNOICACwv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Block Model"
      ],
      "metadata": {
        "id": "MOWbL1u6BwJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate the vanishing gradients problem, we need a deeper model.\n",
        "\n",
        "Let’s call it the \"block\" model: It is a block of several hidden\n",
        "layers (and activation functions) stacked together, every layer containing the same\n",
        "number of hidden units (neurons)."
      ],
      "metadata": {
        "id": "1pWMxzI7Bw4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "\n",
        "n_layers = 5\n",
        "n_features = X.shape[1]\n",
        "hidden_units = 100\n",
        "activation_fn = nn.ReLU\n",
        "\n",
        "model = build_model(n_features, n_layers, hidden_units, activation_fn, use_bn=False)"
      ],
      "metadata": {
        "id": "QW0r77mzsKad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "fgMpzO1_Cdx1",
        "outputId": "f296f2a4-81d8-44de-8b48-94e6fb16a4b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (h1): Linear(in_features=10, out_features=100, bias=True)\n",
            "  (a1): ReLU()\n",
            "  (h2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a2): ReLU()\n",
            "  (h3): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a3): ReLU()\n",
            "  (h4): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a4): ReLU()\n",
            "  (h5): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (a5): ReLU()\n",
            "  (o): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We’re only missing a loss function and an optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "v8JKnoukC_Eo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Weights, Activations, and Gradients"
      ],
      "metadata": {
        "id": "ia5v6-ebtp6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n_yi7LIptqsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model = resnet18(weights=True)\n",
        "model.fc = nn.Linear(512, 3)"
      ],
      "metadata": {
        "id": "8a1kaUIB1fvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no freezing since fine-tuning entails the training of all the weights, not only\n",
        "those from the \"top\" layer."
      ],
      "metadata": {
        "id": "RucReUnt15ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer_model = optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "c7Z1rAuIqHdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have everything set to train."
      ],
      "metadata": {
        "id": "fCpfs2pa04UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_transfer = StepByStep(model, multi_loss_fn, optimizer_model)\n",
        "sbs_transfer.set_loaders(train_loader, val_loader)\n",
        "sbs_transfer.train(1)"
      ],
      "metadata": {
        "id": "b600rp9I04uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see what the model can accomplish after training for a single epoch."
      ],
      "metadata": {
        "id": "0bu0-2yjwiTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_transfer.correct)"
      ],
      "metadata": {
        "id": "BpfWatQwjN0W",
        "outputId": "6c3111cf-b3ad-443d-bfb7-43b55ca986a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[124, 124],\n",
              "        [124, 124],\n",
              "        [124, 124]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we had frozen the layers in the model above, it would have been a case of\n",
        "feature extraction suitable for data augmentation since we would be training the\n",
        "\"top\" layer while it was still attached to the rest of the model."
      ],
      "metadata": {
        "id": "WK6C5w333Okb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extraction"
      ],
      "metadata": {
        "id": "puyt8uzt3VkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we’re modifying the model (replacing the \"top\" layer\n",
        "with an identity layer) to generate a dataset of features first and then using it to\n",
        "train the real \"top\" layer independently."
      ],
      "metadata": {
        "id": "43sdVWSe3WMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Configuration\n",
        "model = resnet18(weights=True).to(device)\n",
        "model.fc = nn.Identity()\n",
        "freeze_model(model)"
      ],
      "metadata": {
        "id": "cJn6EdV54KgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation — Preprocessing\n",
        "train_preproc = preprocessed_dataset(model, train_loader)\n",
        "val_preproc = preprocessed_dataset(model, val_loader)\n",
        "train_preproc_loader = DataLoader(train_preproc, batch_size=16, shuffle=True)\n",
        "val_preproc_loader = DataLoader(val_preproc, batch_size=16)"
      ],
      "metadata": {
        "id": "zCpc8FU25DU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the dataset of features and its corresponding loaders are ready, we only need\n",
        "to create a model corresponding to the \"top\" layer and train it in the usual way."
      ],
      "metadata": {
        "id": "KR4yWH8_6RsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Configuration — Top Model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "top_model = nn.Sequential(nn.Linear(512, 3))\n",
        "\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer_top = optim.Adam(top_model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "7QguWiPx6Slu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training — Top Model\n",
        "sbs_top = StepByStep(top_model, multi_loss_fn, optimizer_top)\n",
        "sbs_top.set_loaders(train_preproc_loader, val_preproc_loader)\n",
        "sbs_top.train(10)"
      ],
      "metadata": {
        "id": "xvxcH05T7GOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We surely can evaluate the model now\n",
        "StepByStep.loader_apply(val_preproc_loader, sbs_top.correct)"
      ],
      "metadata": {
        "id": "fSvoNmsb7c4N",
        "outputId": "4500ac05-0fc0-4696-a08c-315d5e0399cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 98, 124],\n",
              "        [124, 124],\n",
              "        [104, 124]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, if we want to try it out on the original dataset (containing the images), we need to reattach the \"top\" layer."
      ],
      "metadata": {
        "id": "lPsqYes87-P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = top_model\n",
        "sbs_temp = StepByStep(model, None, None)"
      ],
      "metadata": {
        "id": "Ci2MWGM-8AY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, both loss function and\n",
        "optimizers are set to None since we won’t be training the model anymore."
      ],
      "metadata": {
        "id": "W_-HHZNH8Yyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_temp.correct)"
      ],
      "metadata": {
        "id": "cAQ4J_op8YNd",
        "outputId": "e82defd8-1e72-4069-e841-5c99662f9d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 98, 124],\n",
              "        [124, 124],\n",
              "        [104, 124]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the same results, as expected."
      ],
      "metadata": {
        "id": "Six2em2L8mRd"
      }
    }
  ]
}