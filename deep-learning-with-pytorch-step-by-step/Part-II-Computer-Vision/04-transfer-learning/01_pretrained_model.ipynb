{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOI8APNseSmmtx5DL7DhtS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/04-transfer-learning/01_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transfer Learning: Pretrained model"
      ],
      "metadata": {
        "id": "qKDOcAGwkGhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's consider what is Transfer learning?\n",
        "\n",
        "The idea is quite simple. First, some big tech company, which has access to virtually\n",
        "infinite amounts of data and computing power, develops and trains a huge model\n",
        "for their own purpose. \n",
        "\n",
        "Next, once it is trained, its architecture and the corresponding trained weights (the pre-trained model) are released. Finally,\n",
        "everyone else can use these weights as a starting point and fine-tune them\n",
        "further for a different (but similar) purpose.\n",
        "\n",
        "That’s transfer learning in a nutshell."
      ],
      "metadata": {
        "id": "KIj__-HokOxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "3WRqxBEvlvgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sJOR-0hwlwr7",
        "outputId": "3b4888f7-0070-445f-fcb6-2d0b450033ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter7()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter7 import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlXOl_Holz6M",
        "outputId": "9ba65212-4218-4286-fd96-45977d0b095d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, ToPILImage, CenterCrop, RandomResizedCrop\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import alexnet, resnet18, inception_v3\n",
        "#from torchvision.models.alexnet import model_urls\n",
        "try:\n",
        "  from torchvision.models.utils import load_state_dict_from_url\n",
        "except ImportError:\n",
        "  from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from stepbystep.v3 import StepByStep\n",
        "from data_generation.rps import download_rps"
      ],
      "metadata": {
        "id": "54a8hhq8l2AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "0A-YvvDpDl-1",
        "outputId": "debf8027-abdc-451a-ac13-40761de3c8b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# content/gdrive/My Drive/Kaggle is the path where kaggle.json is  present in the Google Drive\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/kaggle-keys\""
      ],
      "metadata": {
        "id": "iDxyNz9iEDxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# download dataset from kaggle> URL: https://www.kaggle.com/datasets/sanikamal/rock-paper-scissors-dataset\n",
        "kaggle datasets download -d sanikamal/rock-paper-scissors-dataset\n",
        "\n",
        "unzip -qq rock-paper-scissors-dataset.zip\n",
        "rm -rf rock-paper-scissors-dataset.zip"
      ],
      "metadata": {
        "id": "imNthwCGdJTw",
        "outputId": "09b26262-23bf-4974-fb26-afab6b5d6495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading rock-paper-scissors-dataset.zip to /content\n",
            " 99% 449M/452M [00:04<00:00, 122MB/s]\n",
            "100% 452M/452M [00:04<00:00, 98.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Z9Sqyzlxl-Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data preparation step will be a bit more demanding this time since we’ll be\n",
        "standardizing the images.Besides, we can use the ImageFolder dataset now.\n",
        "\n",
        "The Rock Paper Scissors dataset is organized like that:\n",
        "\n",
        "```\n",
        "rps/paper/paper01-000.png\n",
        "rps/paper/paper01-001.png\n",
        "\n",
        "rps/rock/rock01-000.png\n",
        "rps/rock/rock01-001.png\n",
        "\n",
        "rps/scissors/scissors01-000.png\n",
        "rps/scissors/scissors01-001.png\n",
        "```\n",
        "\n",
        "The dataset is also perfectly balanced, with each sub-folder containing 840 images\n",
        "of its particular class."
      ],
      "metadata": {
        "id": "NMt6wXGPl_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_FOLDER = \"Rock-Paper-Scissors\""
      ],
      "metadata": {
        "id": "-04BquM1nbe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we’re using a pre-trained model, we need to use the standardization\n",
        "parameters used to train the original model. \n",
        "\n",
        "In other words, we need to use the\n",
        "statistics of the original dataset used to train that model.\n",
        "\n",
        "So, the data preparation step for the Rock Paper Scissors dataset looks like this now:"
      ],
      "metadata": {
        "id": "Zl4V53YwsCxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "composer = Compose([\n",
        "  Resize(256),\n",
        "  CenterCrop(224),\n",
        "  ToTensor(),\n",
        "  normalizer\n",
        "])\n",
        "\n",
        "train_data = ImageFolder(root=f\"{ROOT_FOLDER}/train\", transform=composer)\n",
        "val_data = ImageFolder(root=f\"{ROOT_FOLDER}/test\", transform=composer)\n",
        "\n",
        "# Builds a loader of each set\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16)"
      ],
      "metadata": {
        "id": "QW0r77mzsKad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-Trained Model"
      ],
      "metadata": {
        "id": "ia5v6-ebtp6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating an instance of AlexNet without loading its pre-trained\n",
        "weights."
      ],
      "metadata": {
        "id": "n_yi7LIptqsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alex = alexnet(weights=False)\n",
        "print(alex)"
      ],
      "metadata": {
        "id": "9VnF8htTmng4",
        "outputId": "a0bafc4a-5573-4836-d021-28c47a388621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adaptive Pooling"
      ],
      "metadata": {
        "id": "6IIXNpNGK_YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AdaptiveAvgPool2d` is a special kind of pooling: Instead of requiring the kernel size\n",
        "(and stride), it requires the desired output size. \n",
        "\n",
        "In other words, whatever the\n",
        "image size it gets as input, it will return a tensor with the desired size.\n",
        "\n",
        "It gives you the freedom to use images of different sizes as inputs.\n",
        "\n",
        "Let’s verify it."
      ],
      "metadata": {
        "id": "7ImU48BSJDI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = F.adaptive_avg_pool2d(torch.randn(16, 32, 32), output_size=(6, 6))\n",
        "result2 = F.adaptive_avg_pool2d(torch.randn(16, 12, 12), output_size=(6, 6))\n",
        "\n",
        "result1.shape, result2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0oY6PmYKd4C",
        "outputId": "a9693d92-a0fe-4b6e-c8aa-21cf43e08454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 6, 6]), torch.Size([16, 6, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading Weights"
      ],
      "metadata": {
        "id": "B3gaIQZRLET3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s download the weights\n",
        "from a given URL, which gives you the flexibility to use pre-trained weights from\n",
        "wherever you want!"
      ],
      "metadata": {
        "id": "On1OaMBqnl5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL = model_urls[\"alexnet\"]\n",
        "\n",
        "# ref: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/torchvision_models.py\n",
        "URL = \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\""
      ],
      "metadata": {
        "id": "sfhWjLZknU3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = load_state_dict_from_url(URL, model_dir=\"pretrained\", progress=True)"
      ],
      "metadata": {
        "id": "QtKjfFDNbVCn",
        "outputId": "f7e55c83-d75c-4ccc-b9e2-77bb4d0ea7d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to pretrained/alexnet-owt-4df8aa71.pth\n",
            "100%|██████████| 233M/233M [00:04<00:00, 52.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load model\n",
        "alex.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "ySu-bovibn5W",
        "outputId": "74448c0d-9abf-47ce-aab4-1194af1f9765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Freezing"
      ],
      "metadata": {
        "id": "Tb-ieAysrUL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing the model means it won’t learn anymore; that is, its\n",
        "parameters / weights will not be updated anymore.\n",
        "\n",
        "What best characterizes a tensor representing a learnable parameter? It requires\n",
        "gradients. \n",
        "\n",
        "So, if we’d like to make them stop learning anything, we need to change\n",
        "exactly that:"
      ],
      "metadata": {
        "id": "VpiORRVBrVN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_model(model):\n",
        "  for parameter in model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "freeze_model(alex)"
      ],
      "metadata": {
        "id": "QTP6DzzJtqUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model is frozen, how I am supposed to train it for my own\n",
        "purpose?\n",
        "\n",
        "We have to unfreeze a small part of the model or, better yet,\n",
        "replace a small part of the model."
      ],
      "metadata": {
        "id": "4TqUUInJ418a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Top of the Model"
      ],
      "metadata": {
        "id": "WV8WiWi0dEHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"top\" of the model is loosely defined as the last layer(s) of the model, usually\n",
        "belonging to its classifier part. \n",
        "\n",
        "The featurizer part is usually left untouched since\n",
        "we’re trying to leverage the model’s ability to generate features for us."
      ],
      "metadata": {
        "id": "7v4iZbrThnPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(alex.features)"
      ],
      "metadata": {
        "id": "jY6vuwDHhwuC",
        "outputId": "8998a38c-e48f-4889-f834-e09d7c46e9eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU(inplace=True)\n",
            "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(alex.classifier)"
      ],
      "metadata": {
        "id": "L6hIxrJkiAFM",
        "outputId": "aa95fb8e-1aff-4fe5-8204-3e8c5350c763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our Rock Paper Scissors dataset, we have three classes. \n",
        "\n",
        "So, we need to replace the\n",
        "output layer accordingly:"
      ],
      "metadata": {
        "id": "hyImVSR_49W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alex.classifier[6] = nn.Linear(in_features=4096, out_features=3)"
      ],
      "metadata": {
        "id": "PtHW64B84934"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alex.classifier)"
      ],
      "metadata": {
        "id": "P2r_8HOdikUM",
        "outputId": "8766c356-2ccc-49b0-fdf3-e21704993259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the number of input features remains the same, since it still takes the\n",
        "output from the hidden layer that precedes it. \n",
        "\n",
        "The new output layer requires\n",
        "gradients by default, but we can double-check it:"
      ],
      "metadata": {
        "id": "ihUCAeLy5ETR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in alex.named_parameters():\n",
        "  if param.requires_grad == True:\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "fkA3ofAN5K8w",
        "outputId": "e36d8aa6-0e00-462e-c8fe-6a3a0491056a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier.6.weight\n",
            "classifier.6.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "PxkOMQPH7E7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The configuration part is short and straightforward: We use alex model, a loss\n",
        "function, and an optimizer."
      ],
      "metadata": {
        "id": "pZIN0BMD7Flv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(17)\n",
        "\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer_alex = optim.Adam(alex.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "c7Z1rAuIqHdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have everything set to train the \"top\" layer of our modified version of AlexNet."
      ],
      "metadata": {
        "id": "fCpfs2pa04UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_alex = StepByStep(alex, multi_loss_fn, optimizer_alex)\n",
        "sbs_alex.set_loaders(train_loader, val_loader)\n",
        "sbs_alex.train(1)"
      ],
      "metadata": {
        "id": "b600rp9I04uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see how effective transfer learning is by evaluating our model after\n",
        "having trained it over one epoch only."
      ],
      "metadata": {
        "id": "0bu0-2yjwiTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_alex.correct)"
      ],
      "metadata": {
        "id": "BpfWatQwjN0W",
        "outputId": "3a50f062-6cbb-43e3-b606-7f8c2f99e7cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[111, 124],\n",
              "        [124, 124],\n",
              "        [124, 124]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating features"
      ],
      "metadata": {
        "id": "PVFwJIKN0Ksx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, since the frozen layers are simply generating features that will be the input\n",
        "of the trainable layers, why not treat the frozen layers as such? \n",
        "\n",
        "We could do it in\n",
        "four easy steps:\n",
        "\n",
        "* Keep only the frozen layers in the model.\n",
        "* Run the whole dataset through it and collect its outputs as a dataset of\n",
        "features.\n",
        "* Train a separate model (that corresponds to the \"top\" of the original model)\n",
        "using the dataset of features.\n",
        "* Attach the trained model to the top of the frozen layers.\n",
        "\n",
        "This way, we’re effectively splitting the feature extraction and actual training\n",
        "phases, thus avoiding the overhead of generating features over and over again for\n",
        "every single forward pass.\n",
        "\n",
        "To keep only the frozen layers, we need to get rid of the \"top\" of the original model.\n",
        "\n",
        "\n",
        "But, since we also want to attach our new layer to the whole model after training,\n",
        "it is a better idea to simply replace the \"top\" layer with an identity layer instead of\n",
        "removing it entirely:"
      ],
      "metadata": {
        "id": "cbsv_jSM0PqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alex.classifier[6] = nn.Identity()\n",
        "print(alex.classifier)"
      ],
      "metadata": {
        "id": "x7HrA_N40yoE",
        "outputId": "6794bfe9-1ae6-45c1-ba13-bb9ee1fe8ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Identity()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, the last effective layer is still `classifier.5`, which will produce the\n",
        "features we’re interested in. We have a feature extractor in our hands now! \n",
        "\n",
        "Let’s use it to pre-process our dataset."
      ],
      "metadata": {
        "id": "kCtLx1gd1OgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessed_dataset(model, loader, device=None):\n",
        "  if device is None:\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "  features = None\n",
        "  labels = None\n",
        "\n",
        "  for i, (x, y) in enumerate(loader):\n",
        "    model.eval()\n",
        "    x = x.to(device)\n",
        "    output = model(x)\n",
        "    if i == 0:\n",
        "      features = output.detach().cpu()\n",
        "      labels = y.cpu()\n",
        "    else:\n",
        "      features = torch.cat([features, output.detach().cpu()])\n",
        "      labels = torch.cat([labels, y.cpu()])\n",
        "\n",
        "  dataset = TensorDataset(features, labels)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "veDf3VnS0vkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preproc = preprocessed_dataset(alex, train_loader)\n",
        "val_preproc = preprocessed_dataset(alex, val_loader)"
      ],
      "metadata": {
        "id": "nHWAiUdt18HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also save these tensors to disk:"
      ],
      "metadata": {
        "id": "iUCNuPcs2vnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(train_preproc.tensors, \"rps_preproc.pth\")\n",
        "torch.save(val_preproc.tensors, \"rps_val_preproc.pth\")"
      ],
      "metadata": {
        "id": "fuiEix6Q2wCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, they can be used to build datasets later:"
      ],
      "metadata": {
        "id": "HgZwNGXw8poX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = torch.load(\"rps_preproc.pth\")\n",
        "\n",
        "train_preproc = TensorDataset(x, y)\n",
        "val_preproc = TensorDataset(*torch.load(\"rps_val_preproc.pth\"))"
      ],
      "metadata": {
        "id": "y3vu4pMb8qDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step of data preparation, as usual, is the creation of the data loader:"
      ],
      "metadata": {
        "id": "d4VbCwxH9Aq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_preproc_loader = DataLoader(train_preproc, batch_size=16, shuffle=True)\n",
        "val_preproc_loader = DataLoader(val_preproc, batch_size=16)"
      ],
      "metadata": {
        "id": "r_NNPO6C9BEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has only one layer, which matches the one we used in the \"Top of the\n",
        "Model\" subsection. \n",
        "\n",
        "The rest of the model configuration part remains unchanged:"
      ],
      "metadata": {
        "id": "iAd7TUe79h07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(17)\n",
        "\n",
        "top_model = nn.Sequential(nn.Linear(4096, 3))\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer_top = optim.Adam(top_model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "ePTqALwF9izo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model above using the\n",
        "pre-processed dataset."
      ],
      "metadata": {
        "id": "xqh_zkb690Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_top = StepByStep(top_model, multi_loss_fn, optimizer_top)\n",
        "sbs_top.set_loaders(train_preproc_loader, val_preproc_loader)\n",
        "sbs_top.train(10)"
      ],
      "metadata": {
        "id": "p-zZuXwD96yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can attach the trained model to the top of the full (frozen) model:"
      ],
      "metadata": {
        "id": "aOvSQgpE-PP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_alex.model.classifier[6] = top_model\n",
        "print(sbs_alex.model.classifier)"
      ],
      "metadata": {
        "id": "lsaz3VCj-PuX",
        "outputId": "399d75f3-dc3d-4d11-e44d-a03c31349f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Sequential(\n",
            "    (0): Linear(in_features=4096, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see how it performs on the validation set.\n",
        "\n",
        "We’re using the full model again, so we should use the original\n",
        "dataset instead of the pre-processed one."
      ],
      "metadata": {
        "id": "uFmNYhf8-sTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_alex.correct)"
      ],
      "metadata": {
        "id": "O_a9Fd-c-voS",
        "outputId": "01bd2b79-1982-48cd-bb88-fab1eab0eba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[109, 124],\n",
              "        [124, 124],\n",
              "        [124, 124]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is almost the same result as before.\n",
        "\n",
        "So this show you how to use\n",
        "transfer learning and how you can pre-process your dataset to speed up model\n",
        "training."
      ],
      "metadata": {
        "id": "NkiW9iLj_EOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Auxiliary Classifiers"
      ],
      "metadata": {
        "id": "fSNp6NmN_Yd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first version of the Inception model introduced\n",
        "auxiliary classifiers; that is, side-heads attached to intermediate parts of the\n",
        "model that would also try to perform classification, independently from the typical\n",
        "main classifier at the very end of the network.\n",
        "\n",
        "The technique was originally developed to mitigate the vanishing gradients\n",
        "problem, but it was later found that the auxiliary\n",
        "classifiers were more likely to have a regularizer effect instead.\n",
        "\n",
        "First, we load the pre-trained model, freeze its layers, and replace the layers for\n",
        "both main and auxiliary classifiers:"
      ],
      "metadata": {
        "id": "0JZ1-WkE_a1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = inception_v3(pretrained=True)\n",
        "freeze_model(model)"
      ],
      "metadata": {
        "id": "6P3C40BgPQTA",
        "outputId": "4732f055-34c0-4395-b45b-e21e09c4fb3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:01<00:00, 89.4MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.AuxLogits)"
      ],
      "metadata": {
        "id": "xnD9aA0QP_HJ",
        "outputId": "cf8d3708-924b-468b-f0fe-cf8628842091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InceptionAux(\n",
            "  (conv0): BasicConv2d(\n",
            "    (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv1): BasicConv2d(\n",
            "    (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.fc)"
      ],
      "metadata": {
        "id": "ilNZZGPtQDs_",
        "outputId": "6a7f1dbc-77fb-4cdc-efda-d2124f617294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=2048, out_features=1000, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model.AuxLogits.fc = nn.Linear(768, 3)\n",
        "model.fc = nn.Linear(2048, 3)"
      ],
      "metadata": {
        "id": "rHK2PC4gQLAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, we cannot use the standard cross-entropy loss because the\n",
        "Inception model outputs two tensors, one for each classifier (although it is possible\n",
        "to force it to return only the main classifier by setting its aux_logits argument to\n",
        "False). \n",
        "\n",
        "But we can create a simple function that can handle multiple outputs,\n",
        "compute the corresponding losses, and return their total:"
      ],
      "metadata": {
        "id": "ZIQsXLIbQyRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_loss(outputs, labels):\n",
        "  try:\n",
        "    main, aux = outputs\n",
        "  except ValueError:\n",
        "    main = outputs\n",
        "    aux = None\n",
        "    loss_aux = 0\n",
        "\n",
        "  multi_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "  loss_main = multi_loss_fn(main, labels)\n",
        "  if aux is not None:\n",
        "    loss_aux = multi_loss_fn(aux, labels)\n",
        "  return loss_main + 0.4 * loss_aux"
      ],
      "metadata": {
        "id": "YtLGE-pfQ2Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_model = optim.Adam(model.parameters(), lr=3e-4)\n",
        "sbs_inception = StepByStep(model, inception_loss, optimizer_model)"
      ],
      "metadata": {
        "id": "UwuHYaR-S2wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Wait, aren’t we pre-processing the dataset this time?\n",
        "\n",
        "Unfortunately, no. The `preprocessed_dataset()` function cannot handle multiple\n",
        "outputs.\n",
        "\n",
        "The Inception model is also different from the others in its expected input size: 299\n",
        "instead of 224. \n",
        "\n",
        "So, we need to recreate the data loaders accordingly:"
      ],
      "metadata": {
        "id": "kmLRWbhuS-xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "composer = Compose([\n",
        "  Resize(299),\n",
        "  ToTensor(),\n",
        "  normalizer\n",
        "])\n",
        "\n",
        "train_data = ImageFolder(root=f\"{ROOT_FOLDER}/train\", transform=composer)\n",
        "val_data = ImageFolder(root=f\"{ROOT_FOLDER}/test\", transform=composer)\n",
        "\n",
        "# Builds a loader of each set\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16)"
      ],
      "metadata": {
        "id": "_HbVXI_MTPbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re ready, so let’s train our model for a single epoch and evaluate the result:"
      ],
      "metadata": {
        "id": "9qB8LXFiaW36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_inception.set_loaders(train_loader, val_loader)\n",
        "sbs_inception.train(1)"
      ],
      "metadata": {
        "id": "omtWs8aTaXRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_inception.correct)"
      ],
      "metadata": {
        "id": "We2ssIepaoNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1x1 Convolutions"
      ],
      "metadata": {
        "id": "4MoQlx78fMLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea of a kernel of size one-by-one is somewhat counterintuitive at first. For a\n",
        "single channel, this kernel is only scaling the values of its input and nothing else.\n",
        "\n",
        "But everything changes if you have multiple channels.\n",
        "\n",
        "A filter has as many channels as its input.\n",
        "This means that each channel will be scaled independently and the results will be\n",
        "added up, resulting in one channel as output(per filter).\n",
        "\n",
        ">A `1x1` convolution can be used to reduce the number of\n",
        "channels; that is, it may work as a dimension-reduction layer.\n",
        "\n",
        "A filter using a `1x1` convolution corresponds to a weighted\n",
        "average of the input channels.\n",
        "\n",
        "In other words, a `1x1` convolution is a linear combination of the\n",
        "input channels, computed pixel by pixel.\n",
        "\n",
        "Performing a `1x1` convolution is akin to\n",
        "applying a linear layer to each individual pixel over its channels.\n",
        "\n",
        ">This is the reason why a `1x1` convolution is said to be equivalent\n",
        "to a fully connected (linear) layer.\n",
        "\n",
        "Let's see that grayscale images can be computed using a linear combination of the red,\n",
        "green, and blue channels of colored images. \n",
        "\n",
        "So, we can convert an image to\n",
        "grayscale using a `1x1` convolution!"
      ],
      "metadata": {
        "id": "YjS9OYHffM6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scissors = Image.open(f\"{ROOT_FOLDER}/train/scissors/scissors01-001.png\")\n",
        "image = ToTensor()(scissors)[:3, :, :].view(1, 3, 300, 300)\n",
        "\n",
        "weights = torch.tensor([0.2126, 0.7152, 0.0722]).view(1, 3, 1, 1)\n",
        "convolved = F.conv2d(input=image, weight=weights)\n",
        "\n",
        "converted = ToPILImage()(convolved[0])\n",
        "grayscale = scissors.convert(\"L\")"
      ],
      "metadata": {
        "id": "B6orvAJ5C250"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = compare_grayscale(converted, grayscale)"
      ],
      "metadata": {
        "id": "pifu9Uf1D_zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can think of converting colored images to grayscale as reducing the\n",
        "dimensions of the image, since the size of the output is one-third of the input size\n",
        "(one instead of three channels). \n",
        "\n",
        "This translates into having three times fewer\n",
        "parameters in the layer that is receiving it as its own input, and it allows the\n",
        "networks to grow deeper (and wider)."
      ],
      "metadata": {
        "id": "iXEYPO2iETnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inception Modules"
      ],
      "metadata": {
        "id": "87OIy3w4EVLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inception module with dimension-reduction layers uses\n",
        "1x1 convolutions to:\n",
        "\n",
        "* reduce the number of input channels for both `3x3` and `5x5` convolution\n",
        "branches; and\n",
        "* reduce the number of output channels for the max pooling branch.\n",
        "\n",
        "The `3x3` and `5x5` convolution branches may still output many channels (one for each filter), but each filter is convolving a reduced number of input channels.\n",
        "\n",
        "Let’s see what the Inception module looks like in code."
      ],
      "metadata": {
        "id": "do8QECPKEXbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(Inception, self).__init__()\n",
        "\n",
        "    # in_channels@HxW -> 2@HxW\n",
        "    self.branch1x1_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
        "\n",
        "    # in_channels@HxW -> 2@HxW -> 3@HxW\n",
        "    self.branch3x3_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
        "    self.branch3x3_2 = nn.Conv2d(2, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    # in_channels@HxW -> 2@HxW -> 3@HxW\n",
        "    self.branch5x5_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
        "    self.branch5x5_2 = nn.Conv2d(2, 3, kernel_size=5, padding=2)\n",
        "\n",
        "    # in_channels@HxW -> in_channels@HxW -> 1@HxW\n",
        "    self.branch_pool_1 = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    self.branch_pool_2 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Produces 2 channels\n",
        "    branch1x1 = self.branch1x1_1(x)\n",
        "\n",
        "    # Produces 3 channels\n",
        "    branch3x3 = self.branch3x3_1(x)   # Dimension reduction with 1x1 convolution\n",
        "    branch3x3 = self.branch3x3_2(branch3x3)\n",
        "\n",
        "    # Produces 3 channels\n",
        "    branch5x5 = self.branch5x5_1(x)   # Dimension reduction with 1x1 convolution\n",
        "    branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "    # Produces 2 channels\n",
        "    branch_pool = self.branch_pool_1(x)\n",
        "    branch_pool = self.branch_pool_2(branch_pool)  # Dimension reduction with 1x1 convolution\n",
        "\n",
        "    # Concatenates all channels together (10)\n",
        "    outputs = torch.cat([branch1x1, branch3x3, branch5x5, branch_pool], 1)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "w_8QdfOFFF3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we run our example image (scissors, in the color version) through the\n",
        "Inception module?"
      ],
      "metadata": {
        "id": "zVwEuO21Lot7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inception = Inception(in_channels=3)\n",
        "output = inception(image)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "rMs9CkBELpX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go: The output has the expected ten channels."
      ],
      "metadata": {
        "id": "D05XiRf9Pi0h"
      }
    }
  ]
}