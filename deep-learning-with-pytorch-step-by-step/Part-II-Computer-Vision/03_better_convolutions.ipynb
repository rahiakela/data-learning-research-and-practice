{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOk6RSZQYeSvWBX+CNzJLn8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/03_better_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Better Convolutions"
      ],
      "metadata": {
        "id": "qKDOcAGwkGhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, let's use a different dataset: Rock Paper Scissors.\n",
        "\n",
        "The dataset contains 2,892 images of diverse hands in the typical rock, paper, and\n",
        "scissors poses against a white background. This is a synthetic dataset as well since\n",
        "the images were generated using CGI techniques. Each image is 300x300 pixels in\n",
        "size and has four channels (RGBA).\n",
        "\n",
        "Here are some examples of its images, one for each pose.\n",
        "\n",
        "![](images/download.png)"
      ],
      "metadata": {
        "id": "KIj__-HokOxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "3WRqxBEvlvgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sJOR-0hwlwr7",
        "outputId": "d1ed6f37-cd83-47ac-b03a-2302a53ab5b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter6()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter6 import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlXOl_Holz6M",
        "outputId": "594e8910-3491-4892-958b-8026cf259136"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, Resize\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR, CyclicLR, LambdaLR\n",
        "\n",
        "from stepbystep.v2 import StepByStep\n",
        "from data_generation.rps import download_rps"
      ],
      "metadata": {
        "id": "54a8hhq8l2AK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Z9Sqyzlxl-Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data preparation step will be a bit more demanding this time since we’ll be\n",
        "standardizing the images.Besides, we can use the ImageFolder dataset now.\n",
        "\n",
        "The Rock Paper Scissors dataset is organized like that:\n",
        "\n",
        "```\n",
        "rps/paper/paper01-000.png\n",
        "rps/paper/paper01-001.png\n",
        "\n",
        "rps/rock/rock01-000.png\n",
        "rps/rock/rock01-001.png\n",
        "\n",
        "rps/scissors/scissors01-000.png\n",
        "rps/scissors/scissors01-001.png\n",
        "```\n",
        "\n",
        "The dataset is also perfectly balanced, with each sub-folder containing 840 images\n",
        "of its particular class.\n",
        "\n",
        "Let’s create a dataset then:"
      ],
      "metadata": {
        "id": "NMt6wXGPl_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir rps\n",
        "!mkdir rps/paper\n",
        "!mkdir rps/rock\n",
        "!mkdir rps/scissors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoAY6HLooA8J",
        "outputId": "feb46c1b-3b55-45b4-cfe3-6ee94393ba13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘rps’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_transform = Compose([Resize(28), ToTensor()])\n",
        "temp_dataset = ImageFolder(root=\"rps\", transform=temp_transform)"
      ],
      "metadata": {
        "id": "-04BquM1nbe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}