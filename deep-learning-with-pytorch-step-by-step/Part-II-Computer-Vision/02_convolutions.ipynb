{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVfe0Ei1DTvDYw2H7jdH38",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/02_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutions"
      ],
      "metadata": {
        "id": "8A9MVvQ5_T5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convolution is \"a mathematical operation on two functions (`f` and `g`) that produces a third function (`f * g`) expressing how the shape of one is modified by the other.\"\n",
        "\n",
        "In image processing, a convolution matrix is also called a kernel or filter. \n",
        "\n",
        "Typical image processing operations—like blurring, sharpening, edge detection, and more, are\n",
        "accomplished by performing a convolution between a kernel and an image."
      ],
      "metadata": {
        "id": "7K_rxSuE_Uhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "BebAYVWo_VlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ],
      "metadata": {
        "id": "iiQ_qcSp_Uq6",
        "outputId": "72524f88-86be-4cdd-85f4-17be0a8b6ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter5()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter5 import *"
      ],
      "metadata": {
        "id": "6EwSrmrQ_cxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose, Normalize\n",
        "\n",
        "from data_generation.image_classification import generate_dataset\n",
        "from helpers import index_splitter, make_balanced_sampler\n",
        "from stepbystep.v1 import StepByStep"
      ],
      "metadata": {
        "id": "ZNIlTCW9_gBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter / Kernel"
      ],
      "metadata": {
        "id": "Rsnr8thUATxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, the filters are\n",
        "small square matrices. The convolution itself is performed by applying the filter on\n",
        "the image repeatedly. \n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv1.png?raw=1)\n",
        "\n",
        "That’s the region to which the filter is being applied and is called the\n",
        "receptive field, drawing an analogy to the way human vision works.\n",
        "\n",
        "Let’s try a concrete example to make it more clear."
      ],
      "metadata": {
        "id": "xHjtaGl4ETM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single = np.array([\n",
        "  [ # batch dim\n",
        "      [ # channel dim\n",
        "          [5, 0, 8, 7, 8, 1], # height and width dim\n",
        "          [1, 9, 5, 0, 7, 7],\n",
        "          [6, 0, 2, 4, 6, 6],\n",
        "          [9, 7, 6, 6, 8, 4],\n",
        "          [8, 3, 8, 5, 1, 3],\n",
        "          [7, 2, 7, 0, 1, 0]\n",
        "      ]\n",
        "  ]\n",
        "])\n",
        "single.shape"
      ],
      "metadata": {
        "id": "jPyifdt-AUSX",
        "outputId": "35c528e2-d6e8-4abf-bf04-6ddc2cc179a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 6, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "identity = np.array([\n",
        "    [\n",
        "        [\n",
        "            [0, 0, 0],\n",
        "            [0, 1, 0],\n",
        "            [0, 0, 0]\n",
        "        ]\n",
        "    ]\n",
        "])\n",
        "identity.shape"
      ],
      "metadata": {
        "id": "oiO3F9UGFO9G",
        "outputId": "cf817872-8620-44bd-c41a-3a00dfb07944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolving"
      ],
      "metadata": {
        "id": "KkW-hK_CFhg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convulution performs an element-wise multiplication between the\n",
        "two, region and filter, and adds everything up.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv2.png?raw=1)"
      ],
      "metadata": {
        "id": "EUxO8zMfFkXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region = single[:, :, 0:3, 0:3]\n",
        "filtered_region = region * identity\n",
        "total = filtered_region.sum()\n",
        "total"
      ],
      "metadata": {
        "id": "CVEwetTtF1H2",
        "outputId": "8872223d-b923-4b02-ba9f-fb022e42e343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing a convolution produces an image with a\n",
        "reduced size.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv3.png?raw=1)"
      ],
      "metadata": {
        "id": "X69C4K0gGbWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving Around"
      ],
      "metadata": {
        "id": "2CYbWbW-HxNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we move the region one step to the right; that is, we change the receptive\n",
        "field and apply the filter again.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/stride1.png?raw=1)\n",
        "\n",
        "In code, it means we’re changing the slice of the input image:"
      ],
      "metadata": {
        "id": "B-uOhu84IWU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_region = single[:, :, 0:3, (0+1):(3+1)]"
      ],
      "metadata": {
        "id": "UN7eF6XnGotK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the operation remains the same: First, an element-wise multiplication, and\n",
        "then adding up the elements of the resulting matrix.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv5.png?raw=1)"
      ],
      "metadata": {
        "id": "p801DPVnJLI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_filtered_region = new_region * identity\n",
        "new_total = new_filtered_region.sum()\n",
        "new_total"
      ],
      "metadata": {
        "id": "tu1Xmj6HJOyX",
        "outputId": "f899fe3c-f991-4067-e840-e129f5a642e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have a second pixel value to add to our resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv6.png?raw=1)\n",
        "\n",
        "We can keep moving the gray region to the right until we can’t move it anymore.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv7.png?raw=1)\n",
        "\n",
        "The fourth step to the right will actually place the region partially outside the\n",
        "input image."
      ],
      "metadata": {
        "id": "N_zfUbc4KUzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
      ],
      "metadata": {
        "id": "g8r2NNINKeUI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected region does not match the shape of the filter anymore. \n",
        "\n",
        "So, if we try to\n",
        "perform the element-wise multiplication, it fails:"
      ],
      "metadata": {
        "id": "XP8Uby8_K4nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  last_horizontal_region * identity\n",
        "except Exception as exp:\n",
        "  print(exp)"
      ],
      "metadata": {
        "id": "zcWMNoMCK5w_",
        "outputId": "5dc1d2b9-60c9-4b3e-fc05-04592202875f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Shape"
      ],
      "metadata": {
        "id": "jBkcwd9cLRHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we go back to the left side and move down one step. If we repeat the\n",
        "operation, covering all valid regions, we’ll end up with a resulting image that is\n",
        "smaller (on the right).\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv8.png?raw=1)\n",
        "\n",
        "How much smaller is it going to be?\n",
        "\n",
        "It depends on the size of the filter.\n",
        "\n",
        ">The larger the filter, the smaller the resulting image.\n",
        "\n",
        "Since applying a filter always produces a single value, the reduction is equal to the\n",
        "filter size minus one.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * (h_f, w_f) = (h_i - (h_f - 1), w_i - (w_f - 1))\n",
        "$$\n",
        "\n",
        "If we assume the filter is a square matrix of size f, we can simplify the expression\n",
        "above to:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
        "$$\n",
        "\n",
        "But I’d like to keep the image size, is it possible?\n",
        "\n",
        "Sure it is! Padding comes to our rescue in this case."
      ],
      "metadata": {
        "id": "lhKlTPhcLR-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolving in PyTorch"
      ],
      "metadata": {
        "id": "iZdPF3vJSJNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how a convolution works, let’s try it out using PyTorch."
      ],
      "metadata": {
        "id": "svvUMTDGSKHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our image and filter to tensors\n",
        "image = torch.as_tensor(single).float()\n",
        "kernel = torch.as_tensor(identity).float()"
      ],
      "metadata": {
        "id": "FpDNP9utXtc6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the activation functions, convolutions come in two\n",
        "flavors: functional and module. \n",
        "\n",
        "There is a fundamental difference between the\n",
        "two, though: The functional convolution takes the kernel / filter as an argument\n",
        "while the module has (learnable) weights to represent the kernel / filter.\n",
        "\n",
        "Let’s use the functional convolution, `F.conv2d()`, to apply the identity filter to our\n",
        "input image."
      ],
      "metadata": {
        "id": "7XxFSecQYQao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved = F.conv2d(image, kernel, stride=1)\n",
        "convolved"
      ],
      "metadata": {
        "id": "Fm6TCTCpYfRF",
        "outputId": "de084d49-ca53-4005-884e-7ac802860a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 5., 0., 7.],\n",
              "          [0., 2., 4., 6.],\n",
              "          [7., 6., 6., 8.],\n",
              "          [3., 8., 5., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s turn our attention to PyTorch’s convolution module, `nn.Conv2d`."
      ],
      "metadata": {
        "id": "2jPbCKifZuGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
        "conv(image)"
      ],
      "metadata": {
        "id": "UxDu6pa1ZwRu",
        "outputId": "bfdb96a6-3682-4dbd-c203-f367e8f8620c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.9316, -0.0864,  0.8621, -0.2419],\n",
              "          [ 0.5833,  0.5895, -2.6865, -0.9461],\n",
              "          [-2.1605, -1.3302, -0.5297, -0.9163],\n",
              "          [ 0.1566,  1.0850,  0.5501,  2.5671]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results are gibberish now because the convolutional module randomly initializes the weights representing\n",
        "the kernel / filter.\n",
        "\n",
        "That’s the whole point of the convolutional module: It will learn\n",
        "the kernel / filter on its own.\n",
        "\n",
        "In traditional computer vision, people would develop different\n",
        "filters for different purposes: blurring, sharpening, edge\n",
        "detection, and so on.\n",
        "\n",
        "Can we tell it to learn multiple filters at once?"
      ],
      "metadata": {
        "id": "0mRxitR_aIFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_multiple = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1)\n",
        "conv_multiple(image)"
      ],
      "metadata": {
        "id": "-tkVCkuuasJi",
        "outputId": "732c217e-c938-4603-eef5-01dccc645474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.8032,  2.2359,  5.1481,  3.4676],\n",
              "          [ 5.4154,  4.2610,  2.1250,  2.6113],\n",
              "          [ 1.5885,  1.0453,  1.4714,  1.5167],\n",
              "          [ 5.0344,  1.5347,  2.3275,  3.2528]],\n",
              "\n",
              "         [[ 1.7809, -4.4379, -2.1811,  3.2840],\n",
              "          [-1.0926,  2.8527,  3.5799,  1.4452],\n",
              "          [ 2.4228,  2.2969,  0.9183, -1.2018],\n",
              "          [-1.5256,  0.3706, -4.1544, -4.4124]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_multiple.weight"
      ],
      "metadata": {
        "id": "ToKEQ1v1a_fp",
        "outputId": "56bbf2f4-0233-4705-b33e-e5b3b25fa1de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[ 0.1394,  0.3148,  0.0152],\n",
              "          [ 0.0386, -0.2673,  0.0346],\n",
              "          [ 0.0805,  0.0766,  0.1790]]],\n",
              "\n",
              "\n",
              "        [[[-0.0703, -0.2870, -0.2112],\n",
              "          [-0.3325,  0.2426,  0.1940],\n",
              "          [ 0.1441,  0.2770,  0.2128]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also force a convolutional module to use a particular filter by setting its weights."
      ],
      "metadata": {
        "id": "T3e8p3TVdKkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  conv.weight[0] = kernel\n",
        "  conv.bias[0] = 0\n",
        "\n",
        "conv(image)"
      ],
      "metadata": {
        "id": "uzMNjIh6dMBq",
        "outputId": "7ab17018-d503-4a50-fbd2-aa7afdf33f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 5., 0., 7.],\n",
              "          [0., 2., 4., 6.],\n",
              "          [7., 6., 6., 8.],\n",
              "          [3., 8., 5., 1.]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.weight"
      ],
      "metadata": {
        "id": "b5TEqL4ud8zW",
        "outputId": "ed6d3e69-a11a-46f7-e3ed-e4b0fcb2ea6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[0., 0., 0.],\n",
              "          [0., 1., 0.],\n",
              "          [0., 0., 0.]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the weights to get specific filters is at the heart of\n",
        "transfer learning. \n",
        "\n",
        "Someone else trained a model, and that model\n",
        "learned lots of useful filters, so we don’t have to learn them\n",
        "again. We can set the corresponding weights and go from there."
      ],
      "metadata": {
        "id": "5gYV5KxYeIWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Striding"
      ],
      "metadata": {
        "id": "t9HI8NnZeKMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try a stride of two for a change and see what happens to the resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/strider2.png?raw=1)\n",
        "\n",
        "The resulting image, after the only four valid operations, looks like this.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/strider3.png?raw=1)\n",
        "\n",
        "Also, notice that using a larger stride made the shape of the resulting image even smaller.\n",
        "\n",
        ">The larger the stride, the smaller the resulting image.\n",
        "\n",
        "Once again, it makes sense: If we are skipping pixels in the input image, there are\n",
        "fewer regions of interest to apply the filter to. \n",
        "\n",
        "We can extend our previous formula\n",
        "to include the stride size (s):\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{h_i - f + 1}{s}, \\frac{w_i - f + 1}{s}\\right)\n",
        "$$\n",
        "\n",
        "Let’s use\n",
        "PyTorch’s functional convolution to double-check the results."
      ],
      "metadata": {
        "id": "fyfsZe1MeMGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved_stride2 = F.conv2d(image, kernel, stride=2)\n",
        "convolved_stride2"
      ],
      "metadata": {
        "id": "NSgTQekvyu-3",
        "outputId": "6e85f7c6-db54-4985-d10f-14697be146cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 0.],\n",
              "          [7., 6.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, it works!"
      ],
      "metadata": {
        "id": "3TTYNw37zEc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Padding"
      ],
      "metadata": {
        "id": "nVacXfTkzE46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, the operations we have performed have been shrinking the images. What\n",
        "about restoring them to their original glory, I mean, size?\n",
        "\n",
        "Padding means stuffing. We need to stuff the original image so it can sustain the \"attack\" on its size.\n",
        "\n",
        "How do I stuff an image?\n",
        "\n",
        "Simply add zeros around it.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding1.png?raw=1)\n",
        "\n",
        "By adding columns and rows of zeros around it, we expand the\n",
        "input image such that the gray region starts centered in the actual top left corner\n",
        "of the input image. \n",
        "\n",
        "This simple trick can be used to preserve the original size of the\n",
        "image.\n",
        "\n",
        "In code, as usual, PyTorch gives us two options: functional (`F.pad()`) and module (`nn.ConstantPad2d`).\n",
        "\n",
        "Let’s start with the module version this time:"
      ],
      "metadata": {
        "id": "gexEBVsuzH1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constant_padder = nn.ConstantPad2d(padding=1, value=0)\n",
        "constant_padder(image)"
      ],
      "metadata": {
        "id": "GIuPaIjY0dxq",
        "outputId": "eaa54290-edfb-4038-8f95-033a956367ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
              "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
              "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
              "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also do asymmetric padding by specifying a tuple in the padding\n",
        "argument representing (left, right, top, bottom). \n",
        "\n",
        "So, if we were to stuff our\n",
        "image on the left and right sides only, the argument would go like this: `(1, 1, 0, 0)`.\n",
        "\n",
        "We can achieve the same result using the functional padding:"
      ],
      "metadata": {
        "id": "8My1qAQY1ynT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"constant\", value=0)\n",
        "padded"
      ],
      "metadata": {
        "id": "wrfPCPMz1yCk",
        "outputId": "2340f0ec-49c7-41f8-fe51-6ef7225d9f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
              "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
              "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
              "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, there is another argument, mode, which was\n",
        "set to constant to match the module version above.\n",
        "\n",
        "What are the other available modes?\n",
        "\n",
        "There are three other modes: replicate, reflect, and circular.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/paddings.png?raw=1)\n",
        "\n",
        "In replication padding, the padded pixels have the same value as the closest real\n",
        "pixel.\n",
        "\n",
        "In PyTorch, one can use the functional form `F.pad()` with mode=\"replicate\", or use\n",
        "the module version `nn.ReplicationPad2d`:"
      ],
      "metadata": {
        "id": "vfbBSTl_2yYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replication_padder = nn.ReplicationPad2d(padding=1)\n",
        "replication_padder(image)"
      ],
      "metadata": {
        "id": "JtIfohjj4Bss",
        "outputId": "a6bbc1fd-3a15-4e47-c4a4-8756eb09555a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
              "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"replicate\")\n",
        "padded"
      ],
      "metadata": {
        "id": "bTbOtd1_4el9",
        "outputId": "5b8c8472-a15c-41dc-bc65-da6a55df9edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
              "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reflection padding, it gets a bit trickier. It is like the outer columns and rows are\n",
        "used as axes for the reflection.\n",
        "\n",
        "In PyTorch, you can use the functional form `F.pad()` with mode=\"reflect\", or use\n",
        "the module version `nn.ReflectionPad2d`:"
      ],
      "metadata": {
        "id": "A2l0de5N4ybA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reflection_padder = nn.ReflectionPad2d(padding=1)\n",
        "reflection_padder(image)"
      ],
      "metadata": {
        "id": "bE-tinAm44rs",
        "outputId": "6f0628ea-f9cf-4014-b7ae-e736dbaf58a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
              "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
              "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"reflect\")\n",
        "padded"
      ],
      "metadata": {
        "id": "-HedDa4b5Gor",
        "outputId": "7898d5d1-ec19-44b2-9759-b50db1a26ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
              "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
              "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In circular padding, the left-most (right-most) column gets copied as the right (left) padded column.\n",
        "\n",
        "Similarly, the top-most\n",
        "(bottom-most) row gets copied as the bottom (top) padded row. The corners\n",
        "receive the values of the diametrically opposed corner.\n",
        "\n",
        "In PyTorch, you must use the functional form `F.pad()` with mode=\"circular\" since\n",
        "there is no module version of the circular padding."
      ],
      "metadata": {
        "id": "qscKbJFg5atz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"circular\")\n",
        "padded"
      ],
      "metadata": {
        "id": "wIjXVU7i5smI",
        "outputId": "330a82b4-38ab-434a-895e-42a2ca14655e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
              "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
              "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
              "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By padding an image, it is possible to get resulting images with the same shape as\n",
        "input images, or even larger, should you choose to stuff more and more rows and\n",
        "columns into the input image. \n",
        "\n",
        "Assuming we’re doing symmetrical padding of size p,\n",
        "the resulting shape is given by the formula below:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{(h_i + 2p) - f + 1}{s}, \\frac{(w_i + 2p) - f + 1}{s}\\right)\n",
        "$$\n",
        "\n",
        "We’re basically extending the original dimensions by 2p pixels each."
      ],
      "metadata": {
        "id": "00snBJCt5-eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A REAL Filter"
      ],
      "metadata": {
        "id": "yb1FSMZ16JvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try an edge detector filter from traditional\n",
        "computer vision for a change:"
      ],
      "metadata": {
        "id": "TOS3C8WI6KN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_matrix = np.array([\n",
        "  [[\n",
        "    [0, 1, 0],\n",
        "    [1, -4, 1],\n",
        "    [0, 1, 0]\n",
        "  ]]\n",
        "])\n",
        "\n",
        "kernel_edge = torch.as_tensor(edge_matrix).float()\n",
        "kernel_edge.shape"
      ],
      "metadata": {
        "id": "1b3D6o4e-kMx",
        "outputId": "38ab18c7-936c-4085-d72f-61547905e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let’s apply it to a different region of our (padded) input image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding2.png?raw=1)\n",
        "\n",
        "As you can see, filters, other than the identity one, do not simply copy the value at\n",
        "the center. \n",
        "\n",
        "The element-wise multiplication finally means something.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding3.png?raw=1)\n",
        "\n",
        "Let’s apply this filter to our image."
      ],
      "metadata": {
        "id": "ym94Q2j-_SxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"constant\", value=0)\n",
        "conv_padded = F.conv2d(padded, kernel_edge, stride=1)\n",
        "conv_padded"
      ],
      "metadata": {
        "id": "1370UbvBAgTx",
        "outputId": "e4db7709-0c8e-43fe-8ea6-675e4e8f582a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-19.,  22., -20., -12., -17.,  11.],\n",
              "          [ 16., -30.,  -1.,  23.,  -7., -14.],\n",
              "          [-14.,  24.,   7.,  -2.,   1.,  -7.],\n",
              "          [-15., -10.,  -1.,  -1., -15.,   1.],\n",
              "          [-13.,  13., -11.,  -5.,  13.,  -7.],\n",
              "          [-18.,   9., -18.,  13.,  -3.,   4.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling"
      ],
      "metadata": {
        "id": "FrV8PymAA8kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling is different than the former operations: It splits the image into tiny chunks, performs an operation on each chunk (that yields a single value), and puts the chunks together as the resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/pooling1.png?raw=1)\n",
        "\n",
        "Our input image is split into nine chunks, and we perform a simple max operation\n",
        "(hence, max pooling) on each chunk (really, it is just taking the largest value in each\n",
        "chunk). Then, these values are put together, in order, to produce a smaller\n",
        "resulting image.\n",
        "\n",
        ">The larger the pooling kernel, the smaller the resulting image.\n",
        "\n",
        "In PyTorch, as usual, we have both forms: `F.max_pool2d()` and `nn.MaxPool2d`. \n",
        "\n",
        "Let’s\n",
        "use the functional form to replicate the max pooling."
      ],
      "metadata": {
        "id": "Yxve0AcE8dGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ],
      "metadata": {
        "id": "127inD4k97KY",
        "outputId": "4315be26-5bcd-4ee8-b330-70260a339dcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[22., 23., 11.],\n",
              "          [24.,  7.,  1.],\n",
              "          [13., 13., 13.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then let’s use the module version to illustrate the large four-by-four pooling."
      ],
      "metadata": {
        "id": "9svGlR23_VEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxpool4 = nn.MaxPool2d(kernel_size=4)\n",
        "pooled4 = maxpool4(conv_padded)\n",
        "pooled4"
      ],
      "metadata": {
        "id": "8tMu3Vk5_Vte",
        "outputId": "e0af3ae2-a931-4b12-ec41-42369a6116c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[24.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides max pooling, average pooling is also fairly common. As the name\n",
        "suggests, it will output the average pixel value for each chunk. \n",
        "\n",
        "In PyTorch, we have\n",
        "`F.avg_pool2d()` and `nn.AvgPool2d`."
      ],
      "metadata": {
        "id": "NgJ8QWy-_0A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.avg_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ],
      "metadata": {
        "id": "_Yrn8wBE_8zR",
        "outputId": "eb715907-94bc-4c7d-9396-8fd4a01daca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.7500, -2.5000, -6.7500],\n",
              "          [-3.7500,  0.7500, -5.0000],\n",
              "          [-2.2500, -5.2500,  1.7500]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avgpool4 = nn.AvgPool2d(kernel_size=4)\n",
        "pooled4 = avgpool4(conv_padded)\n",
        "pooled4"
      ],
      "metadata": {
        "id": "4_DdnfoDAMx_",
        "outputId": "c2a4d4ed-12ab-4383-a639-006585949034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.0625]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can I use a stride of a different size?"
      ],
      "metadata": {
        "id": "CNm1egCsAXcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2, stride=1)\n",
        "pooled"
      ],
      "metadata": {
        "id": "947Cu913AYAb",
        "outputId": "db345774-fc08-4642-bcb6-59bada571a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[22., 22., 23., 23., 11.],\n",
              "          [24., 24., 23., 23.,  1.],\n",
              "          [24., 24.,  7.,  1.,  1.],\n",
              "          [13., 13., -1., 13., 13.],\n",
              "          [13., 13., 13., 13., 13.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Flattening"
      ],
      "metadata": {
        "id": "SOvlpuBiAsC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It simply flattens a tensor, preserving the first\n",
        "dimension such that we keep the number of data points while collapsing all other\n",
        "dimensions."
      ],
      "metadata": {
        "id": "Abg65oZUAs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flattened = nn.Flatten()(pooled)\n",
        "flattened"
      ],
      "metadata": {
        "id": "Mciiiw9JAx00",
        "outputId": "a6277888-9033-46d3-d8e6-0cee2d563089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[22., 22., 23., 23., 11., 24., 24., 23., 23.,  1., 24., 24.,  7.,  1.,\n",
              "          1., 13., 13., -1., 13., 13., 13., 13., 13., 13., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also accomplish\n",
        "the same thing using `view()`."
      ],
      "metadata": {
        "id": "WfQT9HbRBD6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled.view(1, -1)"
      ],
      "metadata": {
        "id": "EaCFFSjuBHm_",
        "outputId": "1cd735d4-085e-4d55-9e03-55f16ecd31de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[22., 22., 23., 23., 11., 24., 24., 23., 23.,  1., 24., 24.,  7.,  1.,\n",
              "          1., 13., 13., -1., 13., 13., 13., 13., 13., 13., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Typical Architecture"
      ],
      "metadata": {
        "id": "JctalF_yBkwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical architecture uses a sequence of one or more typical convolutional\n",
        "blocks, with each block consisting of three operations:\n",
        "\n",
        "1. Convolution\n",
        "2. Activation function\n",
        "3. Pooling\n",
        "\n",
        "As images go through these operations, they will shrink in size.\n",
        "\n",
        "After the sequence of blocks, the image gets flattened: Hopefully, at this stage,\n",
        "there is no loss of information occurring by considering each value in the flattened\n",
        "tensor a feature on its own.\n",
        "\n",
        "If you think of it, what those typical convolutional blocks do is\n",
        "akin to pre-processing images and converting them into\n",
        "features."
      ],
      "metadata": {
        "id": "8l-OK9GrBlmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LeNet-5"
      ],
      "metadata": {
        "id": "SoYzyLmtDvdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 is a seven-level convolutional neural network developed by Yann LeCun in\n",
        "1998 to recognize hand-written digits in 28x28 pixel images—the famous MNIST\n",
        "dataset!\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/architecture_lenet.png?raw=1)\n",
        "\n",
        "Adapting LeNet-5 to today’s standards, it could be implemented like this:"
      ],
      "metadata": {
        "id": "mxNhaZqrDwNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet = nn.Sequential()\n",
        "\n",
        "###### Featurizer ##########\n",
        "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
        "lenet.add_module(\"C1\", nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, padding=2))\n",
        "lenet.add_module(\"func1\", nn.ReLU())\n",
        "lenet.add_module(\"S2\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
        "lenet.add_module(\"C3\", nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
        "lenet.add_module(\"func2\", nn.ReLU())\n",
        "lenet.add_module(\"S4\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Block 3: 16@5x5 -> 120@1x1\n",
        "lenet.add_module(\"C3\", nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
        "lenet.add_module(\"func3\", nn.ReLU())\n",
        "\n",
        "# Flattening\n",
        "lenet.add_module(\"flatten\", nn.Flatten())\n",
        "\n",
        "###### Classification ##########\n",
        "# Hidden Layer\n",
        "lenet.add_module(\"F6\", nn.Linear(in_features=120, out_features=84))\n",
        "lenet.add_module(\"func4\", nn.ReLU())\n",
        "# Output Layer\n",
        "lenet.add_module(\"OUTPUT\", nn.Linear(in_features=84, out_features=10))"
      ],
      "metadata": {
        "id": "5D96gCAWFiT-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 used three convolutional blocks, although the last one does not have a\n",
        "max pooling, because the convolution already produces a single pixel.\n",
        "\n",
        "Then, these 120 values (or features) are flattened and fed to a typical hidden layer\n",
        "with 84 units. \n",
        "\n",
        "The last step is, obviously, the output layer, which produces ten\n",
        "logits to be used for digit classification (from 0 to 9, there are ten classes)."
      ],
      "metadata": {
        "id": "AHf-VodqHllp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiclass Classification"
      ],
      "metadata": {
        "id": "zQBdv5EHH1Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem is considered a multiclass classification problem if there are more than\n",
        "two classes. \n",
        "\n",
        "So, let’s keep it as simple as possible and build a model to classify\n",
        "images into three classes."
      ],
      "metadata": {
        "id": "MUOmv1jzH2Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Generation"
      ],
      "metadata": {
        "id": "0HQF-4LrRX0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our images are going to have either a diagonal or a parallel line, BUT this time we\n",
        "will make a distinction between a diagonal line tilted to the right, a diagonal line\n",
        "tilted to the left, and a parallel line like this.\n",
        "\n",
        "| Line | Label/Class Index |\n",
        "|---|---|\n",
        "|Parallel (Horizontal OR Vertical)|0|\n",
        "|Diagonal, Tilted to the Right|0|\n",
        "|Diagonal, Tilted to the Left|1|\n",
        "\n",
        "\n",
        "Also, let’s generate more and larger images: one thousand images, each one tenby-\n",
        "ten pixels in size.\n"
      ],
      "metadata": {
        "id": "hWShsbv3RYhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=17)"
      ],
      "metadata": {
        "id": "ZOaSDcMnR_mo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plot_images(images, labels, n_plot=30)"
      ],
      "metadata": {
        "id": "V6ZYjv5ASPbl",
        "outputId": "580b1b77-9e26-46f0-e8bf-3ba4640dbc0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x324 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preparation"
      ],
      "metadata": {
        "id": "rTucZ1-ESX1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our particular problem, flipping an image has the potential to ruin the label.\n",
        "\n",
        "So, we’re only keeping the min-max scaling by using the Normalize()\n",
        "transform. All the rest remains the same: splitting, datasets, sampler, and data loaders."
      ],
      "metadata": {
        "id": "GQ5OijgDSYgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformedDataset(Dataset):\n",
        "  def __init__(self, x, y, transform=None):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.x[index]\n",
        "\n",
        "    # transforms the features\n",
        "    if self.transform:\n",
        "      x = self.transform(x)\n",
        "\n",
        "    return x, self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ],
      "metadata": {
        "id": "cLU_wNwnTO40"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Builds tensors from numpy arrays BEFORE split\n",
        "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
        "x_tensor = torch.as_tensor(images / 255).float()\n",
        "y_tensor = torch.as_tensor(labels).long()\n",
        "\n",
        "# Uses index_splitter to generate indices for training and validation sets\n",
        "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
        "# Uses indices to perform the split\n",
        "x_train_tensor = x_tensor[train_idx]\n",
        "y_train_tensor = y_tensor[train_idx]\n",
        "x_val_tensor = x_tensor[val_idx]\n",
        "y_val_tensor = y_tensor[val_idx]\n",
        "\n",
        "# Builds different composers because of data augmentation on training set\n",
        "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "\n",
        "# Uses custom dataset to apply composed transforms to each set\n",
        "train_dataset = TransformedDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
        "val_dataset = TransformedDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
        "\n",
        "# Builds a weighted random sampler to handle imbalanced classes\n",
        "weighted_sampler = make_balanced_sampler(y_train_tensor)\n",
        "\n",
        "# Uses sampler in the training set to get a balanced data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=weighted_sampler)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "XsWiPeOnTY5F"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loss"
      ],
      "metadata": {
        "id": "r11hE-9NUoIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In binary classification problems, the model would produce one logit, and one logit\n",
        "only, for each data point.\n",
        "\n",
        "And we used a sigmoid function to map logits to probabilities. It was a\n",
        "simple world :-)\n",
        "\n",
        "But a multiclass classification is more complex:\n",
        "that is, we need to get log odds ratios for every possible class. \n",
        "\n",
        "In other words, we\n",
        "need as many logits as there are classes.\n",
        "\n",
        "The softmax function returns, for each class, the contribution that a given class\n",
        "had to the sum of odds ratios. The class with a higher odds ratio will have the\n",
        "largest contribution and thus the highest probability.\n",
        "\n",
        "Since the softmax is computed using odds ratios instead of log\n",
        "odds ratios (logits), we need to exponentiate the logits!\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\begin{array}\n",
        "& z & = \\text{logit}(p) & = \\text{log odds ratio }(p) & = \\text{log}\\left(\\frac{p}{1-p}\\right)\n",
        "\\\\\n",
        "e^z & = e^{\\text{logit}(p)} & = \\text{odds ratio }(p) & = \\left(\\frac{p}{1-p}\\right)\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The softmax formula itself is quite simple:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{c=0}^{N_c-1}{e^{z_c}}}\n",
        "$$\n",
        "\n",
        "In our example, we have three classes, so our model\n",
        "needs to output three logits $(z_0, z_1, z_2)$.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{softmax}(z) = \\left[\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\right]\n",
        "$$\n",
        "\n",
        "Simple, right? Let’s see it in code now."
      ],
      "metadata": {
        "id": "Ud9Cm5-_UpLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([1.3863, 0.0000, -0.6931])\n",
        "\n",
        "# We exponentiate the logits to get the corresponding odds ratios\n",
        "odds_ratios = torch.exp(logits)\n",
        "odds_ratios"
      ],
      "metadata": {
        "id": "BqBNi_jwZ8hQ",
        "outputId": "694d220f-3598-45cf-9485-367eb3b7e20e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.0000, 1.0000, 0.5000])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we take these\n",
        "odds and add them together, and then compute each class' contribution to the\n",
        "sum:"
      ],
      "metadata": {
        "id": "1BGQLQUWaVhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmaxed = odds_ratios / odds_ratios.sum()\n",
        "softmaxed"
      ],
      "metadata": {
        "id": "xNNCa6oRaWMD",
        "outputId": "1ce92922-7d30-4f7b-995f-7f31686ddb45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voilà! Our logits were softmaxed: The probabilities are proportional to the odds\n",
        "ratios. \n",
        "\n",
        "This data point most likely belongs to the first class since it has a probability\n",
        "of `72.73%`.\n",
        "\n",
        "PyTorch\n",
        "provides the typical implementations: functional (`F.softmax()`) and module\n",
        "(`nn.Softmax`):"
      ],
      "metadata": {
        "id": "Q4oRHiE7ajB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Softmax(dim=-1)(logits)"
      ],
      "metadata": {
        "id": "IfxB53jkaqUc",
        "outputId": "fdbe3454-9486-4aff-b1d7-29fbcb59a898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "cbQrD9Pja1H8",
        "outputId": "70ec369d-e8db-4055-dfaf-dae6ac090b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, our models will produce logits with the shape (number of\n",
        "data points, number of classes), so the right dimension to apply softmax to is the\n",
        "last one (`dim=-1`)."
      ],
      "metadata": {
        "id": "fEor6RIGbCVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Negative Log Likelihood Loss"
      ],
      "metadata": {
        "id": "HaH_KJtAbSiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since softmax returns probabilities, logsoftmax returns log probabilities. And\n",
        "that’s the input for computing the negative log-likelihood loss, or `nn.NLLLoss()` for\n",
        "short. \n",
        "\n",
        "This loss is simply an extension of the binary cross-entropy loss for handling\n",
        "multiple classes.\n",
        "\n",
        "This was the formula for computing binary cross-entropy:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\texttt{BCE}(y)={-\\frac{1}{(N_{\\text{pos}}+N_{\\text{neg}})}\\Bigg[{\\sum_{i=1}^{N_{\\text{pos}}}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_{\\text{neg}}}{\\text{log}(1 - \\text{P}(y_i=1))}}\\Bigg]}\n",
        "$$\n",
        "\n",
        "In our example, there are three\n",
        "classes; that is, our `labels (y)` could be either zero, one, or two. \n",
        "\n",
        "So, the loss function\n",
        "will look like this:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+N_1+N_2)}\\Bigg[{\\sum_{i=1}^{N_0}{\\text{log}(\\text{P}(y_i=0))} + \\sum_{i=1}^{N_1}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_2}{\\text{log}(\\text{P}(y_i=2))}}\\Bigg]}\n",
        "$$\n",
        "\n",
        "The loss only considers the predicted probability for the true\n",
        "class.\n",
        "\n",
        "If a data point is labeled as belonging to class index two, the loss will consider the\n",
        "probability assigned to class index two only. The other probabilities will be\n",
        "completely ignored.\n",
        "\n",
        "For a total of C classes, the formula can be written like this:\n",
        "\n",
        "$$\n",
        "\\Large \\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+\\cdots+N_{C-1})}\\sum_{c=0}^{C-1}{\\sum_{i=1}^{N_c}{\\text{log}(\\text{P}(y_i=c))} }}\n",
        "$$\n",
        "\n",
        "Since the log probabilities are obtained by applying logsoftmax, this loss isn’t doing\n",
        "much more than looking up the inputs corresponding to the true class and adding\n",
        "them up."
      ],
      "metadata": {
        "id": "awfzVKepbXl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "log_probs"
      ],
      "metadata": {
        "id": "vSWFFddA4cXQ",
        "outputId": "d6ef126a-a46c-4fcd-cc2e-fe8355abf8ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3185, -1.7048, -2.3979])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the log probabilities for each class we computed using logsoftmax for\n",
        "our single data point. \n",
        "\n",
        "Now, let’s assume its label is two: What is the corresponding\n",
        "loss?"
      ],
      "metadata": {
        "id": "nGVn701U4qXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.tensor([2])\n",
        "F.nll_loss(log_probs.view(-1, 3), label)"
      ],
      "metadata": {
        "id": "QdhWtNv_4rmW",
        "outputId": "4442b85c-2ec4-4056-af5b-e1e795f5027a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3979)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is the negative of the log probability corresponding to the class index (two) of the true label.\n",
        "\n",
        "Let’s go through some quick examples."
      ],
      "metadata": {
        "id": "KbCzoPye5FZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "\n",
        "dummy_logits = torch.randn((5, 3))\n",
        "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
        "dummy_log_probs = F.log_softmax(dummy_logits, dim=-1)\n",
        "dummy_log_probs"
      ],
      "metadata": {
        "id": "mR0zpBroCnpd",
        "outputId": "4570701f-8d19-40fa-8b8a-67c9a09100c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5229, -0.3146, -2.9600],\n",
              "        [-1.7934, -1.0044, -0.7607],\n",
              "        [-1.2513, -1.0136, -1.0471],\n",
              "        [-2.6799, -0.2219, -2.0367],\n",
              "        [-1.0728, -1.9098, -0.6737]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_log_probs = torch.tensor([-1.5229, -1.7934, -1.0136, -2.0367, -1.9098])\n",
        "- relevant_log_probs.mean()"
      ],
      "metadata": {
        "id": "yBvviQ6p5Ewv",
        "outputId": "a0503007-ce26-443d-f89c-5b02b8f787cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6553)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s use `nn.NLLLoss()` to create the actual loss function, and then use\n",
        "predictions and labels to check if we got the relevant log probabilities right:"
      ],
      "metadata": {
        "id": "A1obaVV0Dm_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss()\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ],
      "metadata": {
        "id": "edSASBOeDo78",
        "outputId": "e872fc38-22f1-4db3-b52f-2d3ee32a463b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6553)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to balance our dataset, giving data points with label\n",
        "(y=2) double the weight of the other classes?"
      ],
      "metadata": {
        "id": "eMQhm4ERD-De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss(weight=torch.tensor([1., 1., 2.]))\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ],
      "metadata": {
        "id": "WMq3-ywGD-iu",
        "outputId": "b10ad060-e446-441d-a6d8-6374787859ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7188)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what if we want to simply ignore data points with label (y=2)?"
      ],
      "metadata": {
        "id": "N9PptD6oEOuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss(ignore_index=2)\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ],
      "metadata": {
        "id": "leCDdk5zEPHN",
        "outputId": "5e98919d-1727-4a1f-becc-8565f9a856cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5599)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, once again, there is yet another loss function available for multiclass\n",
        "classification. \n",
        "\n",
        "And, once again, it is very important to know when to use one or the\n",
        "other.\n",
        "\n",
        "So you don’t end up with an inconsistent combination of model and loss\n",
        "function."
      ],
      "metadata": {
        "id": "BOlVtIMPEf50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "0-6_NJmLElqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loss function combines both the logsoftmax layer and\n",
        "the negative log-likelihood loss into one.\n",
        "\n",
        "But you must use the right\n",
        "combination of model and loss function!\n",
        "\n",
        "- **Option 1**: nn.LogSoftmax as the last layer, meaning your model is\n",
        "producing log probabilities, combined with the `nn.NLLLoss()`\n",
        "function.\n",
        "- **Option 2**: No logsoftmax in the last layer, meaning your model is\n",
        "producing logits, combined with the `nn.CrossEntropyLoss()`\n",
        "function.\n",
        "\n",
        "Mixing `nn.LogSoftmax` and `nn.CrossEntropyLoss()` is just wrong.\n",
        "\n",
        "Let’s see a quick example of its usage."
      ],
      "metadata": {
        "id": "CDb2kCe0Enzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "\n",
        "dummy_logits = torch.randn((5, 3))\n",
        "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn(dummy_logits, dummy_labels)"
      ],
      "metadata": {
        "id": "ZYnXOOgl_nuL",
        "outputId": "b98b91b2-cffe-4bbe-b859-a3226dc83a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6553)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No logsoftmax whatsoever, but the same resulting loss, as expected."
      ],
      "metadata": {
        "id": "IIiI-s1TBWgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional neural network"
      ],
      "metadata": {
        "id": "d2A3AgkuCkzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s build our first convolutional neural network for real! \n",
        "\n",
        "We can use the typical\n",
        "convolutional block: convolutional layer, activation function, pooling layer. \n",
        "\n",
        "Our images are quite small, so we only need one of those."
      ],
      "metadata": {
        "id": "DAeTuz9UCnCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(13)\n",
        "\n",
        "cnn_model = nn.Sequential()\n",
        "\n",
        "##### Featurizer #######\n",
        "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
        "n_channels = 1\n",
        "cnn_model.add_module(\"conv1\", nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
        "cnn_model.add_module(\"relu1\", nn.ReLU())\n",
        "cnn_model.add_module(\"maxp1\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Flattening: n_channels * 4 * 4\n",
        "cnn_model.add_module(\"flatten\", nn.Flatten())\n",
        "\n",
        "###### Classification ######\n",
        "# Hidden Layer\n",
        "cnn_model.add_module(\"fc1\", nn.Linear(in_features=n_channels * 4 * 4, out_features=10))\n",
        "cnn_model.add_module(\"relu2\", nn.ReLU())\n",
        "# Output Layer\n",
        "cnn_model.add_module(\"fc2\", nn.Linear(in_features=10, out_features=3))"
      ],
      "metadata": {
        "id": "Ji7IJNt8DG4w"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model produces logits, we must use the `nn.CrossEntropyLoss()`."
      ],
      "metadata": {
        "id": "O1gz86WJGaJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = optim.SGD(cnn_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "2FzYwMgGGcq0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model."
      ],
      "metadata": {
        "id": "oD03MVkQG85T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_cnn1 = StepByStep(cnn_model, loss_fn, optimizer)\n",
        "sbs_cnn1.set_loaders(train_loader, val_loader)"
      ],
      "metadata": {
        "id": "Kyx80lYKG_Ye"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_cnn1.train(20)"
      ],
      "metadata": {
        "id": "Oueo-KIxHTEz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = sbs_cnn1.plot_losses()"
      ],
      "metadata": {
        "id": "CrrLaiZbIzz4",
        "outputId": "1b27d713-d926-4e0e-c66f-37fd40d47f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualizing Filters"
      ],
      "metadata": {
        "id": "AkikDvumI3Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nY2ZIKl4I4YS"
      }
    }
  ]
}