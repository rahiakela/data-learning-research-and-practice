{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzfHdgz5akgSxpGiFhnJnk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/02_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutions"
      ],
      "metadata": {
        "id": "8A9MVvQ5_T5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convolution is \"a mathematical operation on two functions (`f` and `g`) that produces a third function (`f * g`) expressing how the shape of one is modified by the other.\"\n",
        "\n",
        "In image processing, a convolution matrix is also called a kernel or filter. \n",
        "\n",
        "Typical image processing operations—like blurring, sharpening, edge detection, and more, are\n",
        "accomplished by performing a convolution between a kernel and an image."
      ],
      "metadata": {
        "id": "7K_rxSuE_Uhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "BebAYVWo_VlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ],
      "metadata": {
        "id": "iiQ_qcSp_Uq6",
        "outputId": "23b491bf-af47-4bee-bad2-fff0e909fa68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter5()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter5 import *"
      ],
      "metadata": {
        "id": "6EwSrmrQ_cxf",
        "outputId": "f16fd21b-f51f-4337-e166-b050827233be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose, Normalize\n",
        "\n",
        "from data_generation.image_classification import generate_dataset\n",
        "from helpers import index_splitter, make_balanced_sampler\n",
        "from stepbystep.v1 import StepByStep"
      ],
      "metadata": {
        "id": "ZNIlTCW9_gBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter / Kernel"
      ],
      "metadata": {
        "id": "Rsnr8thUATxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, the filters are\n",
        "small square matrices. The convolution itself is performed by applying the filter on\n",
        "the image repeatedly. \n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv1.png?raw=1)\n",
        "\n",
        "That’s the region to which the filter is being applied and is called the\n",
        "receptive field, drawing an analogy to the way human vision works.\n",
        "\n",
        "Let’s try a concrete example to make it more clear."
      ],
      "metadata": {
        "id": "xHjtaGl4ETM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single = np.array([\n",
        "  [ # batch dim\n",
        "      [ # channel dim\n",
        "          [5, 0, 8, 7, 8, 1], # height and width dim\n",
        "          [1, 9, 5, 0, 7, 7],\n",
        "          [6, 0, 2, 4, 6, 6],\n",
        "          [9, 7, 6, 6, 8, 4],\n",
        "          [8, 3, 8, 5, 1, 3],\n",
        "          [7, 2, 7, 0, 1, 0]\n",
        "      ]\n",
        "  ]\n",
        "])\n",
        "single.shape"
      ],
      "metadata": {
        "id": "jPyifdt-AUSX",
        "outputId": "c29ff7e2-e8f6-45db-df00-34509ee3c8a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 6, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "identity = np.array([\n",
        "    [\n",
        "        [\n",
        "            [0, 0, 0],\n",
        "            [0, 1, 0],\n",
        "            [0, 0, 0]\n",
        "        ]\n",
        "    ]\n",
        "])\n",
        "identity.shape"
      ],
      "metadata": {
        "id": "oiO3F9UGFO9G",
        "outputId": "6fcd9d6e-a3c9-4e90-c558-1e5976fbba46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolving"
      ],
      "metadata": {
        "id": "KkW-hK_CFhg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convulution performs an element-wise multiplication between the\n",
        "two, region and filter, and adds everything up.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv2.png?raw=1)"
      ],
      "metadata": {
        "id": "EUxO8zMfFkXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region = single[:, :, 0:3, 0:3]\n",
        "filtered_region = region * identity\n",
        "total = filtered_region.sum()\n",
        "total"
      ],
      "metadata": {
        "id": "CVEwetTtF1H2",
        "outputId": "09b76b46-8d7a-4f6f-e322-1c315ec7dc85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing a convolution produces an image with a\n",
        "reduced size.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv3.png?raw=1)"
      ],
      "metadata": {
        "id": "X69C4K0gGbWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving Around"
      ],
      "metadata": {
        "id": "2CYbWbW-HxNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we move the region one step to the right; that is, we change the receptive\n",
        "field and apply the filter again.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/stride1.png?raw=1)\n",
        "\n",
        "In code, it means we’re changing the slice of the input image:"
      ],
      "metadata": {
        "id": "B-uOhu84IWU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_region = single[:, :, 0:3, (0+1):(3+1)]"
      ],
      "metadata": {
        "id": "UN7eF6XnGotK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the operation remains the same: First, an element-wise multiplication, and\n",
        "then adding up the elements of the resulting matrix.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv5.png?raw=1)"
      ],
      "metadata": {
        "id": "p801DPVnJLI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_filtered_region = new_region * identity\n",
        "new_total = new_filtered_region.sum()\n",
        "new_total"
      ],
      "metadata": {
        "id": "tu1Xmj6HJOyX",
        "outputId": "978318b1-75a8-45ad-fb12-f49cbb84381d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have a second pixel value to add to our resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv6.png?raw=1)\n",
        "\n",
        "We can keep moving the gray region to the right until we can’t move it anymore.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv7.png?raw=1)\n",
        "\n",
        "The fourth step to the right will actually place the region partially outside the\n",
        "input image."
      ],
      "metadata": {
        "id": "N_zfUbc4KUzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
      ],
      "metadata": {
        "id": "g8r2NNINKeUI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected region does not match the shape of the filter anymore. \n",
        "\n",
        "So, if we try to\n",
        "perform the element-wise multiplication, it fails:"
      ],
      "metadata": {
        "id": "XP8Uby8_K4nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  last_horizontal_region * identity\n",
        "except Exception as exp:\n",
        "  print(exp)"
      ],
      "metadata": {
        "id": "zcWMNoMCK5w_",
        "outputId": "83bea8a9-9e9e-43f9-f702-43c99357f0d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Shape"
      ],
      "metadata": {
        "id": "jBkcwd9cLRHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we go back to the left side and move down one step. If we repeat the\n",
        "operation, covering all valid regions, we’ll end up with a resulting image that is\n",
        "smaller (on the right).\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv8.png?raw=1)\n",
        "\n",
        "How much smaller is it going to be?\n",
        "\n",
        "It depends on the size of the filter.\n",
        "\n",
        ">The larger the filter, the smaller the resulting image.\n",
        "\n",
        "Since applying a filter always produces a single value, the reduction is equal to the\n",
        "filter size minus one.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * (h_f, w_f) = (h_i - (h_f - 1), w_i - (w_f - 1))\n",
        "$$\n",
        "\n",
        "If we assume the filter is a square matrix of size f, we can simplify the expression\n",
        "above to:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
        "$$\n",
        "\n",
        "But I’d like to keep the image size, is it possible?\n",
        "\n",
        "Sure it is! Padding comes to our rescue in this case."
      ],
      "metadata": {
        "id": "lhKlTPhcLR-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolving in PyTorch"
      ],
      "metadata": {
        "id": "iZdPF3vJSJNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how a convolution works, let’s try it out using PyTorch."
      ],
      "metadata": {
        "id": "svvUMTDGSKHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our image and filter to tensors\n",
        "image = torch.as_tensor(single).float()\n",
        "kernel = torch.as_tensor(identity).float()"
      ],
      "metadata": {
        "id": "FpDNP9utXtc6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the activation functions, convolutions come in two\n",
        "flavors: functional and module. \n",
        "\n",
        "There is a fundamental difference between the\n",
        "two, though: The functional convolution takes the kernel / filter as an argument\n",
        "while the module has (learnable) weights to represent the kernel / filter.\n",
        "\n",
        "Let’s use the functional convolution, `F.conv2d()`, to apply the identity filter to our\n",
        "input image."
      ],
      "metadata": {
        "id": "7XxFSecQYQao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved = F.conv2d(image, kernel, stride=1)\n",
        "convolved"
      ],
      "metadata": {
        "id": "Fm6TCTCpYfRF",
        "outputId": "634f15ce-2686-408e-fc5c-c2a57e80bcbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 5., 0., 7.],\n",
              "          [0., 2., 4., 6.],\n",
              "          [7., 6., 6., 8.],\n",
              "          [3., 8., 5., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s turn our attention to PyTorch’s convolution module, `nn.Conv2d`."
      ],
      "metadata": {
        "id": "2jPbCKifZuGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
        "conv(image)"
      ],
      "metadata": {
        "id": "UxDu6pa1ZwRu",
        "outputId": "2888c6f6-cf06-4c32-feb3-f73c48733585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-3.3911, -3.7047, -4.7970, -3.7433],\n",
              "          [-2.7030, -2.2508, -1.7098, -3.9630],\n",
              "          [-1.2715, -2.0736, -4.1887, -4.2676],\n",
              "          [-4.5482, -6.0497, -4.9992, -4.8995]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results are gibberish now because the convolutional module randomly initializes the weights representing\n",
        "the kernel / filter.\n",
        "\n",
        "That’s the whole point of the convolutional module: It will learn\n",
        "the kernel / filter on its own.\n",
        "\n",
        "In traditional computer vision, people would develop different\n",
        "filters for different purposes: blurring, sharpening, edge\n",
        "detection, and so on.\n",
        "\n",
        "Can we tell it to learn multiple filters at once?"
      ],
      "metadata": {
        "id": "0mRxitR_aIFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_multiple = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1)\n",
        "conv_multiple(image)"
      ],
      "metadata": {
        "id": "-tkVCkuuasJi",
        "outputId": "b9eb7b30-3a48-4192-bf47-490499ceceb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1.1950, -0.5475, -1.6562, -4.1498],\n",
              "          [-4.2551, -5.4121, -0.5433, -1.3919],\n",
              "          [-0.2890,  0.4585,  0.2697, -0.6600],\n",
              "          [-1.8518, -1.9701, -1.3744, -1.9982]],\n",
              "\n",
              "         [[ 0.6643, -0.7386, -1.2956, -0.3658],\n",
              "          [-4.5518, -2.0294, -0.3616, -2.6893],\n",
              "          [ 1.5473, -0.3799, -1.9312, -0.1955],\n",
              "          [-0.4493, -2.2443, -2.3139, -1.5712]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_multiple.weight"
      ],
      "metadata": {
        "id": "ToKEQ1v1a_fp",
        "outputId": "5f5f1654-79f7-493e-9b8e-aaa6c4dde418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.2203, -0.3247,  0.2006],\n",
              "          [ 0.1443,  0.0635,  0.2688],\n",
              "          [-0.1570, -0.1970, -0.0752]]],\n",
              "\n",
              "\n",
              "        [[[-0.0349, -0.1760, -0.1336],\n",
              "          [ 0.0235,  0.1963,  0.1929],\n",
              "          [-0.2317, -0.3288,  0.2630]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also force a convolutional module to use a particular filter by setting its weights."
      ],
      "metadata": {
        "id": "T3e8p3TVdKkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  conv.weight[0] = kernel\n",
        "  conv.bias[0] = 0\n",
        "\n",
        "conv(image)"
      ],
      "metadata": {
        "id": "uzMNjIh6dMBq",
        "outputId": "5fbdfd8b-9bb8-4998-ac88-e919ce5e62d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 5., 0., 7.],\n",
              "          [0., 2., 4., 6.],\n",
              "          [7., 6., 6., 8.],\n",
              "          [3., 8., 5., 1.]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.weight"
      ],
      "metadata": {
        "id": "b5TEqL4ud8zW",
        "outputId": "fd0e9260-cf7a-4f80-a1a2-84e6cd751343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[0., 0., 0.],\n",
              "          [0., 1., 0.],\n",
              "          [0., 0., 0.]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the weights to get specific filters is at the heart of\n",
        "transfer learning. \n",
        "\n",
        "Someone else trained a model, and that model\n",
        "learned lots of useful filters, so we don’t have to learn them\n",
        "again. We can set the corresponding weights and go from there."
      ],
      "metadata": {
        "id": "5gYV5KxYeIWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Striding"
      ],
      "metadata": {
        "id": "t9HI8NnZeKMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try a stride of two for a change and see what happens to the resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/strider2.png?raw=1)\n",
        "\n",
        "The resulting image, after the only four valid operations, looks like this.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/strider3.png?raw=1)\n",
        "\n",
        "Also, notice that using a larger stride made the shape of the resulting image even smaller.\n",
        "\n",
        ">The larger the stride, the smaller the resulting image.\n",
        "\n",
        "Once again, it makes sense: If we are skipping pixels in the input image, there are\n",
        "fewer regions of interest to apply the filter to. \n",
        "\n",
        "We can extend our previous formula\n",
        "to include the stride size (s):\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{h_i - f + 1}{s}, \\frac{w_i - f + 1}{s}\\right)\n",
        "$$\n",
        "\n",
        "Let’s use\n",
        "PyTorch’s functional convolution to double-check the results."
      ],
      "metadata": {
        "id": "fyfsZe1MeMGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved_stride2 = F.conv2d(image, kernel, stride=2)\n",
        "convolved_stride2"
      ],
      "metadata": {
        "id": "NSgTQekvyu-3",
        "outputId": "72dfc51a-656e-4f4d-8224-e1c685988aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 0.],\n",
              "          [7., 6.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, it works!"
      ],
      "metadata": {
        "id": "3TTYNw37zEc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Padding"
      ],
      "metadata": {
        "id": "nVacXfTkzE46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, the operations we have performed have been shrinking the images. What\n",
        "about restoring them to their original glory, I mean, size?\n",
        "\n",
        "Padding means stuffing. We need to stuff the original image so it can sustain the \"attack\" on its size.\n",
        "\n",
        "How do I stuff an image?\n",
        "\n",
        "Simply add zeros around it.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding1.png?raw=1)\n",
        "\n",
        "By adding columns and rows of zeros around it, we expand the\n",
        "input image such that the gray region starts centered in the actual top left corner\n",
        "of the input image. \n",
        "\n",
        "This simple trick can be used to preserve the original size of the\n",
        "image.\n",
        "\n",
        "In code, as usual, PyTorch gives us two options: functional (`F.pad()`) and module (`nn.ConstantPad2d`).\n",
        "\n",
        "Let’s start with the module version this time:"
      ],
      "metadata": {
        "id": "gexEBVsuzH1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constant_padder = nn.ConstantPad2d(padding=1, value=0)\n",
        "constant_padder(image)"
      ],
      "metadata": {
        "id": "GIuPaIjY0dxq",
        "outputId": "8374f968-4646-4287-998e-a5eda33cd5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
              "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
              "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
              "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also do asymmetric padding by specifying a tuple in the padding\n",
        "argument representing (left, right, top, bottom). \n",
        "\n",
        "So, if we were to stuff our\n",
        "image on the left and right sides only, the argument would go like this: `(1, 1, 0, 0)`.\n",
        "\n",
        "We can achieve the same result using the functional padding:"
      ],
      "metadata": {
        "id": "8My1qAQY1ynT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"constant\", value=0)\n",
        "padded"
      ],
      "metadata": {
        "id": "wrfPCPMz1yCk",
        "outputId": "5c9cd760-5053-4dc4-a573-2edc5b7c9c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
              "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
              "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
              "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, there is another argument, mode, which was\n",
        "set to constant to match the module version above.\n",
        "\n",
        "What are the other available modes?\n",
        "\n",
        "There are three other modes: replicate, reflect, and circular.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/paddings.png?raw=1)\n",
        "\n",
        "In replication padding, the padded pixels have the same value as the closest real\n",
        "pixel.\n",
        "\n",
        "In PyTorch, one can use the functional form `F.pad()` with mode=\"replicate\", or use\n",
        "the module version `nn.ReplicationPad2d`:"
      ],
      "metadata": {
        "id": "vfbBSTl_2yYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replication_padder = nn.ReplicationPad2d(padding=1)\n",
        "replication_padder(image)"
      ],
      "metadata": {
        "id": "JtIfohjj4Bss",
        "outputId": "2f18e6be-a766-4435-b3ea-c53e722a48f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
              "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"replicate\")\n",
        "padded"
      ],
      "metadata": {
        "id": "bTbOtd1_4el9",
        "outputId": "14b393dd-c5a7-4a6f-f750-b883c20a53f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
              "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reflection padding, it gets a bit trickier. It is like the outer columns and rows are\n",
        "used as axes for the reflection.\n",
        "\n",
        "In PyTorch, you can use the functional form `F.pad()` with mode=\"reflect\", or use\n",
        "the module version `nn.ReflectionPad2d`:"
      ],
      "metadata": {
        "id": "A2l0de5N4ybA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reflection_padder = nn.ReflectionPad2d(padding=1)\n",
        "reflection_padder(image)"
      ],
      "metadata": {
        "id": "bE-tinAm44rs",
        "outputId": "f021af9e-ec56-4bd8-e59e-083d52716e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
              "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
              "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"reflect\")\n",
        "padded"
      ],
      "metadata": {
        "id": "-HedDa4b5Gor",
        "outputId": "3dd74680-cad9-4adb-ea73-73ad472eb458",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
              "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
              "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In circular padding, the left-most (right-most) column gets copied as the right (left) padded column.\n",
        "\n",
        "Similarly, the top-most\n",
        "(bottom-most) row gets copied as the bottom (top) padded row. The corners\n",
        "receive the values of the diametrically opposed corner.\n",
        "\n",
        "In PyTorch, you must use the functional form `F.pad()` with mode=\"circular\" since\n",
        "there is no module version of the circular padding."
      ],
      "metadata": {
        "id": "qscKbJFg5atz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"circular\")\n",
        "padded"
      ],
      "metadata": {
        "id": "wIjXVU7i5smI",
        "outputId": "ba0b4a4c-e4a6-4c0c-96b9-b9eea991861d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
              "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
              "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
              "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By padding an image, it is possible to get resulting images with the same shape as\n",
        "input images, or even larger, should you choose to stuff more and more rows and\n",
        "columns into the input image. \n",
        "\n",
        "Assuming we’re doing symmetrical padding of size p,\n",
        "the resulting shape is given by the formula below:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{(h_i + 2p) - f + 1}{s}, \\frac{(w_i + 2p) - f + 1}{s}\\right)\n",
        "$$\n",
        "\n",
        "We’re basically extending the original dimensions by 2p pixels each."
      ],
      "metadata": {
        "id": "00snBJCt5-eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A REAL Filter"
      ],
      "metadata": {
        "id": "yb1FSMZ16JvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try an edge detector filter from traditional\n",
        "computer vision for a change:"
      ],
      "metadata": {
        "id": "TOS3C8WI6KN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_matrix = np.array([\n",
        "  [[\n",
        "    [0, 1, 0],\n",
        "    [1, -4, 1],\n",
        "    [0, 1, 0]\n",
        "  ]]\n",
        "])\n",
        "\n",
        "kernel_edge = torch.as_tensor(edge_matrix).float()\n",
        "kernel_edge.shape"
      ],
      "metadata": {
        "id": "1b3D6o4e-kMx",
        "outputId": "aea0a3db-a0dc-4928-b9c0-68bfe0e9de88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let’s apply it to a different region of our (padded) input image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding2.png?raw=1)\n",
        "\n",
        "As you can see, filters, other than the identity one, do not simply copy the value at\n",
        "the center. \n",
        "\n",
        "The element-wise multiplication finally means something.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding3.png?raw=1)\n",
        "\n",
        "Let’s apply this filter to our image."
      ],
      "metadata": {
        "id": "ym94Q2j-_SxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"constant\", value=0)\n",
        "conv_padded = F.conv2d(padded, kernel_edge, stride=1)\n",
        "conv_padded"
      ],
      "metadata": {
        "id": "1370UbvBAgTx",
        "outputId": "98b9e152-5970-443f-8b68-379a93d4c227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-19.,  22., -20., -12., -17.,  11.],\n",
              "          [ 16., -30.,  -1.,  23.,  -7., -14.],\n",
              "          [-14.,  24.,   7.,  -2.,   1.,  -7.],\n",
              "          [-15., -10.,  -1.,  -1., -15.,   1.],\n",
              "          [-13.,  13., -11.,  -5.,  13.,  -7.],\n",
              "          [-18.,   9., -18.,  13.,  -3.,   4.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling"
      ],
      "metadata": {
        "id": "FrV8PymAA8kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling is different than the former operations: It splits the image into tiny chunks, performs an operation on each chunk (that yields a single value), and puts the chunks together as the resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/pooling1.png?raw=1)\n",
        "\n",
        "Our input image is split into nine chunks, and we perform a simple max operation\n",
        "(hence, max pooling) on each chunk (really, it is just taking the largest value in each\n",
        "chunk). Then, these values are put together, in order, to produce a smaller\n",
        "resulting image.\n",
        "\n",
        ">The larger the pooling kernel, the smaller the resulting image.\n",
        "\n",
        "In PyTorch, as usual, we have both forms: `F.max_pool2d()` and `nn.MaxPool2d`. \n",
        "\n",
        "Let’s\n",
        "use the functional form to replicate the max pooling."
      ],
      "metadata": {
        "id": "Yxve0AcE8dGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ],
      "metadata": {
        "id": "127inD4k97KY",
        "outputId": "3e09c6a9-5e5f-4bc1-dd65-b4d533fdd403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[22., 23., 11.],\n",
              "          [24.,  7.,  1.],\n",
              "          [13., 13., 13.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then let’s use the module version to illustrate the large four-by-four pooling."
      ],
      "metadata": {
        "id": "9svGlR23_VEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxpool4 = nn.MaxPool2d(kernel_size=4)\n",
        "pooled4 = maxpool4(conv_padded)\n",
        "pooled4"
      ],
      "metadata": {
        "id": "8tMu3Vk5_Vte",
        "outputId": "99348d80-6bbe-4390-f87e-dd91fcb9e0b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[24.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides max pooling, average pooling is also fairly common. As the name\n",
        "suggests, it will output the average pixel value for each chunk. \n",
        "\n",
        "In PyTorch, we have\n",
        "`F.avg_pool2d()` and `nn.AvgPool2d`."
      ],
      "metadata": {
        "id": "NgJ8QWy-_0A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.avg_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ],
      "metadata": {
        "id": "_Yrn8wBE_8zR",
        "outputId": "7f8ada9c-6222-4ee8-9ac8-d8a83064f9d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.7500, -2.5000, -6.7500],\n",
              "          [-3.7500,  0.7500, -5.0000],\n",
              "          [-2.2500, -5.2500,  1.7500]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avgpool4 = nn.AvgPool2d(kernel_size=4)\n",
        "pooled4 = avgpool4(conv_padded)\n",
        "pooled4"
      ],
      "metadata": {
        "id": "4_DdnfoDAMx_",
        "outputId": "8d3ea968-a21d-4752-bac3-898a5a06b6e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.0625]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can I use a stride of a different size?"
      ],
      "metadata": {
        "id": "CNm1egCsAXcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2, stride=1)\n",
        "pooled"
      ],
      "metadata": {
        "id": "947Cu913AYAb",
        "outputId": "4911fa68-1505-43ac-ea2e-1cf24a6e49b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[22., 22., 23., 23., 11.],\n",
              "          [24., 24., 23., 23.,  1.],\n",
              "          [24., 24.,  7.,  1.,  1.],\n",
              "          [13., 13., -1., 13., 13.],\n",
              "          [13., 13., 13., 13., 13.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Flattening"
      ],
      "metadata": {
        "id": "SOvlpuBiAsC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It simply flattens a tensor, preserving the first\n",
        "dimension such that we keep the number of data points while collapsing all other\n",
        "dimensions."
      ],
      "metadata": {
        "id": "Abg65oZUAs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flattened = nn.Flatten()(pooled)\n",
        "flattened"
      ],
      "metadata": {
        "id": "Mciiiw9JAx00",
        "outputId": "3737bbcf-1daf-4467-8fa3-caee3624001a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[22., 22., 23., 23., 11., 24., 24., 23., 23.,  1., 24., 24.,  7.,  1.,\n",
              "          1., 13., 13., -1., 13., 13., 13., 13., 13., 13., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also accomplish\n",
        "the same thing using `view()`."
      ],
      "metadata": {
        "id": "WfQT9HbRBD6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled.view(1, -1)"
      ],
      "metadata": {
        "id": "EaCFFSjuBHm_",
        "outputId": "f7fdc2bb-6e70-4375-aaf9-8529a7132628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[22., 22., 23., 23., 11., 24., 24., 23., 23.,  1., 24., 24.,  7.,  1.,\n",
              "          1., 13., 13., -1., 13., 13., 13., 13., 13., 13., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Typical Architecture"
      ],
      "metadata": {
        "id": "JctalF_yBkwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical architecture uses a sequence of one or more typical convolutional\n",
        "blocks, with each block consisting of three operations:\n",
        "\n",
        "1. Convolution\n",
        "2. Activation function\n",
        "3. Pooling\n",
        "\n",
        "As images go through these operations, they will shrink in size.\n",
        "\n",
        "After the sequence of blocks, the image gets flattened: Hopefully, at this stage,\n",
        "there is no loss of information occurring by considering each value in the flattened\n",
        "tensor a feature on its own.\n",
        "\n",
        "If you think of it, what those typical convolutional blocks do is\n",
        "akin to pre-processing images and converting them into\n",
        "features."
      ],
      "metadata": {
        "id": "8l-OK9GrBlmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LeNet-5"
      ],
      "metadata": {
        "id": "SoYzyLmtDvdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 is a seven-level convolutional neural network developed by Yann LeCun in\n",
        "1998 to recognize hand-written digits in 28x28 pixel images—the famous MNIST\n",
        "dataset!\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/architecture_lenet.png?raw=1)\n",
        "\n",
        "Adapting LeNet-5 to today’s standards, it could be implemented like this:"
      ],
      "metadata": {
        "id": "mxNhaZqrDwNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet = nn.Sequential()\n",
        "\n",
        "###### Featurizer ##########\n",
        "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
        "lenet.add_module(\"C1\", nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, padding=2))\n",
        "lenet.add_module(\"func1\", nn.ReLU())\n",
        "lenet.add_module(\"S2\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
        "lenet.add_module(\"C3\", nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
        "lenet.add_module(\"func2\", nn.ReLU())\n",
        "lenet.add_module(\"S4\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Block 3: 16@5x5 -> 120@1x1\n",
        "lenet.add_module(\"C3\", nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
        "lenet.add_module(\"func3\", nn.ReLU())\n",
        "\n",
        "# Flattening\n",
        "lenet.add_module(\"flatten\", nn.Flatten())\n",
        "\n",
        "###### Classification ##########\n",
        "# Hidden Layer\n",
        "lenet.add_module(\"F6\", nn.Linear(in_features=120, out_features=84))\n",
        "lenet.add_module(\"func4\", nn.ReLU())\n",
        "# Output Layer\n",
        "lenet.add_module(\"OUTPUT\", nn.Linear(in_features=84, out_features=10))"
      ],
      "metadata": {
        "id": "5D96gCAWFiT-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 used three convolutional blocks, although the last one does not have a\n",
        "max pooling, because the convolution already produces a single pixel.\n",
        "\n",
        "Then, these 120 values (or features) are flattened and fed to a typical hidden layer\n",
        "with 84 units. \n",
        "\n",
        "The last step is, obviously, the output layer, which produces ten\n",
        "logits to be used for digit classification (from 0 to 9, there are ten classes)."
      ],
      "metadata": {
        "id": "AHf-VodqHllp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiclass Classification"
      ],
      "metadata": {
        "id": "zQBdv5EHH1Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem is considered a multiclass classification problem if there are more than\n",
        "two classes. \n",
        "\n",
        "So, let’s keep it as simple as possible and build a model to classify\n",
        "images into three classes."
      ],
      "metadata": {
        "id": "MUOmv1jzH2Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Generation"
      ],
      "metadata": {
        "id": "0HQF-4LrRX0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our images are going to have either a diagonal or a parallel line, BUT this time we\n",
        "will make a distinction between a diagonal line tilted to the right, a diagonal line\n",
        "tilted to the left, and a parallel line like this.\n",
        "\n",
        "| Line | Label/Class Index |\n",
        "|---|---|\n",
        "|Parallel (Horizontal OR Vertical)|0|\n",
        "|Diagonal, Tilted to the Right|0|\n",
        "|Diagonal, Tilted to the Left|1|\n",
        "\n",
        "\n",
        "Also, let’s generate more and larger images: one thousand images, each one tenby-\n",
        "ten pixels in size.\n"
      ],
      "metadata": {
        "id": "hWShsbv3RYhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=17)"
      ],
      "metadata": {
        "id": "ZOaSDcMnR_mo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plot_images(images, labels, n_plot=30)"
      ],
      "metadata": {
        "id": "V6ZYjv5ASPbl",
        "outputId": "c2cf91c6-1877-40c0-9070-da11a8b8c306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x324 with 30 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABB4AAAE0CAYAAACLqfDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVyU9d7/8TerGmqYlqPAgFAhqSluCTcnLXPrWBq5W6QldlzKldTs3KlZuGR6KtO7+3S808z9mGunY4tGopaZiUfNUBAEzVJxB1nm94cP59fEPjMXDPh6Ph48HuXMcH0u3tf3uq75zDXX1y0rK8siAAAAAAAAA7hXdgEAAAAAAKD6ovEAAAAAAAAMQ+MBAAAAAAAYhsYDAAAAAAAwDI0HAAAAAABgGBoPAAAAAADAMNW68XDixAn5+voqLy+vQl8L+/j6+ur48eMV/lqUDzlVDeRUNXCcqjrIqupg/1c1kFPV0aJFC23fvr3CX4vyc+Vx5VDj4eGHH1ZycrJSU1P14IMP2jx2/vx5DR48WI0bN1bz5s21Zs0au5eTkJCg++67z5FSnS4nJ0ejRo1SQECA7r33Xr377ruVXVKJSsrq/fffV6dOnXTXXXdpxIgRDi3HFU+uLBaLXn31VTVp0kRNmjTRq6++KovFUtllFam4nHJycjR69Gg1b95c/v7+ioqK0rZt2+xejivm9PXXX6tnz54ym81q0aJFZZdTopLG0/DhwxUaGqqAgAC1adNGS5cutXs5rpjT22+/rYiICPn7++v+++/X22+/XdkllaikrG46duyYGjZsqOHDh9u9HI5Tjispqz//+c9q2LCh/Pz85Ofnp7Zt29q9HFfMav369eratasaNWqkP//5z5VdTolKG1Pr1q1T+/bt1bhxY7Vq1UqJiYl2LYf9n2NKyunmOLr5c8cddyguLs6u5bhiTlXpfEIqOasTJ06ob9++CgwM1L333qu4uDiH/tau1rD5+OOP1bFjRwUEBOi+++7Tf//3f7vUtvRHJWX1008/6bHHHpPZbFZ4eLg2bdpk93JccVw54/2U3Y2H3NxcpaenKyQkRPv371fLli1tHp84caK8vb119OhR/e///q8mTJigw4cP27s4lzNr1iwdP35cSUlJ2rRpk95++219/vnnlV1WkUrLymQyaeLEiXrqqacqqUJj/d///Z+2bNmib775Rjt37tS//vUvLVmypLLLKqSknPLy8uTn56ctW7YoLS1Nr7zyioYOHaoTJ05UYsXO5ePjo6eeekozZsyo7FJKVNp4GjdunA4cOKD09HStWLFCM2fO1P79+yupWuezWCxatGiRUlNTtW7dOr3//vtat25dZZdVpNKyumnixIlq3bp1BVdnvOp0nJKkuXPnKiMjQxkZGdq7d28lVGmcevXqacSIERo7dmxll1Ki0nL66quv9Oqrr2rhwoU6efKktm7dqqCgoMop1gBVZf9XWk43x1FGRoZ++ukn1apVS717966kap2vqpxPSGV7P9WgQQP99NNPSkhI0M6dO/X3v/+9kqp1vmvXrik+Pl7Hjh3T559/rh07duidd96p7LKKVNp5+qBBg9StWzelpKRowYIFev7555WcnFyJFTuXM95P2d14OHTokEJDQ+Xm5qYffvjB5o9/5coVbdy4UVOnTlXt2rUVERGh7t27a9WqVfYurlifffaZ/vSnPykgIEDNmjVTfHx8oed89NFHatq0qUJDQ2025oKCAs2fP1+tWrVSkyZNNGTIEJ0/f75My12xYoXi4uLk6+ur0NBQxcTE6OOPP3baejlTSVlJ0uOPP66ePXvqjjvuMLSO77//Xl26dJHZbFZoaKji4uJ0/fp1m+f8+9//VsuWLRUcHKy//vWvKigosD62bNkytW/fXoGBgYqOjlZaWlqZlrtixQqNHj1afn5+aty4sUaNGuWSWZWUk4+Pj6ZMmaLAwEC5u7ure/fuMpvNhryhrayc2rRpowEDBrj8SWpp4yksLEw1atSQJLm5ucnNzU0pKSlOr6OychozZoxatWolT09P3XPPPXr00Ue1e/dup66bs5SWlXTj09nbb7+92KshnIHjVOnKklVFqKysOnXqpCeeeEKNGjVy2roYobSc4uPj9dJLL6ldu3Zyd3dX48aN1bhxY6fXwf6vZOUZTxs3blSDBg0UGRnp9Do4nyhdaVmdOHFCTzzxhGrWrKmGDRuqc+fOOnLkiNPrSElJ0WOPPaYmTZooODhYsbGxysrKsnnOvn379MADDygwMFAjR45Udna29bF//etfioqKktlsVteuXXXw4MEyLfe5555TZGSkvL291bhxY/Xt29clx5RUclZHjx7V6dOnNWrUKHl4eKhjx4564IEHtHLlSqfXUZXfT5W78fDRRx/JbDare/fu+u6772Q2m/Xuu+9q2rRpMpvNSk1NVXJysjw9PXX33XdbX9eiRQtDrni47bbbtHjxYp04cUKrVq3SP/7xD23evNnmOQkJCfr+++/1z3/+UwsWLLB+z+h//ud/tGXLFm3ZskVHjhyRr6+vJk6cWORy5s+fr/79+0uSsrKydPr0aTVv3txm/YzYETiiLFlVJA8PD73xxhs6fvy4/v3vf2vHjh2FurabN2/W9u3btWPHDm3dulUfffSRJGnLli166623tGzZMh07dkwREREaNmxYkctZs2aNzQH0yJEjLp2VPTmdOXNGx44dU1hYmNPrqaycXF15cpowYYIaNWqkdu3aqWHDhurSpYvT63GFnCwWi3bt2mXIduiIsmZ18eJFvfHGG3r99dcNrYfjVPHKM66mT5+u4OBgdevWTQkJCYbUUxlZVQVlySk/P18//PCDzp49q/DwcN13332Ki4vTtWvXnF4P+7+i2XM+sWLFCg0YMEBubm5Or8cVcnJVZc1qxIgRWrduna5evarMzEx9/vnn6ty5s9PrsVgsGj9+vI4cOaJvv/1WJ0+e1KxZs2yes2bNGq1bt0779+/XsWPH9Oabb0qSfvzxR40ePVoLFixQSkqKhgwZooEDByonJ6fQcnbt2iWz2VxsHYmJiS41piT7309ZLBZD3vtW5fdT5W48PPXUU0pLS1OrVq20bds27dy5U2FhYUpPT1daWpqCgoJ05coV1alTx+Z1devW1eXLl8u7uFL96U9/UrNmzeTu7q7mzZvrySef1M6dO22eM2nSJPn4+KhZs2YaPHiw1q5dK0lasmSJ/vrXv8rPz081atTQ5MmTtWHDhiK/TzNu3DjrFRs316Nu3bo263fp0iWnr58jypJVRWrVqpXatWsnT09PBQYGasiQIYWyGjt2rOrVq6eAgACNGDHCJqtx48YpNDRUnp6emjBhgpKSkors0vXt29fmO6WXL18ulNXly5dd5j4P5c0pNzdXsbGxGjhwoO69916n11NZObm68uQ0b948nTx5Up9++qkee+wx6xUQzuQKOcXHx6ugoECDBw92+vo5oqxZvf7663r66afl5+dnaD0cp4pX1qymT5+u/fv36/Dhw3rmmWc0cOBAQ64kqoysqoKy5HTmzBnl5uZqw4YN+vTTT5WQkKADBw5Y35w4E/u/opX3fCItLU07d+7UwIEDDanHFXJyVWXNKjIyUkeOHLHeA6FVq1bq2bOn0+sJDg7WQw89pBo1aqhBgwYaNWpUoaxiY2Pl7++vevXqacKECdasPvzwQw0ZMkRt27aVh4eHBg0apBo1aui7774rtJyIiIhiP2FftmyZ9u/frxdeeMHp6+eIsmR1zz33qEGDBnr77beVm5urL7/8Ujt37jSk8VqV30+Vq/Fw/vx5mc1mmc1m7dmzRz179lS7du2UnJyswMBAvffee5JuXBb+x5Obixcvqnbt2kX+3t/f4CY9Pb08JWnv3r3q2bOnQkJCZDabtWTJEp09e7bQ778pICBAp0+fliSlp6frqaeesq5T+/bt5eHhoTNnzpS4zJvr8ft1vHjxYqFmS2Uqa1bl1aFDB2tW5d3JJycnq3///rr33nsVEBCg1157TefOnbN5TklZTZkyxbpOQUFBslgsOnXqVKnLrV27tk1Wly5dUu3atQ3p7pdXeXMqKCjQ888/L29vb82dO7fY31sVc3Jl9ownDw8PRUREKDMzUx988EGRv7cq5/T+++9r5cqVWr16tSGNFXuVNasDBw5ox44dGjlyZJl+L8cp5yvPuGrbtq3q1KmjGjVqaNCgQXrggQf073//u8jfW9WycnVlzalWrVqSbtxg12QyqX79+ho5cmSxObH/cy57jlOrVq1Shw4dSvwgqirn5KrKmlVBQYGefPJJPfbYY8rMzNTx48eVlZWlV199tcjf26dPH2tWq1evLldNZ86c0bPPPquwsDAFBATo+eefL1dWCxcutK6T2WxWRkaG9fGy2Lx5s2bMmKE1a9aofv365ardSGXNysvLS8uXL9dnn31mvZnzE088UexXzariuHLG+ynPMj9TN25+lJaWpnXr1ikhIUELFizQ4MGDFRsbq06dOlmfd/fddysvL0/Hjh1TSEiIJOngwYPFXjqTkZFRnjJsDBs2TLGxsVq7dq1q1qypyZMnF/rjZ2RkWD8ZPnnypEwmk6Qbobz77rvq0KFDod9b0k37fH19ZTKZdPDgQT300EOSbqxf06ZN7V4PZytrVuXlyPeuxo8fr/vvv19///vfVadOHb333nvauHGjzXMyMjKs28kfs5owYYL69etX7uU2bdpUBw8eVJs2bSRJSUlJLpNVeXKyWCwaPXq0zpw5ozVr1sjLy6vY31sVc3JljoynvLy8Yj+Zrao5LVu2TAsWLNDWrVsNv1qgvMqa1TfffKO0tDTrZYNXrlxRfn6+jhw5oq+//rrQ7+U45XyOjCs3N7diP2Wpalm5urLm5OvrKz8/P5uT0JJOSNn/OZc942nlypWl3tC0qubkysqa1fnz53Xy5EnFxsaqRo0aqlGjhgYPHqzXX3+9yBtn3vxU2x4zZsyQm5ubEhMTVa9ePW3evFkvvfSSzXN+v28tKqvivlpWms8//1xjxozR6tWr1axZM7vXwQjlGVfNmzfX1q1brf/ftWvXYq8mqorjyhnvp+y6ueTv7+R54MABtWrVyuZxHx8fPfbYY3rjjTd05coV7d69W59++qnD32fMzs62+bFYLLp8+bLq1aunmjVr6vvvvy9y0M2dO1dXr17V4cOHtXz5ckVHR0uShg4dqtdee816eclvv/2mLVu2lKmWAQMGaO7cucrKytLRo0e1dOlSDRo0yKH1M0JpWUk33hhlZ2crPz9f+fn5ys7Odnj6lpycHJusCgoKdPnyZdWpU0e1a9fW0aNH9Y9//KPQ695++21lZWXp5MmTWrx4sU1W8+fPt35X6sKFC/rkk0/KVMuAAQO0cOFCZWZm6tSpU1q4cKHLZVWWnMaPH6+jR49q5cqV1k+WHOVKORUUFCg7O1u5ubmyWCzKzs4udLOcylZaTr/++qvWrVuny5cvKz8/X1988YXWrVunjh07OrRcV8pp9erVeu2117R+/XqXvnFXaVkNGTJEP/zwgxISEpSQkKChQ4eqa9eu+uc//+nQcjlOlV9pWWVlZemLL76wHptWr16txMREPfLIIw4t15Wy+v2x9/f7QldSluPUoEGD9P777+vXX39VVlaWFi1apG7dujm0XPZ/5VOWnCRpz549OnXqlNNms3ClnKrC+YRUelb169dXYGCg/vGPfygvL09ZWVlasWKFw2/Or1+/bpNVfn6+Ll++LB8fH9WtW1eZmZlFzizx97//XRkZGTp//rzmzZtnzeqZZ57RkiVLtHfvXlksFl25ckWfffZZmb7at2PHDsXGxmrp0qXWN7SuqCzj6uDBg8rOztbVq1f1zjvv6PTp0w4fc11pXDnj/ZRDjYdz587Jw8NDvr6+hZ4zb948Xbt2Tffcc4+GDRumefPmOXSzkMzMTJlMJpuflJQUzZs3T2+88Yb8/f01Z84cPfHEE4Ve+1//9V9q3bq1evXqpRdeeEEPP/ywpBs3bOnRo4eio6Pl7++vRx55RN9//32Ry583b5769Olj/f8pU6aoSZMmatGihf785z/rhRdecPgkyAhlyWru3LkymUyaP3++Vq9eLZPJVOJl/GXh5+dnk9XXX3+t1157TWvXrpW/v7/GjBlTZFaPPvqoOnbsqD/96U/q2rWrnn76aUnSY489pjFjxui5555TQECAIiMjtW3btiKXvXr1aptPnIYOHaru3bsrMjJSERER6tq1q4YOHerQ+jlbaTmlpaVpyZIlSkpKUmhoqN2X0v2RK+W0c+dOmUwm9e3b19qdLWrZlam0nNzc3PTBBx/ovvvuU1BQkP76178qPj5ejz76qEPLdaWcZs6cqXPnzunhhx+2bofjxo1zaP2MUFpWt912mxo2bGj98fHxUc2aNdWgQQO7l8lxyj6lZZWXl6eZM2fq7rvvVnBwsN5//30tX77c5gbW5eVqWa1cuVImk0njx4/Xrl27ZDKZ9OKLL9q9fkYoy/nESy+9pNatW6tNmzZq3769WrRoYfenoDex/yufsuQk3bipZM+ePZ329StXyqkqnE9IZctq2bJl+vzzzxUSEqLWrVvLy8tLb7zxhkPL7dChg01Wy5cv16RJk/Tjjz/KbDarX79+Rd5Hok+fPoqOjlbLli0VFBRkHdvh4eH629/+pri4OAUGBqp169bFznaQmJhoc6XQ3LlzdfHiRfXr1886pn6/b3QVZclq1apVCg0N1T333KMdO3bok08+cfirWK40rpzxfsotKyvLNe6wBwAAAAAAqh27rngAAAAAAAAoCxoPAAAAAADAMDQeAAAAAACAYWg8AAAAAAAAw9B4AAAAAAAAhqHxAAAAAAAADEPjAQAAAAAAGMbT3hf6+vo6s45qISsrq7JLKIScCnPFnKSqkZXFYin2MTc3N6cvzxWzcnZOFf03NYIr5iRVjTFV0Vwxq4rMqaqMN1fMSXKdMeVKObpiVq6SkytxxZwksiqKK2ZV3pxK2kdJrnW8sZc9OXHFAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDA0HgAAAAAAgGHsvrkkgOrJlW7adSso6W9KFkD5MW6qP/abAFD1cMUDAAAAAAAwDI0HAAAAAABgGBoPAAAAAADAMDQeAAAAAACAYWg8AAAAAAAAw9B4AAAAAAAAhmE6TeAWxHRjVQNTxgGFlbTtS2z/tzp795ulvRZVQ3EZX7hwoYIrAfBHXPEAAAAAAAAMQ+MBAAAAAAAYhsYDAAAAAAAwDI0HAAAAAABgGBoPAAAAAADAMDQeAAAAAACAYWg8AAAAAAAAw3hWdgEAjFHSfOXMVV712TtXPdmjKmAbhhFK23bY7qoGe3LKysoyqhyHFLUubGuorrjiAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDA0HgAAAAAAgGFoPAAAAAAAAMMwnSZQRZU0nZTEdEy3Mnun2izttYARmE4OrsKefeeFCxeMKueWdSsdp4paF6Z1RXXFFQ8AAAAAAMAwNB4AAAAAAIBhaDwAAAAAAADD0HgAAAAAAACGofEAAAAAAAAMQ+MBAAAAAAAYhuk0ARfGlEpwttK2G7Y5VDS2K1QFxW2nWVlZFVxJ9cCxpnhMiY3qiiseAAAAAACAYWg8AAAAAAAAw9B4AAAAAAAAhqHxAAAAAAAADEPjAQAAAAAAGIbGAwAAAAAAMIzd02mWNp3LrejChQuVXQKqsKLGFNMioaLZM40X+z4AwB8xZabzMSU2qjKueAAAAAAAAIah8QAAAAAAAAxD4wEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwdk+nyZQshWVlZVV2CajCGFNwdcVto66672Pa58KY+hSAMzF9o2uxZ0rs0l4HOAtXPAAAAAAAAMPQeAAAAAAAAIah8QAAAAAAAAxD4wEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwnpVdAIzFPPaFMY89cGtgXvLCsrKyKrsEAFVMSeeS7GerjpKyImNUBK54AAAAAAAAhqHxAAAAAAAADEPjAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDBMp1nNMQVOYUwnBwAAcENpU69zLln9MdUmKgJXPAAAAAAAAMPQeAAAAAAAAIah8QAAAAAAAAxD4wEAAAAAABiGxgMAAAAAADCMW1ZWVsm3sgUAAAAAALATVzwAAAAAAADD0HgAAAAAAACGofEAAAAAAAAMQ+MBAAAAAAAYhsYDAAAAAAAwDI0HAAAAAABgGBoPAAAAAADAMDQeAAAAAACAYWg8AAAAAAAAw9B4AAAAAAAAhqHxAAAAAAAADEPjAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDC3VOMhPj5ew4cPr/DXovyWL1+u7t27V/hrUT7kVHWQVdXAcarqIKuqgX1f1UFWVUNCQoLuu+++Cn8tys/VxlS5Gg8PP/ywkpOTlZqaqgcffNDmsffff1+dOnXSXXfdpREjRhR67Y4dO9SuXTs1atRIPXv2VFpamt1FjxgxQjNnzrT79UYYM2aM2rZtq3r16mn58uWVXY7dWV2/fl0xMTFq0aKFfH19lZCQ4FAdrnhydeLECfXs2VONGjVSu3bttH379kqrxd6cvvvuO/Xu3VtBQUEKCQnRM888o9OnT9tdh6vl9Ouvv+q5555T06ZNZTab1a1bN+3du7dSa7I3qyNHjqhTp04KDAxUYGCgevXqpSNHjthdh6tlJUkzZ85UZGSk6tevr/j4+EqtxZHj1E2zZ8+Wr6+vQ/sGVztOJScna+DAgQoJCVFQUJCio6P1888/V2pN9mZ14sQJ+fr6ys/Pz/ozZ84cu+sgq5I5MqauXr2qCRMmKDg4WGazWT169LC7Dlfb91Wn49Tq1attxlOjRo3k6+ur/fv321WHq2UlVY9zP0lav3692rdvL39/fz3wwAPavHmz3XW4WrMmJydHo0ePVvPmzeXv76+oqCht27atUmtyJKulS5cqPDxcfn5+evLJJ3Xq1Cm766guY6rMjYfc3Fylp6crJCRE+/fvV8uWLW0eN5lMmjhxop566qlCrz179qyefvppTZ06VSkpKQoPD9ezzz5b1kVXCc2bN9e8efMK/V0qgyNZSVKHDh30/vvvq2HDhhVRboUbNmyY7r//fh0/flyvvPKKYmJi9Ntvv1V4HY7klJWVpSFDhujAgQNKSkpS7dq1NWrUqIoq3XBXrlxReHi4tm/frpSUFA0cOFD9+vXT5cuXK6UeR7IymUz68MMPlZqaquPHj6tHjx7Vbv8XHBys6dOnq2vXrpVah6P7PklKSUnRhg0bZDKZjC63Ql24cEE9evTQ3r179fPPP6t169YaNGhQpdXjjKxOnDihjIwMZWRk6KWXXjK65ArjSlk5mtPYsWN1/vx5ffvtt0pJSan0xqQzVafjVL9+/axjKSMjQ2+++aaCgoJc4pzWWarDuV9mZqaGDx+u119/Xenp6ZoxY4ZiY2P166+/VlT5hsrLy5Ofn5+2bNmitLQ0vfLKKxo6dKhOnDhRKfU4klVCQoJmzJihjz/+WCkpKQoMDNRzzz1XUaVXCHvGVJkbD4cOHVJoaKjc3Nz0ww8/FPrjP/744+rZs6fuuOOOQq/dtGmTmjZtqt69e6tmzZqaPHmyDh48qKNHj5Z18WU2adIkNWvWTAEBAerYsaMSExNtHs/OztbQoUPl7++vBx98UElJSdbHTp06paefflohISG6//77tXjx4jIvNzY2Vh07dlTNmjWdti72ciQrb29vjRw5UhEREfLw8DC0zvnz56tVq1bWru2mTZtsHrdYLIqLi5PZbFa7du20Y8cO62MXLlzQ6NGjFRoaqrCwMM2cOVP5+fmlLjM5OVk//vijpkyZolq1aqlXr15q1qyZNm7c6PT1K40jOXXp0kW9e/dW3bp1ddtttyk2NlZ79uwxpM7KyCkoKEijR4+WyWSSh4eHhgwZotzcXCUnJzt9/crCkax8fX0VGBgoNzc3WSwWeXh4KCUlxZA6KyMrSRo0aJC6dOmiOnXqOHV9ysuRnG6aOHGipk2bJi8vL8PqrIzjVJs2bRQTE6N69erJy8tLo0aN0s8//6xz5845dd3KyhlZVYRbPStHcjp69Kg+/fRTLViwQA0aNJCHh4datWplSJ0cp5w7plasWKEBAwbIzc3N6XVy7md/TpmZmbr99tvVpUsXubm5qVu3brrtttsMOaf46KOPrFdWtGzZUkuWLCn0nHnz5ik4OFgtWrTQ6tWrrf+ek5OjV155Rc2bN9c999yjcePG6dq1a6Uu08fHR1OmTFFgYKDc3d3VvXt3mc1mu6+8cZQjWX322Wfq3bu3wsLC5O3trbi4OCUmJhqSVVUaU6U2Hj766COZzWZ1795d3333ncxms959911NmzZNZrNZqamppRZ3+PBhNW/e3Pr/Pj4+atKkiQ4fPlzqa8urdevWSkhIUEpKivr06aMhQ4YoOzvb+vjWrVvVu3dvpaSkqG/fvho8eLByc3NVUFCgAQMGqHnz5jp8+LA2btyoRYsW6YsvvihyOZGRkVqzZo3T63eEM7KqSE2aNNGnn36qtLQ0TZo0Sc8//7zN1wX27t2roKAgHTt2TFOmTNHTTz+t8+fPS5JGjhwpT09P7du3T19//bW+/PJLLV26tMjl9O/fX/Pnz5d0Y1sMCgqyeYN0M/OKYkROiYmJatq0qfOLVeXk9EcHDhzQ9evX1aRJE+evYAmcmZXZbFbDhg310ksvafz48YbU6wpZVQZn5fTJJ5/I29vb8Cs3XOE4tXPnTjVs2LDC39g7c0y1aNFC9913n0aOHKmzZ88aUu+tmpUzcvr+++8VEBCg+Ph4BQcHKzIyUhs2bDCkXlfY91WH45QkpaWlKTExUQMGDDCkXs797M8pPDxc9957r7Zu3ar8/Hxt3rxZNWrUULNmzZxe75133qlVq1YpPT1dCxcu1Msvv2zTAPjll1909uxZHT58WIsWLdLYsWOtXwmbNm2akpOTlZCQoH379ikzM7PYr8NNmDBBE+wqmksAACAASURBVCZMKPKxM2fO6NixYwoLC3P6+pXEWWPKYrEU+u9Dhw45vd6qNKZKbTw89dRTSktLU6tWrbRt2zbt3LlTYWFhSk9PV1pamoKCgkr9g1y5ckV169a1+be6desacjla//79dccdd8jT01MvvPCCcnJybL4b2apVK/Xq1cv6KUJOTo6+++477du3T2fPntWkSZPk7e2toKAgPfPMM1q3bl2Ry0lMTFTfvn2dXr8jnJFVRerdu7caNWokd3d3RUdHKzg4WN9//7318TvvvFMjR46Ul5eXoqOjdffdd+uzzz7TmTNntG3bNsXHx8vHx8f6vOKyWrVqlcaNGyepYrfF4jg7p4MHD2rOnDmaMWOGIfVWRk6/d/HiRf3lL3/RpEmTdPvttxuyjsVxZlZpaWlKS0vT3Llzdf/99xtSb2VnVVmckdOlS5c0Y8YMzZo1y/B6K/s4lZGRobi4OL3++uuGrWNxnJFV/fr19dVXXykpKUnbt2/X5cuXFRsba0i9t2pWzsgpMzNThw4dUt26dXXkyBHNmTNHI0eO1E8//eT0eit731ddjlOStHLlSkVERBh2zsi5n/05eXh4aMCAAYqNjdVdd92l2NhYzZ8/Xz4+Pk6vt1u3bmrSpInc3NwUFRWlhx56SLt27bJ5ztSpU1WjRg1FRUWpa9euWr9+vSwWiz788EPFx8erXr16qlOnjiZMmFBsTvPmzdO8efMK/Xtubq5iY2M1cOBA3XvvvU5fv5I4I6tHHnlE69ev18GDB3Xt2jXNmTNHbm5uZbryo7yq0pjyLOnB8+fPWy8ruXz5snr27Knr169LkgIDAzV58mSNHDmylD/HjSscLl26ZPNvly5dUu3atQs9d/Xq1daVioiI0Nq1a0v9/b/3zjvvaNmyZdZOz6VLl2wuT/Tz87P+t7u7uxo3bqzTp0/Lzc1Np06dktlstj5eUFCgiIiIci2/sjgrq/L4/YlSQECAdu/eXa7Xr1ixQgsXLrTeaPTKlSs2n1o1atTI5jK/gIAAnT59Wunp6crNzVVoaKj1MYvFYpNtcYraFi9evFjktmgEZ+d0/Phx9e3bV7NmzVJkZGSRz6mKOd107do1DRgwQG3btjXsKoHiGDGmfHx89OyzzyokJETffvut7rzzTpvHq3JWlcVZOc2aNUv9+/dXYGBgqc+tysep3377TdHR0XruuefUp0+fctXtKGdlVbt2bYWHh0uS7rrrLs2dO1ehoaG6dOlSoa/7kFX5OSunmjVrysvLS3FxcfL09FRUVJSioqL05Zdf2uxrpKq976tux6mVK1eWuB5VMavqcu63fft2vfrqq9q8ebNatmyp/fv3a+DAgVqzZk2hDzTS09PVoUMH6/9nZGSUq+Zt27Zp9uzZSk5OVkFBga5du2YzG4Wvr69Nw+NmTr/99puuXr2qjh072vy+sn51U7qxr3z++efl7e2tuXPnlqtuRzkrq06dOmnKlCmKiYnRpUuXNGLECNWpU0eNGzcu9NxbaUyV2HioV6+e0tLStG7dOiUkJGjBggUaPHiwYmNj1alTp1KLuiksLEwrVqyw/v+VK1eUkpJS5KUz/fr1U79+/cr8u38vMTFRf/vb37RhwwaFhYXJ3d1dgYGBNpe6/H7gFRQUKDMzUyaTSZ6engoMDNS+ffvsWnZlc1ZW5REZGVnuHdlNaWlpGjNmjDZs2KD27dvLw8NDUVFRNs85deqULBaLdbCcPHlSPXr0kJ+fn2rUqKHjx4/L07PETbiQsLAwpaam2pygHjx4sMKuXnFmTmlpaerVq5fi4uJKvCSyKuYk3fiO4ODBg+Xn56cFCxbYVb8jjBpTNw/gmZmZhRoPVTWryuSsnHbs2KHMzEx98MEHkm686RsyZIjGjh2rsWPH2jy3qh6nsrKy9MQTT6hHjx6aOHGiXb/DEUaNqZvbc0FBQaHHyKr8nJXT779ie1Nx9wyoqvu+6nac2r17t06fPq1evXoV+5yqmFV1OfdLSkpSZGSktfHaunVrtWnTRjt27CjUeAgICLA7p5ycHMXExGjx4sV69NFH5eXlVegGt1lZWbpy5Yq1+XDy5EmFhYWpfv36qlWrlnbv3l3km+zSWCwWjR49WmfOnNGaNWsMvd9SUZw5pmJjY61X4yUnJ+vNN98scirRW2lMlenmkr+/k+eBAweKvDlQXl6esrOzlZ+fr/z8fGVnZysvL0+S1LNnTx0+fFgbNmxQdna25syZo2bNmjl06czNZdz8uX79ui5fvixPT081aNBAeXl5mj17dqFuzP79+7Vx40bl5eXpvffek7e3t9q1a6c2bdqodu3aWrBgga5du6b8/HwdOnSozCcN169fV3Z2tiwWi/VvUdRJkNEczUq6scO5+R3W3Nxc63rZq6CgwCarnJwcXb16VW5ubmrQoIGkG9+n+uP3gn799VctXrxYubm5+uSTT3T06FF17dpVJpNJDz30kKZOnaqLFy+qoKBAKSkp+uabb0qt5e6771aLFi00e/ZsZWdna9OmTfrPf/6jxx9/3O71s4ejOWVmZurxxx/X8OHDnTZDgivllJubq5iYGNWsWVOLFi2Su3u5Zv51Kkez+uqrr/Tjjz8qPz9fFy9e1MsvvyxfX99Cn/iVhytlJf3//URBQYF1/cvz6YYzOJrTxo0btWvXLiUkJCghIUGNGjXSggULNGzYMLtrcqXj1MWLFxUdHa0OHTpo2rRpdq+TMzia1c3ZHgoKCnTu3DlNmjRJUVFRDl3iTlaFOZpTZGSk/P399dZbbykvL0+7d+/WN998o86dO9tdkyvt+6rTceqmFStW6LHHHnPKjYJdKavqcu4XHh6uXbt26cCBA5KkH3/8Ubt27XLoHg8Wi8Ump5v7v5ycHNWvX1+enp7atm2bvvrqq0KvjY+P1/Xr15WYmGi9maK7u7tiYmL08ssvW2fbyMzMLPb+Nn80fvx4HT16VCtXrlStWrXsXi9HOZpVdna2Dh06JIvFovT0dI0ZM0Z/+ctf5Ovra3dN1WFMlavxcO7cOXl4eBT5R5s7d65MJpPmz5+v1atXy2QyWS+PadCggZYuXaqZM2cqKChIe/futX6qZK/58+fLZDJZfx5//HF17txZnTt3Vtu2bdWiRQvVrFmz0OUijz76qNavX6+goCCtWrVKy5Ytk5eXlzw8PLRq1SolJSWpZcuWCg4O1osvvqiLFy8WufwOHTrY3MH1iSeekMlk0p49ezRmzBiZTCbt3LnToXW0h6NZSVLbtm1lMpmUmZmp6OhomUwm6+U79li7dq1NVuHh4WratKlGjx6tLl266J577tGhQ4f0wAMP2Lyubdu2On78uEJCQvTaa6/pww8/tN5c6+YA6tChg4KCghQTE6NffvmlyOX36dPH5vtjH3zwgX744QcFBQVp+vTpWrp0qXXAVhRHc1q6dKlSU1M1a9Ysm7m3HeFKOe3Zs0efffaZvvrqKwUGBlrX7493lK8IjmZ14cIFDRs2TGazWeHh4UpNTdXatWsdmgHHlbKSpBdffFEmk0lr167Vm2++KZPJpJUrV9q9fvZwNKc77rhDDRs2tP64u7vL19fXoUtxXek4tXnzZu3bt0/Lly+32Wekp6fbvX72cjSr1NRUPfnkk/L391dERIS8vb2r1TmFq2TlaE5eXl76+OOPtW3bNpnNZo0ZM0aLFi1y6EMnV9r3VafjlHTjjdL69eudNnWrK2UlVY9zv6ioKE2ePFnPPPOM/P39FRMTo/Hjx+vhhx+2u6Y9e/bY5GQymVSrVi3Nnj1bQ4cOVWBgoNasWaMePXrYvK5hw4by9fVV06ZNNXz4cL311lvWsT19+nQFBwfrkUceUUBAgHr37m1zb5zfGzdunPXrcGlpaVqyZImSkpIUGhpqHVO/f79VURzNKjs7W8OGDZOfn586d+6s9u3ba+rUqQ7VVB3GlFtWVpb9H2UDAAAAAACUoPKuCwMAAAAAANUejQcAAAAAAGAYGg8AAAAAAMAwNB4AAAAAAIBhaDwAAAAAAADD0HgAAAAAAACGofEAAAAAAAAM42nvC319fZ1ZR7WQlZVV2SUUQk6FuWJOElkVxRWzIqfCXDEniayK4opZVfWcLBZLiY+7ubmV+3e6Yk5S1ciqpDzsyaI0rpiVs3Oq6L+pEVwxJ4msiuKKWd1+++2F/q2q/D2NYk9OXPEAAAAAAAAMQ+MBAAAAAAAYhsYDAAAAAAAwDI0HAAAAAABgGLtvLgkAuLUUd9OqCxcuVHAlZVNUvbf6zaDgfKVtU9XhZm9VSUl/UyNuBAq4Enu3f7Z9VASueAAAAAAAAIah8QAAAAAAAAxD4wEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwTKcJALCyZ7qtrKwso8pxSFH1Mp0YKpo909u56hS1VR1Tn+JWxlSbqGxc8QAAAAAAAAxD4wEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwNB4AAAAAAIBhaDwAAAAAAADDeFZ2AQCAilPSXN1S9Z+v2955zEt7LWCP4raprKysCq4Ekn37hwsXLhhVDlBh7D02clxEeXDFAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDA0HgAAAAAAgGFoPAAAAAAAAMPQeAAAAAAAAIZhOk0AqGaY+so+pf1t+LsCty6mPsWtimmo4Sxc8QAAAAAAAAxD4wEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwNB4AAAAAAIBhmE6zmittmptb0YULFyq7BMBhTO1Y8eydUow8AADVEdNQozy44gEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwNB4AAAAAAIBhaDwAAAAAAADDMJ1mNcdUNYVlZWVVdglAmTANVdXBVJsAANiy59jItPfVF1c8AAAAAAAAw9B4AAAAAAAAhqHxAAAAAAAADEPjAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBim0wSASsRUi9UfU20CAGCruGMc095XX1zxAAAAAAAADEPjAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDA0HgAAAAAAgGFoPAAAAAAAAMN4VnYBVVFx865fuHChgitBdVLUdlXcHMeoOorbX9xExre2kvJn2wEAANUFVzwAAAAAAADD0HgAAAAAAACGofEAAAAAAAAMQ+MBAAAAAAAYhsYDAAAAAAAwDI0HAAAAAABgGKbTLEZJ05gVN4VZVlaWUeXgFlDUdmXPdoiKR04wQmnbDtsdAACoKrjiAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDA0HgAAAAAAgGHcsrKyir8tNgAAAAAAgAO44gEAAAAAABiGxgMAAAAAADAMjQcAAAAAAGAYGg8AAAAAAMAwNB4AAAAAAIBhaDwAAAAAAADD0HgAAAAAAACGofEAAAAAAAAMQ+MBAAAAAAAYhsYDAAAAAAAwDI0HAAAAAABgGBoPAAAAAADAMDQeAAAAAACAYWg8AAAAAAAAw9B4AAAAAAAAhrmlGg/x8fEaPnx4hb8W5bd8+XJ17969wl+L8iGnqoOsqgaOU1UHWVUN7PuqDrKqGhISEnTfffdV+GtRfq42psrVeHj44YeVnJys1NRUPfjgg9Z/z8nJ0ejRo9W8eXP5+/srKipK27Zts3ntjh071K5dOzVq1Eg9e/ZUWlqa3UWPGDFCM2fOtPv1RhgzZozatm2revXqafny5ZVdjt1ZXb9+XTExMWrRooV8fX2VkJDgUB2ueHJ14sQJ9ezZU40aNVK7du20ffv2SqvF3py+++479e7dW0FBQQoJCdEzzzyj06dP210HOZXO3qyOHDmiTp06KTAwUIGBgerVq5eOHDlidx1kVTJHjlM3zZ49W76+vg6tB8ep0tmb1YkTJ+Tr6ys/Pz/rz5w5c+yuw9WySk5O1sCBAxUSEqKgoCBFR0fr559/rrR6HBlTV69e1YQJExQcHCyz2awePXrYXYcr7vtmzpypyMhI1a9fX/Hx8ZVdjt1ZrV692mY8NWrUSL6+vtq/f79ddbhiVtXlOLV+/Xq1b99e/v7+euCBB7R582a763C1Zk15jtMVxZGsli5dqvDwcPn5+enJJ5/UqVOn7K6juoypMjcecnNzlZ6erpCQEO3fv18tW7a0PpaXlyc/Pz9t2bJFaWlpeuWVVzR06FCdOHFCknT27Fk9/fTTmjp1qlJSUhQeHq5nn322/Gvowpo3b6558+bZ/F0qiyNZSVKHDh30/vvvq2HDhpVRvuGGDRum+++/X8ePH9crr7yimJgY/fbbbxVehyM5ZWVlaciQITpw4ICSkpJUu3ZtjRo1qsLXwUiukpPkWFYmk0kffvihUlNTdfz4cfXo0aPa7f9cJStH932SlJKSog0bNshkMlV0+YarTscp6cZJT0ZGhjIyMvTSSy9V9CoY5sKFC+rRo4f27t2rn3/+Wa1bt9agQYMqpRZHcxo7dqzOnz+vb7/9VikpKS7x5tyZgoODNX36dHXt2rWyS3Eoq379+lnHUkZGht58800FBQW5xL7CWarDcSozM1PDhw/X66+/rvT0dM2YMUOxsbH69ddfK3w9jFDWfX9FcSSrhIQEzZgxQx9//LFSUlIUGBio5557rlLWwyj2jKkyNx4OHTqk0NBQubm56YcffrD54/v4+GjKlCkKDAyUu7u7unfvLrPZbO2Ubtq0SU2bNlXv3r1Vs2ZNTZ48WQcPHtTRo0ftXNXiTZo0Sc2aNVNAQIA6duyoxMREm8ezs7M1dOhQ+fv768EHH1RSUpL1sVOnTunpp59WSEiI7r//fi1evLjMy42NjVXHjh1Vs2ZNp62LvRzJytvbWyNHjlRERIQ8PDwMrXP+/Plq1aqVtWu7adMmm8ctFovi4uJkNpvVrl077dixw/rYhQsXNHr0aIWGhiosLEwzZ85Ufn5+qctMTk7Wjz/+qClTpqhWrVrq1auXmjVrpo0bNzp9/UrjSE5dunRR7969VbduXd12222KjY3Vnj17DKnzVs9JciwrX19fBQYGys3NTRaLRR4eHkpJSTGkzls9K0dyumnixImaNm2avLy8DKuT45RzsqoIlZFVmzZtFBMTo3r16snLy0ujRo3Szz//rHPnzjl13crCkZyOHj2qTz/9VAsWLFCDBg3k4eGhVq1aGVJnZez7JGnQoEHq0qWL6tSp49T1sYczx9SKFSs0YMAAubm5Ob1OjlP255SZmanbb79dXbp0kZubm7p166bbbrvNkHOKjz76yHplRcuWLbVkyZJCz5k3b56Cg4PVokULrV692vrvOTk5euWVV9S8eXPdc889GjdunK5du1bqMl1p3y85ltVnn32m3r17KywsTN7e3oqLi1NiYqIhWVWlMVVq4+Gjjz6S2WxW9+7d9d1338lsNuvdd9/VtGnTZDablZqaWug1Z86c0bFjxxQWFiZJOnz4sJo3b2593MfHR02aNNHhw4dLXbHyat26tRISEpSSkqI+ffpoyJAhys7Otj6+detW9e7dWykpKerbt68GDx6s3NxcFRQUaMCAAWrevLkOHz6sjRs3atGiRfriiy+KXE5kZKTWrFnj9Pod4YysKlKTJk306aefKi0tTZMmTdLzzz9v83WBvXv3KigoSMeOHdOUKVP09NNP6/z585KkkSNHytPTU/v27dPXX3+tL7/8UkuXLi1yOf3799f8+fMl3dgWg4KCbE4SbmZeUYzIKTExUU2bNjWk3ls1J8m5WZnNZjVs2FAvvfSSxo8fb0i9t2pWzsrpk08+kbe3t+GfXnKccs6YatGihe677z6NHDlSZ8+eNaReV8hq586datiwoe644w5D1rEozsjp+++/V0BAgOLj4xUcHKzIyEht2LDBkHorY9/nKpx9TpGWlqbExEQNGDDAkHo5TtmfU3h4uO69915t3bpV+fn52rx5s2rUqKFmzZo5vd4777xTq1atUnp6uhYuXKiXX37ZpgHwyy+/6OzZszp8+LAWLVqksWPHWr8SNm3aNCUnJyshIUH79u1TZmZmsV+HmzBhgiZMmFDkY5X1HsVZY8pisRT670OHDjm93qo0pkptPDz11FNKS0tTq1attG3bNu3cuVNhYWFKT09XWlqagoKCbJ6fm5ur2NhYDRw4UPfee68k6cqVK6pbt67N8+rWravLly+Xtvhy69+/v+644w55enrqhRdeUE5Ojs13I1u1aqVevXpZP0XIycnRd999p3379uns2bOaNGmSvL29FRQUpGeeeUbr1q0rcjmJiYnq27ev0+t3hDOyqki9e/dWo0aN5O7urujoaAUHB+v777+3Pn7nnXdq5MiR8vLyUnR0tO6++2599tlnOnPmjLZt26b4+Hj5+PhYn1dcVqtWrdK4ceMkVey2WBxn53Tw4EHNmTNHM2bMMKTeWzUnyblZpaWlKS0tTXPnztX9999vSL23albOyOnSpUuaMWOGZs2aZXi9HKccy6p+/fr66quvlJSUpO3bt+vy5cuKjY01pN7KziojI0NxcXF6/fXXDVm/4jgjp8zMTB06dEh169bVkSNHNGfOHI0cOVI//fST0+utjH2fq3D2OcXKlSsVERFR6HXOwnHK/pw8PDw0YMAAxcbG6q677lJsbKzmz58vHx8fp9fbrVs3NWnSRG5uboqKitJDDz2kXbt22Txn6tSpqlGjhqKiotS1a1etX79eFotFH374oeLj41WvXj3VqVNHEyZMKDanefPmad68eYX+vTLfozgjq0ceeUTr16/XwYMHde3aNc2ZM0dubm5luvKjvKrSmPIs6cHz589bLyu5fPmyevbsqevXr0uSAgMDNXnyZI0cOdL6/IKCAj3//PPy9vbW3Llzrf/u4+OjS5cu2fzuS5cuqXbt2oWWuXr1autKRUREaO3atSWuwB+98847WrZsmbXTc+nSJZvLE/38/Kz/7e7ursaNG+v06dNyc3PTqVOnZDabbdYnIiKiXMuvLM7Kqjx+f6IUEBCg3bt3l+v1K1as0MKFC603Gr1y5YrNp1aNGjWyucwvICBAp0+fVnp6unJzcxUaGmp9zGKx2GRbnKK2xYsXLxa5LRrB2TkdP35cffv21axZsxQZGVnkMsnJPkaMKR8fHz377LMKCQnRt99+qzvvvNPmcbIqP2flNGvWLPXv31+BgYGlLpPjlH2clVXt2rUVHh4uSbrrrrs0d+5chYaG6tKlS4Uuea/KWf3222+Kjo7Wc889pz59+pSrbkc4K6eaNWvKy8tLcXFx8vT0VFRUlKKiovTll1/a7GukqrnvcwVGHKdWrlxZ4lV5VTGr6nKc2r59u1599VVt3rxZLVu21P79+zVw4ECtWbOm0Aca6enp6tChg/X/MzIyylXztm3bNHv2bCUnJ6ugoEDXrl2zmY3C19fXpuFxM6fffvtNV69eVceOHW1+X1m/viQ55z2KvZyVVadOnTRlyhTFxMTo0qVLGjFihOrUqaPGjRsXWuatNKZKbDzUq1dPaWlpWrdunRISErRgwQINHjxYsbGx6tSpk81zLRaLRo8erTNnzmjNmjU2348NCwvTihUrrP9/5coVpaSkFHnpTL9+/dSvX78Siy5OYmKi/va3v2nDhg0KCwuTu7u7AgMDbS51+f3AKygoUGZmpkwmkzw9PRUYGKh9+/bZtezK5qysyiMyMrLcO7Kb0tLSNGbMGG3YsEHt27eXh4eHoqKibJ5z6tQpWSwW62A5efKkevToIT8/P9WoUUPHjx+Xp2eJm3AhYWFhSk1NtTlBPXjwYIV9KujMnNLS0tSrVy/FxcWVeEkkOdnHqDF18wCemZlZqPFAVuXnrJx27NihzMxMffDBB5JuvOkbMmSIxo4dq7Fjx9r8Ho5T9jFqTN3cngsKCgo9VlWzysrK0hNPPKEePXpo4sSJdv0Oezkrp99/xfam4u4ZUBX3fa7A2WNq9+7dOn36tHr16lXsMqtiVtXlOJWUlKTIyEhr47V169Zq06aNduzYUajxEBAQYHdOOTk5iomJ0eLFi/Xoo4/Ky8ur0A1us7KydOXKFWvz4eTJkwoLC1P9+vVVq1Yt7d69u8g32aVx1nsUezlzTMXGxlqvxktOTtabb75Z5FSit9KYKtPNJX9/J88DBw4UeXOg8ePH6+jRo1q5cqVq1apl81jPnj11+PBhbdiwQdnZ2ZozZ46aNWvm0KUz+fn5ys7Otv5cv35dly9flqenpxo0aKC8vDzNnj27UDdm//792rhxo/Ly8vTee+/J29tb7dq1U5s2bVS7dm0tWLBA165dU35+vg4dOlTmk4br168rOztbFotFeXl5ys7OLvIkyGiOZiXd2OHc/A5rbm6udb3sVVBQYJNVTk6Orl69Kjc3NzVo0EDSje9T/fF7Qb/++qsWL16s3NxcffLJJzp69Ki6du0qk8mkhx56SFOnTtXFixdVUFCglJQUffPNN6XWcvfdd6tFixaaPXu2srOztWnTJv3nP//R448/bvf62cPRnDIzM/X4449r+PDhTpshgZyK5mhWX331lX788Ufl5+fr4sWLevnll+Xr61voE7/yIKvCHM1p48aN2rVrlxISEpSQkKBGjRppwYIFGjZsmN01cZwqmqNZ3ZztoaCgQOfOndOkSZMUFRWl22+/3e6aXCmrixcvKjo6Wh06dNC0adPsXidHOZpTZGSk/P399dZbbykvL0+7d+/WN998o86dO9tdkyvt+6T/f45UUFBg3YbK88muszjj3E+68cnpY4895pSbZbpSVtXlOBUeHq5du3bpwIEDkqQff/xRu3btcugeDxaLxSanm/u/nJwc1a9fX56entq2bZu++uqrQq+Nj4/X9evXlZiYaL2Zoru7u2JiYvTyyy9bZ9vIzMws9v425Vn/iuRoVtnZ2Tp06JAsFovSl8ubzwAABiNJREFU09M1ZswY/eUvf5Gvr6/dNVWHMVWuxsO5c+fk4eFR6I+WlpamJUuWKCkpSaGhodZ5gG/e4bRBgwZaunSpZs6cqaCgIO3du9f6qZK95s+fL5PJZP15/PHH1blzZ3Xu3Flt27ZVixYtVLNmzUKXizz66KNav369goKCtGrVKi1btkxeXl7y8PDQqlWrlJSUpJYtWyo4OFgvvviiLl68WOTyO3ToYHMH1yeeeEImk0l79uzRmDFjZDKZtHPnTofW0R6OZiVJbdu2lclkUmZmpqKjo2UymayX79hj7dq1NlmFh4eradOmGj16tLp06aJ77rlHhw4d0gMPPGDzurZt2+r48eMKCQnRa6+9pg8//NB6c62bA6hDhw4KCgpSTEyMfvnllyKX36dPH5vvj33wwQf64YcfFBQUpOnTp2vp0qXWAVtRHM1p6dKlSk1N1axZs2zm3nYEORXN0awuXLigYcOGyWw2Kzw8XKmpqVq7dq1DMwuQVWGO5nTHHXeoYcOG1h93d3f5+vo6dCkux6miOZpVamqqnnzySfn7+ysiIkLe3t7V6pxi8+bN2rdvn5YvX26zf09PT3doHcvL0Zy8vLz08ccfa9u2bTKbzRozZowWLVrk0IdOrrbve/HFF2UymbR27Vq9+eabMplMWrlypd3rZy9nnPtlZ2dr/fr1Tpu61dWyqg7HqaioKE2ePFnPPPOM/P39FRMTo/Hjx+vhhx+2u6Y9e/bY5GQymVSrVi3Nnj1bQ4cOVWBgoNasWaMePXrYvK5hw4by9fVV06ZNNXz4cL311lvWsT19+nQFBwfrkUceUUBAgHr37m1zb5zfGzdunPXrcGXZTiuKo1llZ2dr2LBh8vPzU+fOndW+fXtNnTrVoZqqw5hyy8rKsv+jbAAAAAAAgBKU6YoHAAAAAAAAe9B4AAAAAAAAhqHxAAAAAAAADEPjAQAAAAAAGIbGAwAAAAAAMAyNBwAAAAAAYBgaDwAAAAAAwDCe9r7Q19fXmXW4HIvFUuxjbm5uRf57VlaWUeXYrbrnZA9XzEkiq6K4YlYVmZM9+6HK4Io5SdV/THGcqvqKy/DChQsVXEnZ3MpZFedWGFNV5VhUElfMSSKrorhiVlV931fSdiHZt23YkxNXPAAAAAAAAMPQeAAAAAD+X3t3lJowEAVQlIL7X2vcQb9aCppYotdMxnP+A0EcHlyGPAAywgMAAACQER4AAACAzO6PS55d8ZENAHgVc2p+Z/oIKDC+rbkww4cnOTc3HgAAAICM8AAAAABkhAcAAAAgIzwAAAAAGeEBAAAAyAgPAAAAQGbqdZrWxgAwMnMKgHewapOjufEAAAAAZIQHAAAAICM8AAAAABnhAQAAAMgIDwAAAEBGeAAAAAAywgMAAACQuRz9As+ydxaAkZlTAIxsaxaZYbyKGw8AAABARngAAAAAMsIDAAAAkBEeAAAAgIzwAAAAAGSEBwAAACBzinWa1rjwCbb+55/qer0e/QrwL+YUzG3tjJtTzG7vqs1Hz/J53HgAAAAAMsIDAAAAkBEeAAAAgIzwAAAAAGSEBwAAACAjPAAAAACZYdZpWkXWsKLx1qirr/zPby3LcvQrwC9zCua254ybU3yyR7PP3OQvNx4AAACAjPAAAAAAZIQHAAAAICM8AAAAABnhAQAAAMgIDwAAAEDmbes0H611tFKl4Xe9ZfUVcI85BXNzxuG9ts7U2nkcde09z3PjAQAAAMgIDwAAAEBGeAAAAAAywgMAAACQER4AAACAjPAAAAAAZF66TnNrTZEVRQAczZyCuTnjcA5r59Ha+3m58QAAAABkhAcAAAAgIzwAAAAAGeEBAAAAyAgPAAAAQEZ4AAAAADLCAwAAAJC57H3w3p5k+5EBGIU5BXO6d7Z/OOMAY3LjAQAAAMgIDwAAAEBGeAAAAAAywgMAAACQER4AAACAjPAAAAAAZHav07SuCICRmVNwXlZmAszFjQcAAAAgIzwAAAAAGeEBAAAAyAgPAAAAQEZ4AAAAADJfy7KsfzYYAAAA4AluPAAAAAAZ4QEAAADICA8AAABARngAAAAAMsIDAAAAkBEeAAAAgIzwAAAAAGS+ASy6qIVjUVuSAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preparation"
      ],
      "metadata": {
        "id": "rTucZ1-ESX1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our particular problem, flipping an image has the potential to ruin the label.\n",
        "\n",
        "So, we’re only keeping the min-max scaling by using the Normalize()\n",
        "transform. All the rest remains the same: splitting, datasets, sampler, and data loaders."
      ],
      "metadata": {
        "id": "GQ5OijgDSYgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformedDataset(Dataset):\n",
        "  def __init__(self, x, y, transform=None):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.x[index]\n",
        "\n",
        "    # transforms the features\n",
        "    if self.transform:\n",
        "      x = self.transform(x)\n",
        "\n",
        "    return x, self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ],
      "metadata": {
        "id": "cLU_wNwnTO40"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Builds tensors from numpy arrays BEFORE split\n",
        "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
        "x_tensor = torch.as_tensor(images / 255).float()\n",
        "y_tensor = torch.as_tensor(labels.reshape(-1, 1)).float()\n",
        "\n",
        "# Uses index_splitter to generate indices for training and validation sets\n",
        "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
        "# Uses indices to perform the split\n",
        "x_train_tensor = x_tensor[train_idx]\n",
        "y_train_tensor = y_tensor[train_idx]\n",
        "x_val_tensor = x_tensor[val_idx]\n",
        "y_val_tensor = y_tensor[val_idx]\n",
        "\n",
        "# Builds different composers because of data augmentation on training set\n",
        "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "\n",
        "# Uses custom dataset to apply composed transforms to each set\n",
        "train_dataset = TransformedDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
        "val_dataset = TransformedDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
        "\n",
        "# Builds a weighted random sampler to handle imbalanced classes\n",
        "weighted_sampler = make_balanced_sampler(y_train_tensor)\n",
        "\n",
        "# Uses sampler in the training set to get a balanced data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=weighted_sampler)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "XsWiPeOnTY5F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loss"
      ],
      "metadata": {
        "id": "r11hE-9NUoIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In binary classification problems, the model would produce one logit, and one logit\n",
        "only, for each data point.\n",
        "\n",
        "And we used a sigmoid function to map logits to probabilities. It was a\n",
        "simple world :-)\n",
        "\n",
        "But a multiclass classification is more complex:\n",
        "that is, we need to get log odds ratios for every possible class. \n",
        "\n",
        "In other words, we\n",
        "need as many logits as there are classes.\n",
        "\n",
        "The softmax function returns, for each class, the contribution that a given class\n",
        "had to the sum of odds ratios. The class with a higher odds ratio will have the\n",
        "largest contribution and thus the highest probability.\n",
        "\n",
        "Since the softmax is computed using odds ratios instead of log\n",
        "odds ratios (logits), we need to exponentiate the logits!\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\begin{array}\n",
        "& z & = \\text{logit}(p) & = \\text{log odds ratio }(p) & = \\text{log}\\left(\\frac{p}{1-p}\\right)\n",
        "\\\\\n",
        "e^z & = e^{\\text{logit}(p)} & = \\text{odds ratio }(p) & = \\left(\\frac{p}{1-p}\\right)\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The softmax formula itself is quite simple:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{c=0}^{N_c-1}{e^{z_c}}}\n",
        "$$\n",
        "\n",
        "In our example, we have three classes, so our model\n",
        "needs to output three logits $(z_0, z_1, z_2)$.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{softmax}(z) = \\left[\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\right]\n",
        "$$\n",
        "\n",
        "Simple, right? Let’s see it in code now."
      ],
      "metadata": {
        "id": "Ud9Cm5-_UpLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([1.3863, 0.0000, -0.6931])\n",
        "\n",
        "# We exponentiate the logits to get the corresponding odds ratios\n",
        "odds_ratios = torch.exp(logits)\n",
        "odds_ratios"
      ],
      "metadata": {
        "id": "BqBNi_jwZ8hQ",
        "outputId": "71075059-2275-4e6b-b923-8e5f5023701b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.0000, 1.0000, 0.5000])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we take these\n",
        "odds and add them together, and then compute each class' contribution to the\n",
        "sum:"
      ],
      "metadata": {
        "id": "1BGQLQUWaVhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmaxed = odds_ratios / odds_ratios.sum()\n",
        "softmaxed"
      ],
      "metadata": {
        "id": "xNNCa6oRaWMD",
        "outputId": "9fd3cf77-7dfc-4d63-8f91-b47c78c76b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voilà! Our logits were softmaxed: The probabilities are proportional to the odds\n",
        "ratios. \n",
        "\n",
        "This data point most likely belongs to the first class since it has a probability\n",
        "of `72.73%`.\n",
        "\n",
        "PyTorch\n",
        "provides the typical implementations: functional (`F.softmax()`) and module\n",
        "(`nn.Softmax`):"
      ],
      "metadata": {
        "id": "Q4oRHiE7ajB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Softmax(dim=-1)(logits)"
      ],
      "metadata": {
        "id": "IfxB53jkaqUc",
        "outputId": "cbc95f89-7da5-47b9-b0cc-e0417b92346f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "cbQrD9Pja1H8",
        "outputId": "5f52e2e7-1771-4266-abe9-8e250ad67270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, our models will produce logits with the shape (number of\n",
        "data points, number of classes), so the right dimension to apply softmax to is the\n",
        "last one (`dim=-1`)."
      ],
      "metadata": {
        "id": "fEor6RIGbCVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Negative Log Likelihood Loss"
      ],
      "metadata": {
        "id": "HaH_KJtAbSiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since softmax returns probabilities, logsoftmax returns log probabilities. And\n",
        "that’s the input for computing the negative log-likelihood loss, or `nn.NLLLoss()` for\n",
        "short. \n",
        "\n",
        "This loss is simply an extension of the binary cross-entropy loss for handling\n",
        "multiple classes.\n",
        "\n",
        "This was the formula for computing binary cross-entropy:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\texttt{BCE}(y)={-\\frac{1}{(N_{\\text{pos}}+N_{\\text{neg}})}\\Bigg[{\\sum_{i=1}^{N_{\\text{pos}}}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_{\\text{neg}}}{\\text{log}(1 - \\text{P}(y_i=1))}}\\Bigg]}\n",
        "$$\n",
        "\n",
        "In our example, there are three\n",
        "classes; that is, our `labels (y)` could be either zero, one, or two. \n",
        "\n",
        "So, the loss function\n",
        "will look like this:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+N_1+N_2)}\\Bigg[{\\sum_{i=1}^{N_0}{\\text{log}(\\text{P}(y_i=0))} + \\sum_{i=1}^{N_1}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_2}{\\text{log}(\\text{P}(y_i=2))}}\\Bigg]}\n",
        "$$\n",
        "\n",
        "The loss only considers the predicted probability for the true\n",
        "class.\n",
        "\n",
        "If a data point is labeled as belonging to class index two, the loss will consider the\n",
        "probability assigned to class index two only. The other probabilities will be\n",
        "completely ignored.\n",
        "\n",
        "For a total of C classes, the formula can be written like this:\n",
        "\n",
        "$$\n",
        "\\Large \\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+\\cdots+N_{C-1})}\\sum_{c=0}^{C-1}{\\sum_{i=1}^{N_c}{\\text{log}(\\text{P}(y_i=c))} }}\n",
        "$$\n",
        "\n",
        "Since the log probabilities are obtained by applying logsoftmax, this loss isn’t doing\n",
        "much more than looking up the inputs corresponding to the true class and adding\n",
        "them up."
      ],
      "metadata": {
        "id": "awfzVKepbXl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "log_probs"
      ],
      "metadata": {
        "id": "vSWFFddA4cXQ",
        "outputId": "eeb2b471-69d0-4582-b3ea-2bac41983646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3185, -1.7048, -2.3979])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the log probabilities for each class we computed using logsoftmax for\n",
        "our single data point. \n",
        "\n",
        "Now, let’s assume its label is two: What is the corresponding\n",
        "loss?"
      ],
      "metadata": {
        "id": "nGVn701U4qXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.tensor([2])\n",
        "F.nll_loss(log_probs.view(-1, 3), label)"
      ],
      "metadata": {
        "id": "QdhWtNv_4rmW",
        "outputId": "1fb5648f-dec2-4f2b-b5ca-dc673709e339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3979)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is the negative of the log probability corresponding to the class index (two) of the true label.\n",
        "\n",
        "Let’s go through some quick examples."
      ],
      "metadata": {
        "id": "KbCzoPye5FZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "\n",
        "dummy_logits = torch.randn((5, 3))\n",
        "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
        "dummy_log_probs = F.log_softmax(dummy_logits, dim=-1)\n",
        "dummy_log_probs"
      ],
      "metadata": {
        "id": "mR0zpBroCnpd",
        "outputId": "21fad723-19b6-4d13-8f49-3662433c4aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5229, -0.3146, -2.9600],\n",
              "        [-1.7934, -1.0044, -0.7607],\n",
              "        [-1.2513, -1.0136, -1.0471],\n",
              "        [-2.6799, -0.2219, -2.0367],\n",
              "        [-1.0728, -1.9098, -0.6737]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_log_probs = torch.tensor([-1.5229, -1.7934, -1.0136, -2.0367, -1.9098])\n",
        "- relevant_log_probs.mean()"
      ],
      "metadata": {
        "id": "yBvviQ6p5Ewv",
        "outputId": "46c4e648-0f01-4ba4-dc0f-36b6ef42862c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6553)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s use `nn.NLLLoss()` to create the actual loss function, and then use\n",
        "predictions and labels to check if we got the relevant log probabilities right:"
      ],
      "metadata": {
        "id": "A1obaVV0Dm_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss()\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ],
      "metadata": {
        "id": "edSASBOeDo78",
        "outputId": "29f231a3-be55-457f-9390-60e0097ef571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6553)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to balance our dataset, giving data points with label\n",
        "(y=2) double the weight of the other classes?"
      ],
      "metadata": {
        "id": "eMQhm4ERD-De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss(weight=torch.tensor([1., 1., 2.]))\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ],
      "metadata": {
        "id": "WMq3-ywGD-iu",
        "outputId": "4ab8375e-8d9f-4bc1-e1fc-1d66e296ec1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7188)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what if we want to simply ignore data points with label (y=2)?"
      ],
      "metadata": {
        "id": "N9PptD6oEOuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss(ignore_index=2)\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ],
      "metadata": {
        "id": "leCDdk5zEPHN",
        "outputId": "014a75c7-e8fd-46ed-b9de-e6fdec8d79ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5599)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, once again, there is yet another loss function available for multiclass\n",
        "classification. \n",
        "\n",
        "And, once again, it is very important to know when to use one or the\n",
        "other.\n",
        "\n",
        "So you don’t end up with an inconsistent combination of model and loss\n",
        "function."
      ],
      "metadata": {
        "id": "BOlVtIMPEf50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "0-6_NJmLElqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CDb2kCe0Enzh"
      }
    }
  ]
}