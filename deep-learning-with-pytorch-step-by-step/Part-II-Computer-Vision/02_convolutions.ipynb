{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNzGYhxvzFZvoz825jXcLIE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/02_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutions"
      ],
      "metadata": {
        "id": "8A9MVvQ5_T5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convolution is \"a mathematical operation on two functions (`f` and `g`) that produces a third function (`f * g`) expressing how the shape of one is modified by the other.\"\n",
        "\n",
        "In image processing, a convolution matrix is also called a kernel or filter. \n",
        "\n",
        "Typical image processing operations—like blurring, sharpening, edge detection, and more, are\n",
        "accomplished by performing a convolution between a kernel and an image."
      ],
      "metadata": {
        "id": "7K_rxSuE_Uhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "BebAYVWo_VlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ],
      "metadata": {
        "id": "iiQ_qcSp_Uq6",
        "outputId": "eb6a1b08-9aa7-422a-980b-590b61304108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter5()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter5 import *"
      ],
      "metadata": {
        "id": "6EwSrmrQ_cxf",
        "outputId": "f83ba4cc-c4e9-4f3b-b9bb-be456cf3edb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose, Normalize\n",
        "\n",
        "from data_generation.image_classification import generate_dataset\n",
        "from helpers import index_splitter, make_balanced_sampler\n",
        "from stepbystep.v1 import StepByStep"
      ],
      "metadata": {
        "id": "ZNIlTCW9_gBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter / Kernel"
      ],
      "metadata": {
        "id": "Rsnr8thUATxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, the filters are\n",
        "small square matrices. The convolution itself is performed by applying the filter on\n",
        "the image repeatedly. \n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv1.png?raw=1)\n",
        "\n",
        "That’s the region to which the filter is being applied and is called the\n",
        "receptive field, drawing an analogy to the way human vision works.\n",
        "\n",
        "Let’s try a concrete example to make it more clear."
      ],
      "metadata": {
        "id": "xHjtaGl4ETM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single = np.array([\n",
        "  [ # batch dim\n",
        "      [ # channel dim\n",
        "          [5, 0, 8, 7, 8, 1], # height and width dim\n",
        "          [1, 9, 5, 0, 7, 7],\n",
        "          [6, 0, 2, 4, 6, 6],\n",
        "          [9, 7, 6, 6, 8, 4],\n",
        "          [8, 3, 8, 5, 1, 3],\n",
        "          [7, 2, 7, 0, 1, 0]\n",
        "      ]\n",
        "  ]\n",
        "])\n",
        "single.shape"
      ],
      "metadata": {
        "id": "jPyifdt-AUSX",
        "outputId": "09a1cde4-3fd3-4ea7-ae81-b1d041afdb0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 6, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "identity = np.array([\n",
        "    [\n",
        "        [\n",
        "            [0, 0, 0],\n",
        "            [0, 1, 0],\n",
        "            [0, 0, 0]\n",
        "        ]\n",
        "    ]\n",
        "])\n",
        "identity.shape"
      ],
      "metadata": {
        "id": "oiO3F9UGFO9G",
        "outputId": "346ebd6b-e10a-48d9-a1bc-aedfe55c2369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolving"
      ],
      "metadata": {
        "id": "KkW-hK_CFhg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convulution performs an element-wise multiplication between the\n",
        "two, region and filter, and adds everything up.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv2.png?raw=1)"
      ],
      "metadata": {
        "id": "EUxO8zMfFkXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region = single[:, :, 0:3, 0:3]\n",
        "filtered_region = region * identity\n",
        "total = filtered_region.sum()\n",
        "total"
      ],
      "metadata": {
        "id": "CVEwetTtF1H2",
        "outputId": "f1e682a6-c5d5-4a57-efc3-91e98847e752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing a convolution produces an image with a\n",
        "reduced size.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv3.png?raw=1)"
      ],
      "metadata": {
        "id": "X69C4K0gGbWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving Around"
      ],
      "metadata": {
        "id": "2CYbWbW-HxNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we move the region one step to the right; that is, we change the receptive\n",
        "field and apply the filter again.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/stride1.png?raw=1)\n",
        "\n",
        "In code, it means we’re changing the slice of the input image:"
      ],
      "metadata": {
        "id": "B-uOhu84IWU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_region = single[:, :, 0:3, (0+1):(3+1)]"
      ],
      "metadata": {
        "id": "UN7eF6XnGotK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the operation remains the same: First, an element-wise multiplication, and\n",
        "then adding up the elements of the resulting matrix.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv5.png?raw=1)"
      ],
      "metadata": {
        "id": "p801DPVnJLI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_filtered_region = new_region * identity\n",
        "new_total = new_filtered_region.sum()\n",
        "new_total"
      ],
      "metadata": {
        "id": "tu1Xmj6HJOyX",
        "outputId": "4331f01f-7693-491b-ede5-9cd0585861f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have a second pixel value to add to our resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv6.png?raw=1)\n",
        "\n",
        "We can keep moving the gray region to the right until we can’t move it anymore.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv7.png?raw=1)\n",
        "\n",
        "The fourth step to the right will actually place the region partially outside the\n",
        "input image."
      ],
      "metadata": {
        "id": "N_zfUbc4KUzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
      ],
      "metadata": {
        "id": "g8r2NNINKeUI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected region does not match the shape of the filter anymore. \n",
        "\n",
        "So, if we try to\n",
        "perform the element-wise multiplication, it fails:"
      ],
      "metadata": {
        "id": "XP8Uby8_K4nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  last_horizontal_region * identity\n",
        "except Exception as exp:\n",
        "  print(exp)"
      ],
      "metadata": {
        "id": "zcWMNoMCK5w_",
        "outputId": "738fe935-db38-40ca-b7da-796c03445c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Shape"
      ],
      "metadata": {
        "id": "jBkcwd9cLRHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we go back to the left side and move down one step. If we repeat the\n",
        "operation, covering all valid regions, we’ll end up with a resulting image that is\n",
        "smaller (on the right).\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/conv8.png?raw=1)\n",
        "\n",
        "How much smaller is it going to be?\n",
        "\n",
        "It depends on the size of the filter.\n",
        "\n",
        ">The larger the filter, the smaller the resulting image.\n",
        "\n",
        "Since applying a filter always produces a single value, the reduction is equal to the\n",
        "filter size minus one.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * (h_f, w_f) = (h_i - (h_f - 1), w_i - (w_f - 1))\n",
        "$$\n",
        "\n",
        "If we assume the filter is a square matrix of size f, we can simplify the expression\n",
        "above to:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
        "$$\n",
        "\n",
        "But I’d like to keep the image size, is it possible?\n",
        "\n",
        "Sure it is! Padding comes to our rescue in this case."
      ],
      "metadata": {
        "id": "lhKlTPhcLR-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolving in PyTorch"
      ],
      "metadata": {
        "id": "iZdPF3vJSJNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how a convolution works, let’s try it out using PyTorch."
      ],
      "metadata": {
        "id": "svvUMTDGSKHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our image and filter to tensors\n",
        "image = torch.as_tensor(single).float()\n",
        "kernel = torch.as_tensor(identity).float()"
      ],
      "metadata": {
        "id": "FpDNP9utXtc6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the activation functions, convolutions come in two\n",
        "flavors: functional and module. \n",
        "\n",
        "There is a fundamental difference between the\n",
        "two, though: The functional convolution takes the kernel / filter as an argument\n",
        "while the module has (learnable) weights to represent the kernel / filter.\n",
        "\n",
        "Let’s use the functional convolution, `F.conv2d()`, to apply the identity filter to our\n",
        "input image."
      ],
      "metadata": {
        "id": "7XxFSecQYQao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved = F.conv2d(image, kernel, stride=1)\n",
        "convolved"
      ],
      "metadata": {
        "id": "Fm6TCTCpYfRF",
        "outputId": "f3a3153b-347f-4430-e98a-eeda42d77c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 5., 0., 7.],\n",
              "          [0., 2., 4., 6.],\n",
              "          [7., 6., 6., 8.],\n",
              "          [3., 8., 5., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s turn our attention to PyTorch’s convolution module, `nn.Conv2d`."
      ],
      "metadata": {
        "id": "2jPbCKifZuGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
        "conv(image)"
      ],
      "metadata": {
        "id": "UxDu6pa1ZwRu",
        "outputId": "9e0422a3-8ce1-4e28-dc38-fe56a599a0e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-3.1584, -3.6587, -3.3084, -1.9776],\n",
              "          [-1.6495, -0.7789, -3.8846, -2.4516],\n",
              "          [-6.4975, -4.3493, -3.1753, -3.6608],\n",
              "          [-5.0802, -1.0271, -2.6073, -1.0769]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results are gibberish now because the convolutional module randomly initializes the weights representing\n",
        "the kernel / filter.\n",
        "\n",
        "That’s the whole point of the convolutional module: It will learn\n",
        "the kernel / filter on its own.\n",
        "\n",
        "In traditional computer vision, people would develop different\n",
        "filters for different purposes: blurring, sharpening, edge\n",
        "detection, and so on.\n",
        "\n",
        "Can we tell it to learn multiple filters at once?"
      ],
      "metadata": {
        "id": "0mRxitR_aIFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_multiple = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1)\n",
        "conv_multiple(image)"
      ],
      "metadata": {
        "id": "-tkVCkuuasJi",
        "outputId": "23cf120e-8b98-4444-c80c-1e7bdf7ccf28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 2.5559,  1.6035,  0.2515, -1.0625],\n",
              "          [-1.8400, -2.9293,  2.6575,  1.8039],\n",
              "          [ 1.3645,  3.5655,  1.4484,  0.7340],\n",
              "          [-0.8595,  1.0977, -0.7592, -1.5410]],\n",
              "\n",
              "         [[ 3.4871,  3.6961,  3.2884,  5.1495],\n",
              "          [ 4.2876,  5.7939,  5.4350,  3.7492],\n",
              "          [ 7.8516,  5.2071,  3.2165,  4.2438],\n",
              "          [ 5.1253,  4.9019,  4.3800,  1.6612]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_multiple.weight"
      ],
      "metadata": {
        "id": "ToKEQ1v1a_fp",
        "outputId": "3bf7e8e5-1180-4ec9-d33e-b2b473e4d1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.2008, -0.3166,  0.2790],\n",
              "          [ 0.0693,  0.1972,  0.1449],\n",
              "          [-0.2383,  0.0989,  0.0931]]],\n",
              "\n",
              "\n",
              "        [[[ 0.2917, -0.1087, -0.0920],\n",
              "          [ 0.2168,  0.2777, -0.2537],\n",
              "          [ 0.0901,  0.2887,  0.2582]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also force a convolutional module to use a particular filter by setting its weights."
      ],
      "metadata": {
        "id": "T3e8p3TVdKkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  conv.weight[0] = kernel\n",
        "  conv.bias[0] = 0\n",
        "\n",
        "conv(image)"
      ],
      "metadata": {
        "id": "uzMNjIh6dMBq",
        "outputId": "edce83d1-1a0e-4588-8e00-623a7faa7eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 5., 0., 7.],\n",
              "          [0., 2., 4., 6.],\n",
              "          [7., 6., 6., 8.],\n",
              "          [3., 8., 5., 1.]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.weight"
      ],
      "metadata": {
        "id": "b5TEqL4ud8zW",
        "outputId": "d916d2db-4ae2-43eb-bb0f-d6d1fc2ccbcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[0., 0., 0.],\n",
              "          [0., 1., 0.],\n",
              "          [0., 0., 0.]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the weights to get specific filters is at the heart of\n",
        "transfer learning. \n",
        "\n",
        "Someone else trained a model, and that model\n",
        "learned lots of useful filters, so we don’t have to learn them\n",
        "again. We can set the corresponding weights and go from there."
      ],
      "metadata": {
        "id": "5gYV5KxYeIWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Striding"
      ],
      "metadata": {
        "id": "t9HI8NnZeKMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try a stride of two for a change and see what happens to the resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/strider2.png?raw=1)\n",
        "\n",
        "The resulting image, after the only four valid operations, looks like this.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/strider3.png?raw=1)\n",
        "\n",
        "Also, notice that using a larger stride made the shape of the resulting image even smaller.\n",
        "\n",
        ">The larger the stride, the smaller the resulting image.\n",
        "\n",
        "Once again, it makes sense: If we are skipping pixels in the input image, there are\n",
        "fewer regions of interest to apply the filter to. \n",
        "\n",
        "We can extend our previous formula\n",
        "to include the stride size (s):\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{h_i - f + 1}{s}, \\frac{w_i - f + 1}{s}\\right)\n",
        "$$\n",
        "\n",
        "Let’s use\n",
        "PyTorch’s functional convolution to double-check the results."
      ],
      "metadata": {
        "id": "fyfsZe1MeMGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved_stride2 = F.conv2d(image, kernel, stride=2)\n",
        "convolved_stride2"
      ],
      "metadata": {
        "id": "NSgTQekvyu-3",
        "outputId": "e434a9b8-510b-40e0-e4aa-7ef3a512344b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 0.],\n",
              "          [7., 6.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, it works!"
      ],
      "metadata": {
        "id": "3TTYNw37zEc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Padding"
      ],
      "metadata": {
        "id": "nVacXfTkzE46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, the operations we have performed have been shrinking the images. What\n",
        "about restoring them to their original glory, I mean, size?\n",
        "\n",
        "Padding means stuffing. We need to stuff the original image so it can sustain the \"attack\" on its size.\n",
        "\n",
        "How do I stuff an image?\n",
        "\n",
        "Simply add zeros around it.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding1.png?raw=1)\n",
        "\n",
        "By adding columns and rows of zeros around it, we expand the\n",
        "input image such that the gray region starts centered in the actual top left corner\n",
        "of the input image. \n",
        "\n",
        "This simple trick can be used to preserve the original size of the\n",
        "image.\n",
        "\n",
        "In code, as usual, PyTorch gives us two options: functional (`F.pad()`) and module (`nn.ConstantPad2d`).\n",
        "\n",
        "Let’s start with the module version this time:"
      ],
      "metadata": {
        "id": "gexEBVsuzH1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constant_padder = nn.ConstantPad2d(padding=1, value=0)\n",
        "constant_padder(image)"
      ],
      "metadata": {
        "id": "GIuPaIjY0dxq",
        "outputId": "98790f29-3b90-4f5f-e0c7-ee8c9da7c07a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
              "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
              "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
              "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also do asymmetric padding by specifying a tuple in the padding\n",
        "argument representing (left, right, top, bottom). \n",
        "\n",
        "So, if we were to stuff our\n",
        "image on the left and right sides only, the argument would go like this: `(1, 1, 0, 0)`.\n",
        "\n",
        "We can achieve the same result using the functional padding:"
      ],
      "metadata": {
        "id": "8My1qAQY1ynT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"constant\", value=0)\n",
        "padded"
      ],
      "metadata": {
        "id": "wrfPCPMz1yCk",
        "outputId": "10c1423a-482c-42d7-d8e3-9a7c4d7d097e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
              "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
              "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
              "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, there is another argument, mode, which was\n",
        "set to constant to match the module version above.\n",
        "\n",
        "What are the other available modes?\n",
        "\n",
        "There are three other modes: replicate, reflect, and circular.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/paddings.png?raw=1)\n",
        "\n",
        "In replication padding, the padded pixels have the same value as the closest real\n",
        "pixel.\n",
        "\n",
        "In PyTorch, one can use the functional form `F.pad()` with mode=\"replicate\", or use\n",
        "the module version `nn.ReplicationPad2d`:"
      ],
      "metadata": {
        "id": "vfbBSTl_2yYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replication_padder = nn.ReplicationPad2d(padding=1)\n",
        "replication_padder(image)"
      ],
      "metadata": {
        "id": "JtIfohjj4Bss",
        "outputId": "4c6c1f17-a2a6-456a-8784-c11780079ff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
              "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"replicate\")\n",
        "padded"
      ],
      "metadata": {
        "id": "bTbOtd1_4el9",
        "outputId": "d022ba58-931b-43cb-e8a8-39be7966aa1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
              "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
              "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
              "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reflection padding, it gets a bit trickier. It is like the outer columns and rows are\n",
        "used as axes for the reflection.\n",
        "\n",
        "In PyTorch, you can use the functional form `F.pad()` with mode=\"reflect\", or use\n",
        "the module version `nn.ReflectionPad2d`:"
      ],
      "metadata": {
        "id": "A2l0de5N4ybA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reflection_padder = nn.ReflectionPad2d(padding=1)\n",
        "reflection_padder(image)"
      ],
      "metadata": {
        "id": "bE-tinAm44rs",
        "outputId": "657743d6-9fbd-411e-bc16-1c417332e826",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
              "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
              "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"reflect\")\n",
        "padded"
      ],
      "metadata": {
        "id": "-HedDa4b5Gor",
        "outputId": "f4b89821-b756-4c6b-d3da-0ca9d73243dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
              "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
              "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
              "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In circular padding, the left-most (right-most) column gets copied as the right (left) padded column.\n",
        "\n",
        "Similarly, the top-most\n",
        "(bottom-most) row gets copied as the bottom (top) padded row. The corners\n",
        "receive the values of the diametrically opposed corner.\n",
        "\n",
        "In PyTorch, you must use the functional form `F.pad()` with mode=\"circular\" since\n",
        "there is no module version of the circular padding."
      ],
      "metadata": {
        "id": "qscKbJFg5atz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"circular\")\n",
        "padded"
      ],
      "metadata": {
        "id": "wIjXVU7i5smI",
        "outputId": "845eb91b-4579-4d70-beeb-97e1771df54d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
              "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
              "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
              "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
              "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
              "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
              "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
              "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By padding an image, it is possible to get resulting images with the same shape as\n",
        "input images, or even larger, should you choose to stuff more and more rows and\n",
        "columns into the input image. \n",
        "\n",
        "Assuming we’re doing symmetrical padding of size p,\n",
        "the resulting shape is given by the formula below:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{(h_i + 2p) - f + 1}{s}, \\frac{(w_i + 2p) - f + 1}{s}\\right)\n",
        "$$\n",
        "\n",
        "We’re basically extending the original dimensions by 2p pixels each."
      ],
      "metadata": {
        "id": "00snBJCt5-eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A REAL Filter"
      ],
      "metadata": {
        "id": "yb1FSMZ16JvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try an edge detector filter from traditional\n",
        "computer vision for a change:"
      ],
      "metadata": {
        "id": "TOS3C8WI6KN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_matrix = np.array([\n",
        "  [[\n",
        "    [0, 1, 0],\n",
        "    [1, -4, 1],\n",
        "    [0, 1, 0]\n",
        "  ]]\n",
        "])\n",
        "\n",
        "kernel_edge = torch.as_tensor(edge_matrix).float()\n",
        "kernel_edge.shape"
      ],
      "metadata": {
        "id": "1b3D6o4e-kMx",
        "outputId": "288399f8-8314-425a-dcc9-a9ec12268e7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let’s apply it to a different region of our (padded) input image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding2.png?raw=1)\n",
        "\n",
        "As you can see, filters, other than the identity one, do not simply copy the value at\n",
        "the center. \n",
        "\n",
        "The element-wise multiplication finally means something.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/padding3.png?raw=1)\n",
        "\n",
        "Let’s apply this filter to our image."
      ],
      "metadata": {
        "id": "ym94Q2j-_SxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode=\"constant\", value=0)\n",
        "conv_padded = F.conv2d(padded, kernel_edge, stride=1)\n",
        "conv_padded"
      ],
      "metadata": {
        "id": "1370UbvBAgTx",
        "outputId": "f8686d22-f33c-4275-8799-050345248598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-19.,  22., -20., -12., -17.,  11.],\n",
              "          [ 16., -30.,  -1.,  23.,  -7., -14.],\n",
              "          [-14.,  24.,   7.,  -2.,   1.,  -7.],\n",
              "          [-15., -10.,  -1.,  -1., -15.,   1.],\n",
              "          [-13.,  13., -11.,  -5.,  13.,  -7.],\n",
              "          [-18.,   9., -18.,  13.,  -3.,   4.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling"
      ],
      "metadata": {
        "id": "FrV8PymAA8kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling is different than the former operations: It splits the image into tiny chunks, performs an operation on each chunk (that yields a single value), and puts the chunks together as the resulting image.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/pooling1.png?raw=1)\n",
        "\n",
        "Our input image is split into nine chunks, and we perform a simple max operation\n",
        "(hence, max pooling) on each chunk (really, it is just taking the largest value in each\n",
        "chunk). Then, these values are put together, in order, to produce a smaller\n",
        "resulting image.\n",
        "\n",
        ">The larger the pooling kernel, the smaller the resulting image.\n",
        "\n",
        "In PyTorch, as usual, we have both forms: `F.max_pool2d()` and `nn.MaxPool2d`. \n",
        "\n",
        "Let’s\n",
        "use the functional form to replicate the max pooling."
      ],
      "metadata": {
        "id": "Yxve0AcE8dGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ],
      "metadata": {
        "id": "127inD4k97KY",
        "outputId": "7aae340e-5f9a-4bd3-9731-b026e5bc2fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[22., 23., 11.],\n",
              "          [24.,  7.,  1.],\n",
              "          [13., 13., 13.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then let’s use the module version to illustrate the large four-by-four pooling."
      ],
      "metadata": {
        "id": "9svGlR23_VEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxpool4 = nn.MaxPool2d(kernel_size=4)\n",
        "pooled4 = maxpool4(conv_padded)\n",
        "pooled4"
      ],
      "metadata": {
        "id": "8tMu3Vk5_Vte",
        "outputId": "f4385e19-1f0c-4eb9-c719-98d850a1ed07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[24.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides max pooling, average pooling is also fairly common. As the name\n",
        "suggests, it will output the average pixel value for each chunk. \n",
        "\n",
        "In PyTorch, we have\n",
        "`F.avg_pool2d()` and `nn.AvgPool2d`."
      ],
      "metadata": {
        "id": "NgJ8QWy-_0A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.avg_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ],
      "metadata": {
        "id": "_Yrn8wBE_8zR",
        "outputId": "71db1335-ecc7-4267-d2f9-49af961d5030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.7500, -2.5000, -6.7500],\n",
              "          [-3.7500,  0.7500, -5.0000],\n",
              "          [-2.2500, -5.2500,  1.7500]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avgpool4 = nn.AvgPool2d(kernel_size=4)\n",
        "pooled4 = avgpool4(conv_padded)\n",
        "pooled4"
      ],
      "metadata": {
        "id": "4_DdnfoDAMx_",
        "outputId": "5fc1f2fa-3047-47cd-aa03-c89e6a689549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.0625]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can I use a stride of a different size?"
      ],
      "metadata": {
        "id": "CNm1egCsAXcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2, stride=1)\n",
        "pooled"
      ],
      "metadata": {
        "id": "947Cu913AYAb",
        "outputId": "24e73cf3-2a1e-48bb-c3df-f543e5885806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[22., 22., 23., 23., 11.],\n",
              "          [24., 24., 23., 23.,  1.],\n",
              "          [24., 24.,  7.,  1.,  1.],\n",
              "          [13., 13., -1., 13., 13.],\n",
              "          [13., 13., 13., 13., 13.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Flattening"
      ],
      "metadata": {
        "id": "SOvlpuBiAsC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It simply flattens a tensor, preserving the first\n",
        "dimension such that we keep the number of data points while collapsing all other\n",
        "dimensions."
      ],
      "metadata": {
        "id": "Abg65oZUAs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flattened = nn.Flatten()(pooled)\n",
        "flattened"
      ],
      "metadata": {
        "id": "Mciiiw9JAx00",
        "outputId": "3ac8c930-d46e-4912-de42-9da1a7acbbea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[22., 22., 23., 23., 11., 24., 24., 23., 23.,  1., 24., 24.,  7.,  1.,\n",
              "          1., 13., 13., -1., 13., 13., 13., 13., 13., 13., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also accomplish\n",
        "the same thing using `view()`."
      ],
      "metadata": {
        "id": "WfQT9HbRBD6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled.view(1, -1)"
      ],
      "metadata": {
        "id": "EaCFFSjuBHm_",
        "outputId": "a0646386-c87f-4bc3-eb65-18ab4544673f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[22., 22., 23., 23., 11., 24., 24., 23., 23.,  1., 24., 24.,  7.,  1.,\n",
              "          1., 13., 13., -1., 13., 13., 13., 13., 13., 13., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Typical Architecture"
      ],
      "metadata": {
        "id": "JctalF_yBkwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical architecture uses a sequence of one or more typical convolutional\n",
        "blocks, with each block consisting of three operations:\n",
        "\n",
        "1. Convolution\n",
        "2. Activation function\n",
        "3. Pooling\n",
        "\n",
        "As images go through these operations, they will shrink in size.\n",
        "\n",
        "After the sequence of blocks, the image gets flattened: Hopefully, at this stage,\n",
        "there is no loss of information occurring by considering each value in the flattened\n",
        "tensor a feature on its own.\n",
        "\n",
        "If you think of it, what those typical convolutional blocks do is\n",
        "akin to pre-processing images and converting them into\n",
        "features."
      ],
      "metadata": {
        "id": "8l-OK9GrBlmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LeNet-5"
      ],
      "metadata": {
        "id": "SoYzyLmtDvdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 is a seven-level convolutional neural network developed by Yann LeCun in\n",
        "1998 to recognize hand-written digits in 28x28 pixel images—the famous MNIST\n",
        "dataset!\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/deep-learning-with-pytorch-step-by-step/Part-II-Computer-Vision/images/architecture_lenet.png?raw=1)\n",
        "\n",
        "Adapting LeNet-5 to today’s standards, it could be implemented like this:"
      ],
      "metadata": {
        "id": "mxNhaZqrDwNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet = nn.Sequential()\n",
        "\n",
        "###### Featurizer ##########\n",
        "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
        "lenet.add_module(\"C1\", nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, padding=2))\n",
        "lenet.add_module(\"func1\", nn.ReLU())\n",
        "lenet.add_module(\"S2\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
        "lenet.add_module(\"C3\", nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
        "lenet.add_module(\"func2\", nn.ReLU())\n",
        "lenet.add_module(\"S4\", nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "# Block 3: 16@5x5 -> 120@1x1\n",
        "lenet.add_module(\"C3\", nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
        "lenet.add_module(\"func3\", nn.ReLU())\n",
        "\n",
        "# Flattening\n",
        "lenet.add_module(\"flatten\", nn.Flatten())\n",
        "\n",
        "###### Classification ##########\n",
        "# Hidden Layer\n",
        "lenet.add_module(\"F6\", nn.Linear(in_features=120, out_features=84))\n",
        "lenet.add_module(\"func4\", nn.ReLU())\n",
        "# Output Layer\n",
        "lenet.add_module(\"OUTPUT\", nn.Linear(in_features=84, out_features=10))"
      ],
      "metadata": {
        "id": "5D96gCAWFiT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 used three convolutional blocks, although the last one does not have a\n",
        "max pooling, because the convolution already produces a single pixel.\n",
        "\n",
        "Then, these 120 values (or features) are flattened and fed to a typical hidden layer\n",
        "with 84 units. \n",
        "\n",
        "The last step is, obviously, the output layer, which produces ten\n",
        "logits to be used for digit classification (from 0 to 9, there are ten classes)."
      ],
      "metadata": {
        "id": "AHf-VodqHllp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiclass Classification"
      ],
      "metadata": {
        "id": "zQBdv5EHH1Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MUOmv1jzH2Sz"
      }
    }
  ]
}