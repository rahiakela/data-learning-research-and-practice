{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/generative-deep-learning/05-autoregressive/01_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
      "metadata": {
        "id": "b076bd1a-b236-4fbc-953d-8295b25122ae"
      },
      "source": [
        "## ðŸ¥™ LSTM on Recipe Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "658a95da-9645-4bcf-bd9d-4b95a4b6f582",
      "metadata": {
        "id": "658a95da-9645-4bcf-bd9d-4b95a4b6f582"
      },
      "source": [
        "In this notebook, we'll walk through the steps required to train your own LSTM on the recipes dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9",
      "metadata": {
        "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "wHbQUtRsSWFc",
        "outputId": "01df94b1-e1ea-47a8-cee4-ed3478c4934b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wHbQUtRsSWFc",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# content/gdrive/My Drive/Kaggle is the path where kaggle.json is  present in the Google Drive\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/kaggle-keys\""
      ],
      "metadata": {
        "id": "eRDYRjF0SWpK"
      },
      "id": "eRDYRjF0SWpK",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# download dataset from kaggle> URL: https://www.kaggle.com/datasets/hugodarwood/epirecipes\n",
        "kaggle datasets download hugodarwood/epirecipes\n",
        "\n",
        "mkdir epirecipes\n",
        "unzip -qq epirecipes.zip\n",
        "rm -rf epirecipes.zip\n",
        "mv *.* epirecipes/"
      ],
      "metadata": {
        "id": "hKEKW54kSYwL",
        "outputId": "a637a39c-a79c-440a-9753-e37fd11aad51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hKEKW54kSYwL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading epirecipes.zip to /content\n",
            "\r  0% 0.00/11.3M [00:00<?, ?B/s]\r 44% 5.00M/11.3M [00:00<00:00, 40.9MB/s]\n",
            "\r100% 11.3M/11.3M [00:00<00:00, 72.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
      "metadata": {
        "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5"
      },
      "source": [
        "## 0. Parameters <a name=\"parameters\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
      "metadata": {
        "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7716fac-0010-49b0-b98e-53be2259edde",
      "metadata": {
        "id": "b7716fac-0010-49b0-b98e-53be2259edde"
      },
      "source": [
        "## 1. Load the data <a name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
      "metadata": {
        "tags": [],
        "id": "93cf6b0f-9667-4146-8911-763a8a2925d3"
      },
      "outputs": [],
      "source": [
        "# Load the full dataset\n",
        "with open(\"epirecipes/full_format_recipes.json\") as json_data:\n",
        "    recipe_data = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361",
      "metadata": {
        "tags": [],
        "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset\n",
        "filtered_data = [\n",
        "    \"Recipe for \" + x[\"title\"] + \" | \" + \" \".join(x[\"directions\"])\n",
        "    for x in recipe_data\n",
        "    if \"title\" in x\n",
        "    and x[\"title\"] is not None\n",
        "    and \"directions\" in x\n",
        "    and x[\"directions\"] is not None\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
      "metadata": {
        "tags": [],
        "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
        "outputId": "fbfd73c3-ffe4-4df6-86a1-af9d42af3686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20111 recipes loaded\n"
          ]
        }
      ],
      "source": [
        "# Count the recipes\n",
        "n_recipes = len(filtered_data)\n",
        "print(f\"{n_recipes} recipes loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
      "metadata": {
        "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
        "outputId": "a092d982-911a-4401-c178-86921e854602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas  | Chop enough parsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan, covered, 5 minutes. Meanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute. Strain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve. Season with salt and pepper. Set bowl in an ice bath and cool to room temperature, stirring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top and chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar, 1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery, cornichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and 1/4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato salad, over ham.\n"
          ]
        }
      ],
      "source": [
        "example = filtered_data[9]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
      "metadata": {
        "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1"
      },
      "source": [
        "## 2. Tokenise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc",
      "metadata": {
        "tags": [],
        "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc"
      },
      "outputs": [],
      "source": [
        "# Pad the punctuation, to treat them as separate 'words'\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
      "metadata": {
        "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
        "outputId": "0a9a2246-3cba-4409-f28b-63f694e29176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas | Chop enough parsley leaves to measure 1 tablespoon ; reserve . Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan , covered , 5 minutes . Meanwhile , sprinkle gelatin over water in a medium bowl and let soften 1 minute . Strain broth through a fine - mesh sieve into bowl with gelatin and stir to dissolve . Season with salt and pepper . Set bowl in an ice bath and cool to room temperature , stirring . Toss ham with reserved parsley and divide among jars . Pour gelatin on top and chill until set , at least 1 hour . Whisk together mayonnaise , mustard , vinegar , 1 / 4 teaspoon salt , and 1 / 4 teaspoon pepper in a large bowl . Stir in celery , cornichons , and potatoes . Pulse peas with marjoram , oil , 1 / 2 teaspoon pepper , and 1 / 4 teaspoon salt in a food processor to a coarse mash . Layer peas , then potato salad , over ham . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Display an example of a recipe\n",
        "example_data = text_data[9]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1",
      "metadata": {
        "tags": [],
        "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1"
      },
      "outputs": [],
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de",
      "metadata": {
        "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de"
      },
      "outputs": [],
      "source": [
        "# Create a vectorisation layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4d6dd34a-d905-497b-926a-405380ebcf98",
      "metadata": {
        "id": "4d6dd34a-d905-497b-926a-405380ebcf98"
      },
      "outputs": [],
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
      "metadata": {
        "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
        "outputId": "f683ce82-cb53-4508-9fe9-b5144f132bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: .\n",
            "3: ,\n",
            "4: and\n",
            "5: to\n",
            "6: in\n",
            "7: the\n",
            "8: with\n",
            "9: a\n"
          ]
        }
      ],
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
      "metadata": {
        "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
        "outputId": "2b0d6270-77d0-43d0-9b41-483f42d662be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  26   16  557    1    8  298  335  189    4 1054  494   27  332  228\n",
            "  235  262    5  594   11  133   22  311    2  332   45  262    4  671\n",
            "    4   70    8  171    4   81    6    9   65   80    3  121    3   59\n",
            "   12    2  299    3   88  650   20   39    6    9   29   21    4   67\n",
            "  529   11  164    2  320  171  102    9  374   13  643  306   25   21\n",
            "    8  650    4   42    5  931    2   63    8   24    4   33    2  114\n",
            "   21    6  178  181 1245    4   60    5  140  112    3   48    2  117\n",
            "  557    8  285  235    4  200  292  980    2  107  650   28   72    4\n",
            "  108   10  114    3   57  204   11  172    2   73  110  482    3  298\n",
            "    3  190    3   11   23   32  142   24    3    4   11   23   32  142\n",
            "   33    6    9   30   21    2   42    6  353    3 3224    3    4  150\n",
            "    2  437  494    8 1281    3   37    3   11   23   15  142   33    3\n",
            "    4   11   23   32  142   24    6    9  291  188    5    9  412  572\n",
            "    2  230  494    3   46  335  189    3   20  557    2    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c195efb-84c6-4be0-a989-a7542188ad35",
      "metadata": {
        "id": "8c195efb-84c6-4be0-a989-a7542188ad35"
      },
      "source": [
        "## 3. Create the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b",
      "metadata": {
        "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b"
      },
      "outputs": [],
      "source": [
        "# Create the training set of recipes and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
      "metadata": {
        "tags": [],
        "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5"
      },
      "source": [
        "## 4. Build the LSTM <a name=\"build\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9230b5bf-b4a8-48d5-b73b-6899a598f296",
      "metadata": {
        "id": "9230b5bf-b4a8-48d5-b73b-6899a598f296",
        "outputId": "a5b52525-b5a6-4f5d-d205-6d98d5098f21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         1000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         117248    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 10000)       1290000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,407,248\n",
            "Trainable params: 2,407,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad",
      "metadata": {
        "tags": [],
        "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad"
      },
      "outputs": [],
      "source": [
        "if LOAD_MODEL:\n",
        "    # model.load_weights('./models/model')\n",
        "    lstm = models.load_model(\"./models/lstm\", compile=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b14665-4359-447b-be58-3fd58ba69084",
      "metadata": {
        "id": "35b14665-4359-447b-be58-3fd58ba69084"
      },
      "source": [
        "## 5. Train the LSTM <a name=\"train\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d",
      "metadata": {
        "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d"
      },
      "outputs": [],
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
      "metadata": {
        "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72"
      },
      "outputs": [],
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }  # <1>\n",
        "\n",
        "    def sample_from(self, probs, temperature):  # <2>\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]  # <3>\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)  # <5>\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)  # <7>\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"recipe for\", max_tokens=100, temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "349865fe-ffbe-450e-97be-043ae1740e78",
      "metadata": {
        "id": "349865fe-ffbe-450e-97be-043ae1740e78"
      },
      "outputs": [],
      "source": [
        "# Create a model save checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# Tokenize starting prompt\n",
        "text_generator = TextGenerator(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
      "metadata": {
        "tags": [],
        "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e"
      },
      "outputs": [],
      "source": [
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
      "metadata": {
        "tags": [],
        "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c"
      },
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "lstm.save(\"./models/lstm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20",
      "metadata": {
        "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20"
      },
      "source": [
        "## 6. Generate text using the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649",
      "metadata": {
        "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649"
      },
      "outputs": [],
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
      "metadata": {
        "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df72866-b483-4489-8e26-d5e1466410fa",
      "metadata": {
        "tags": [],
        "id": "9df72866-b483-4489-8e26-d5e1466410fa"
      },
      "outputs": [],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562e1fe8-cbcb-438f-9637-2f2a6279c924",
      "metadata": {
        "id": "562e1fe8-cbcb-438f-9637-2f2a6279c924"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56356f21-04ac-40e5-94ff-291eca6a7054",
      "metadata": {
        "id": "56356f21-04ac-40e5-94ff-291eca6a7054"
      },
      "outputs": [],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e434497-07f3-4989-a68d-3e31cf8fa4fe",
      "metadata": {
        "id": "2e434497-07f3-4989-a68d-3e31cf8fa4fe"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011cd0e0-956c-4a63-8ec3-f7dfed31764e",
      "metadata": {
        "id": "011cd0e0-956c-4a63-8ec3-f7dfed31764e"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}