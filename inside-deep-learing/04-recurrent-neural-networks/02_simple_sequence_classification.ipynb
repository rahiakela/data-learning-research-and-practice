{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-simple-sequence-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNirV4xk9S+6/gNevRZwn8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/04-recurrent-neural-networks/02_simple_sequence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple sequence classification"
      ],
      "metadata": {
        "id": "ldjKkwDE9r9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by creating a many-to-one classification problem. \n",
        "\n",
        "What do I mean by this?\n",
        "\n",
        "We will have many inputs (every time step), but we will have only one output: the class label we are trying to predict.\n",
        "\n",
        "For a simple sequence classification problem, we will take an example of identifying the language a name comes from.\n",
        "\n",
        "For example, “Steven” is an English name.\n",
        "Note that this problem can’t be solved perfectly—for example, “Frank” could be English\n",
        "or German—so we should expect some errors due to these issues and oversimplification.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/04-recurrent-neural-networks/images/2.png?raw=1' width='600'/>\n",
        "\n",
        "RNN process for classifying a name’s source language. The individual characters of a name\n",
        "make the sequence that is fed into the RNN. \n",
        "\n",
        "We learn how to convert each character into a vector and\n",
        "how to get an RNN to process that sequence and return a final activation $h_T$ , and we end with a linear\n",
        "layer that produces a prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "tFXQb6wufLVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Ju6nJVQUfX5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/EdwardRaff/Inside-Deep-Learning/raw/main/idlmam.py"
      ],
      "metadata": {
        "id": "y3Vgfh4TryfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision \n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import requests, zipfile, io\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from scipy.signal import convolve\n",
        "\n",
        "import time\n",
        "\n",
        "from idlmam import train_simple_network, set_seed, Flatten, weight_reset"
      ],
      "metadata": {
        "id": "bUicuO0T9vRt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('png', 'pdf')\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic=True\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "8U1JNhEmEkzG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "t_R2F_862eZg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's downloads the dataset and extracts all the files."
      ],
      "metadata": {
        "id": "sy3ruGbzF6rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
        "\n",
        "# Zip file is organized as data/names/[LANG].txt , where [LANG] is a specific language\n",
        "req = requests.get(zip_file_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(req.content))\n",
        "z.extractall()"
      ],
      "metadata": {
        "id": "Wh5dmyuUF-S4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing dataset"
      ],
      "metadata": {
        "id": "ycbbAdR0fjAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this dataset is pretty small, we load all of it into memory."
      ],
      "metadata": {
        "id": "Y_4r4xfAJtCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary, which maps the language name\n",
        "namge_language_data = {}\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "alphabet = {}\n",
        "\n",
        "for i in range(n_letters):\n",
        "  alphabet[all_letters[i]] = i"
      ],
      "metadata": {
        "id": "smI9SLH8uMOd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\" and c in all_letters)"
      ],
      "metadata": {
        "id": "RVehRr4DtRF3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loops through every language, opens the zip file entry, and reads all the lines from the text file\n",
        "for zip_path in z.namelist():\n",
        "  if \"data/names/\" in zip_path and zip_path.endswith(\".txt\"):\n",
        "    lang = zip_path[len(\"data/names/\"): -len(\".txt\")]\n",
        "    with z.open(zip_path) as zipfile:\n",
        "      # Turns a Unicode string into plain ASCII\n",
        "      lang_names = [unicode_to_ascii(line).lower() for line in str(zipfile.read(), encoding=\"utf-8\").strip().split(\"\\n\")]\n",
        "      namge_language_data[lang] = lang_names\n",
        "    print(f\"{lang}: {str(len(lang_names))}\")"
      ],
      "metadata": {
        "id": "UOdEyP4mJxe6",
        "outputId": "0bf5d033-db71-4e29-8985-ab64cf088a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic: 2000\n",
            "Chinese: 268\n",
            "Czech: 519\n",
            "Dutch: 297\n",
            "English: 3668\n",
            "French: 277\n",
            "German: 724\n",
            "Greek: 203\n",
            "Irish: 232\n",
            "Italian: 709\n",
            "Japanese: 991\n",
            "Korean: 94\n",
            "Polish: 139\n",
            "Portuguese: 74\n",
            "Russian: 9408\n",
            "Scottish: 100\n",
            "Spanish: 298\n",
            "Vietnamese: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have created a dataset, which you may notice is not well balanced: there are far\n",
        "more Russian names than any other language."
      ],
      "metadata": {
        "id": "nP2JzpMcLZKp"
      }
    }
  ]
}