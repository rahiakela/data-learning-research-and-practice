{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-simple-sequence-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGbzJPGxNq5bzwE91j7ae0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/04-recurrent-neural-networks/02_simple_sequence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple sequence classification"
      ],
      "metadata": {
        "id": "ldjKkwDE9r9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by creating a many-to-one classification problem. \n",
        "\n",
        "What do I mean by this?\n",
        "\n",
        "We will have many inputs (every time step), but we will have only one output: the class label we are trying to predict.\n",
        "\n",
        "For a simple sequence classification problem, we will take an example of identifying the language a name comes from.\n",
        "\n",
        "For example, “Steven” is an English name.\n",
        "Note that this problem can’t be solved perfectly—for example, “Frank” could be English\n",
        "or German—so we should expect some errors due to these issues and oversimplification.\n",
        "\n",
        "<img src='https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/04-recurrent-neural-networks/images/2.png?raw=1' width='600'/>\n",
        "\n",
        "RNN process for classifying a name’s source language. The individual characters of a name\n",
        "make the sequence that is fed into the RNN. \n",
        "\n",
        "We learn how to convert each character into a vector and\n",
        "how to get an RNN to process that sequence and return a final activation $h_T$ , and we end with a linear\n",
        "layer that produces a prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "tFXQb6wufLVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Ju6nJVQUfX5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/EdwardRaff/Inside-Deep-Learning/raw/main/idlmam.py"
      ],
      "metadata": {
        "id": "y3Vgfh4TryfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision \n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import requests, zipfile, io\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from scipy.signal import convolve\n",
        "\n",
        "import time\n",
        "\n",
        "from idlmam import train_simple_network, set_seed, Flatten, weight_reset"
      ],
      "metadata": {
        "id": "bUicuO0T9vRt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('png', 'pdf')\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic=True\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "8U1JNhEmEkzG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "t_R2F_862eZg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's downloads the dataset and extracts all the files."
      ],
      "metadata": {
        "id": "sy3ruGbzF6rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
        "\n",
        "# Zip file is organized as data/names/[LANG].txt , where [LANG] is a specific language\n",
        "req = requests.get(zip_file_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(req.content))\n",
        "z.extractall()"
      ],
      "metadata": {
        "id": "Wh5dmyuUF-S4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing dataset"
      ],
      "metadata": {
        "id": "ycbbAdR0fjAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this dataset is pretty small, we load all of it into memory."
      ],
      "metadata": {
        "id": "Y_4r4xfAJtCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary, which maps the language name\n",
        "namge_language_data = {}\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "alphabet = {}\n",
        "\n",
        "for i in range(n_letters):\n",
        "  alphabet[all_letters[i]] = i"
      ],
      "metadata": {
        "id": "smI9SLH8uMOd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\" and c in all_letters)"
      ],
      "metadata": {
        "id": "RVehRr4DtRF3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loops through every language, opens the zip file entry, and reads all the lines from the text file\n",
        "for zip_path in z.namelist():\n",
        "  if \"data/names/\" in zip_path and zip_path.endswith(\".txt\"):\n",
        "    lang = zip_path[len(\"data/names/\"): -len(\".txt\")]\n",
        "    with z.open(zip_path) as zipfile:\n",
        "      # Turns a Unicode string into plain ASCII\n",
        "      lang_names = [unicode_to_ascii(line).lower() for line in str(zipfile.read(), encoding=\"utf-8\").strip().split(\"\\n\")]\n",
        "      namge_language_data[lang] = lang_names\n",
        "    print(f\"{lang}: {str(len(lang_names))}\")"
      ],
      "metadata": {
        "id": "UOdEyP4mJxe6",
        "outputId": "84271965-32b8-4194-f453-e416e79f6193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic: 2000\n",
            "Chinese: 268\n",
            "Czech: 519\n",
            "Dutch: 297\n",
            "English: 3668\n",
            "French: 277\n",
            "German: 724\n",
            "Greek: 203\n",
            "Irish: 232\n",
            "Italian: 709\n",
            "Japanese: 991\n",
            "Korean: 94\n",
            "Polish: 139\n",
            "Portuguese: 74\n",
            "Russian: 9408\n",
            "Scottish: 100\n",
            "Spanish: 298\n",
            "Vietnamese: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have created a dataset, which you may notice is not well balanced: there are far\n",
        "more Russian names than any other language.\n",
        "\n",
        "With our data loaded in memory, we can now implement a Dataset to represent it."
      ],
      "metadata": {
        "id": "nP2JzpMcLZKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageNameDataset(Dataset):\n",
        "\n",
        "  def __init__(self, lang_name_dict, vocabulary) -> None:\n",
        "    self.label_names = [x for x in lang_name_dict.keys()]\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    self.vocabulary = vocabulary\n",
        "    for y, language in enumerate(self.label_names):\n",
        "      for sample in lang_name_dict[language]:\n",
        "        self.data.append(sample)\n",
        "        self.labels.append(y)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def string2vec(self, input_string):\n",
        "    \"\"\"\n",
        "    This method will convert any input string into a vector of long values, according to the vocabulary used by this object. \n",
        "    input_string: the string to convert to a tensor\n",
        "    \"\"\"\n",
        "    # How many characters long is the string?\n",
        "    T = len(input_string)\n",
        "    # Create a new tensor to store the result in\n",
        "    name_vec = torch.zeros((T), dtype=torch.long)\n",
        "    # iterate through the string and place the appropriate values into the tensor\n",
        "    for pos, character in enumerate(input_string):\n",
        "      name_vec[pos] = self.vocabulary[character]\n",
        "    return name_vec\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    name = self.data[index]\n",
        "    label = self.labels[index]\n",
        "    # Conver the correct class label into a tensor for PyTorch\n",
        "    label_vec = torch.tensor([label], dtype=torch.long)\n",
        "    return self.string2vec(name), label"
      ],
      "metadata": {
        "id": "SfNqXgu6aBCI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's creates a train/test split, with 300 items in the test split."
      ],
      "metadata": {
        "id": "bKWDuYlSjhK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = LanguageNameDataset(namge_language_data, alphabet)\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(dataset, (len(dataset) - 300, 300))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "BcztOiuxjjcL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embedding layers"
      ],
      "metadata": {
        "id": "hDcHeLL7kOyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layers are lookup tables designed to map each integer value to a specific\n",
        "vector representation. You tell the embedding layer how large the vocabulary is (i.e.,\n",
        "how many unique items exist) and how large you want the output dimension to be.\n",
        "\n",
        "The last thing we need is a way to map each integer to a corresponding vector, which is accomplished using an embedding layer."
      ],
      "metadata": {
        "id": "9vpg9GoKkQZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  input_sequence = torch.tensor([0, 1, 1, 0, 2], dtype=torch.long)\n",
        "  embedding = nn.Embedding(3, 2)\n",
        "  x_seq = embedding(input_sequence)\n",
        "  print(input_sequence.shape, x_seq.shape)\n",
        "  print(x_seq)"
      ],
      "metadata": {
        "id": "T521QHpEbUTh",
        "outputId": "b325e9a4-5a2c-4ea1-dda8-865a3cfbf731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5]) torch.Size([5, 2])\n",
            "tensor([[-1.0839,  2.1440],\n",
            "        [ 0.6532,  0.7195],\n",
            "        [ 0.6532,  0.7195],\n",
            "        [-1.0839,  2.1440],\n",
            "        [ 1.0832,  1.1107]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the first and fourth rows of the matrix have the same values, as do the second and third. \n",
        "\n",
        "This is because the order\n",
        "in the output matches the order in the input."
      ],
      "metadata": {
        "id": "3e-MjTttcfFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN algorithm"
      ],
      "metadata": {
        "id": "cObn9Rwgc3Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fO-TAed_c6Xn"
      }
    }
  ]
}