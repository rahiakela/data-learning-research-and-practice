{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8uunsz3kU61SzKCzX1G/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/12-rnn-alternatives/01_torchtext_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Torch text basic"
      ],
      "metadata": {
        "id": "ldjKkwDE9r9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks—in particular, LSTMs—have been used for classifying and\n",
        "working with sequence problems for over two decades. While they have long been\n",
        "reliable tools for the task, they have several undesirable properties. \n",
        "\n",
        "* RNNs are just plain \n",
        "* they do not scale well with more layers or with more GPUs\n",
        "\n",
        "With skip connections and residual\n",
        "layers, we have learned about many ways to get fully connected and convolutional\n",
        "networks to train with more layers to get better results. \n",
        "\n",
        "But RNNs just do not seem to\n",
        "like being deep. You can add more layers and skip connections, but they do not show\n",
        "the same degree of benefits as improved accuracy.\n",
        "\n",
        "So, we look at some methods that can help us with one or both of these\n",
        "problems.\n",
        "\n",
        "* we tackle the slowness of RNNs by violating our prior beliefs.\n",
        "* we look at a different way of representing the sequential component of our data to augment these faster alternatives and regain some of our accuracy.\n",
        "* Finally, we learn about transformers.\n",
        "\n"
      ],
      "metadata": {
        "id": "tFXQb6wufLVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Ju6nJVQUfX5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "id": "q7I44z5XHnZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/EdwardRaff/Inside-Deep-Learning/raw/main/idlmam.py"
      ],
      "metadata": {
        "id": "j8dfqZhAI7hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision \n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torchvision import transforms\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import time\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "\n",
        "from idlmam import train_network, Flatten, weight_reset, View, set_seed\n",
        "from idlmam import AttentionAvg, GeneralScore, DotScore, AdditiveAttentionScore, ApplyAttention, getMaskByFill"
      ],
      "metadata": {
        "id": "_9xoD7-8EibG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('png', 'pdf')\n",
        "\n",
        "from IPython.display import display_pdf\n",
        "from IPython.display import Latex"
      ],
      "metadata": {
        "id": "8U1JNhEmEkzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic=True\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "YTO1Pk4x24PP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "nXiYSSxO707X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Bbs8SwBuguUa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AG News dataset"
      ],
      "metadata": {
        "id": "ycbbAdR0fjAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s quickly load AG_NEWS dataset\n",
        "train_iter, test_iter = AG_NEWS(root=\"./data\", split=(\"train\", \"test\"))\n",
        "\n",
        "train_dataset = list(train_iter)\n",
        "test_dataset = list(test_iter)"
      ],
      "metadata": {
        "id": "_chm5G2Z3Eqs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "NymODd7xqoW7",
        "outputId": "e6d6fd90-7e37-43cf-f75c-cd762ea3bcce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizers break strings like \"this is a string\" into lists of tokens like ['this', 'is', 'a', 'string']\n",
        "tokenizer = get_tokenizer(\"basic_english\") # we will be fine with the default english style tokenizer\n",
        "\n",
        "# we need to create a vocabulary of all the words in the training set\n",
        "counter = Counter()\n",
        "for (label, line) in train_dataset:\n",
        "  # count the number of unique tokens we see and how often we see them (e.g., we will see \"the\" a lot, but \"sasquatch\" maybe once or not at all.)\n",
        "  counter.update(tokenizer(line))\n",
        "# create a vocab object, removing any word that didn't occur at least 10 times, \n",
        "# and add special vocab items for unkown, begining of sentance, end of sentance, and \"padding\"\n",
        "vocab = vocab(counter, min_freq=10, specials=(\"<unk>\", \"<BOS>\", \"<EOS>\", \"<PAD>\"))\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "e-hdDWUGq9tb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "u81fytNkPREF",
        "outputId": "2d418a42-a3c6-4ff7-edc4-f10110276127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20647"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_transform(x):\n",
        "  # vocab acts like a dictionary, handls unkown tokens.\n",
        "  # We can make it pre- and post-pend with the start and end markers, respectively.\n",
        "  return [vocab[\"<BOS>\"]] + [vocab[token] for token in tokenizer(x)] + [vocab[\"<EOS>\"]]\n",
        "\n",
        "def label_transform(y):\n",
        "  # labes are originally [1, 2, 3, 4] but we need them as [0, 1, 2, 3] \n",
        "  return y - 1"
      ],
      "metadata": {
        "id": "NoeUZqd6QRjr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][1]"
      ],
      "metadata": {
        "id": "SxbRlswBMKDr",
        "outputId": "bdaf7834-2e8f-47c8-9f15-da2446673e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's transform the first data point's text into a list of tokens\n",
        "print(text_transform(train_dataset[0][1]))"
      ],
      "metadata": {
        "id": "oISrxQN0JnkS",
        "outputId": "b1c14659-e8f5-4d88-f743-dd0e5e1b57bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 16, 0, 17, 4, 18, 19, 20, 0, 21, 0, 17, 22, 23, 24, 25, 6, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make training faster, let’s limit ourselves to sentences that contain six or fewer words.\n",
        "VOCAB_SIZE  = len(vocab)\n",
        "NUM_CLASS = len(np.unique([z[0] for z in train_dataset]))\n",
        "print(f\"Vocab: {VOCAB_SIZE}\")\n",
        "print(f\"Num Classes: {NUM_CLASS}\")\n",
        "\n",
        "padding_idx = vocab[\"<PAD>\"]\n",
        "embed_dim = 128\n",
        "B = 64\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "jtko00QyrbC3",
        "outputId": "d49b2af5-7b6d-4df3-8c02-07e05ddb30ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: 20647\n",
            "Num Classes: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_batch(batch):\n",
        "  \"\"\"\n",
        "  Pad items in the batch to the length of the longest item in the batch. \n",
        "  Also, re-order so that the values are returned (input, label)\n",
        "  \"\"\"\n",
        "  # transform every label in the batch\n",
        "  labels = [label_transform(z[0]) for z in batch]\n",
        "  # tokenizes every text and puts them into a tensor\n",
        "  texts = [torch.tensor(text_transform(z[1]), dtype=torch.int64) for z in batch]\n",
        "\n",
        "  # what is the longest sequence in this batch?\n",
        "  max_len = max([text.size(0) for text in texts])\n",
        "  # pad each text tensor by whatever amount gets it to the max_len\n",
        "  texts = [F.pad(text, (0, max_len - text.size(0)), value=padding_idx) for text in texts]\n",
        "  # make x and y a single tensor\n",
        "  x, y = torch.stack(texts), torch.tensor(labels, dtype=torch.int64)\n",
        "  return  x, y"
      ],
      "metadata": {
        "id": "GpI1PcRxCCU-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can build our DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, collate_fn=pad_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=B, collate_fn=pad_batch)"
      ],
      "metadata": {
        "id": "xmJZIwpDDpR1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline model"
      ],
      "metadata": {
        "id": "rqx1ey6VECYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-e20StYsEEN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}