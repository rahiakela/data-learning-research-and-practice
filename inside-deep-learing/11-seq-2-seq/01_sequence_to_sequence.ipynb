{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMfZqVGvHgJULt7J+mc28Cv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/11-seq-2-seq/01_sequence_to_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence-tosequence"
      ],
      "metadata": {
        "id": "ldjKkwDE9r9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In particular, we will develop an algorithm known as sequence-tosequence\n",
        "(Seq2Seq for short) that can perform machine translation. As the name implies,\n",
        "this is an approach for getting neural networks to take one sequence as input and\n",
        "produce a different sequence as the output. \n",
        "\n",
        "Seq2Seq has been used to get computers to\n",
        "perform symbolic calculus, summarize long documents, and even translate from one\n",
        "language to another. I’ll show you step by step how we can translate from English to\n",
        "French.\n",
        "\n",
        "You\n",
        "could hypothetically get an RNN to do anything Seq2Seq could do, but it would be\n",
        "difficult to get it to work. One problem is that an RNN alone implies that the output\n",
        "sequence is the same length as the input, which is rarely true. Seq2Seq decouples the\n",
        "input and output into two separate stages and parts and thus works much better.\n",
        "\n",
        "At a high level, the Seq2Seq algorithm trains a\n",
        "denoising autoencoder over sequences rather than static images. \n",
        "\n",
        "You can think of the\n",
        "original English as the noisy input and French as the clean output, and we ask the\n",
        "Seq2Seq model to learn how to remove the noise.\n",
        "\n",
        "We have an\n",
        "encoder and decoder to convert inputs to outputs just like a denoising autoencoder;\n",
        "the difference is that a Seq2Seq model works over sequences instead of images or fully\n",
        "connected inputs.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/11-seq-2-seq/images/seq2seq.png?raw=1)\n",
        "\n",
        "Here we have some original input sequence $X = x_1, x_2, ..., x_T$ , and the goal is to\n",
        "output a new sequence $Y = y_1, y_2, ..., y_{\\hat T}$ . These sequences do not have to be the\n",
        "same. $x_j \\neq y_j$ , and they can even be different lengths so T $6 \\neq \\hat T$ is also possible.\n",
        "\n",
        "The secret to getting a Seq2Seq model working well is adding an attention mechanism.\n",
        "\n",
        "![](https://github.com/rahiakela/deep-learning-research-and-practice/blob/main/inside-deep-learing/11-seq-2-seq/images/seq2seq_attn.png?raw=1)\n",
        "\n",
        "Notice that the last\n",
        "hidden state from the encoder, hT , becomes the initial hidden state of the decoder, but\n",
        "we haven’t indicted what the inputs to the decoder are at each time step."
      ],
      "metadata": {
        "id": "tFXQb6wufLVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Ju6nJVQUfX5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/EdwardRaff/Inside-Deep-Learning/raw/main/idlmam.py"
      ],
      "metadata": {
        "id": "j8dfqZhAI7hK",
        "outputId": "a9380490-2349-4da2-c143-a2ca7b94706f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-31 06:33:26--  https://github.com/EdwardRaff/Inside-Deep-Learning/raw/main/idlmam.py\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/EdwardRaff/Inside-Deep-Learning/main/idlmam.py [following]\n",
            "--2023-03-31 06:33:27--  https://raw.githubusercontent.com/EdwardRaff/Inside-Deep-Learning/main/idlmam.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22064 (22K) [text/plain]\n",
            "Saving to: ‘idlmam.py’\n",
            "\n",
            "idlmam.py           100%[===================>]  21.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-31 06:33:28 (106 MB/s) - ‘idlmam.py’ saved [22064/22064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision \n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import time\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "\n",
        "from idlmam import train_network, Flatten, weight_reset, View, set_seed\n",
        "from idlmam import AttentionAvg, GeneralScore, DotScore, AdditiveAttentionScore, ApplyAttention, getMaskByFill"
      ],
      "metadata": {
        "id": "_9xoD7-8EibG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('png', 'pdf')\n",
        "\n",
        "from IPython.display import display_pdf\n",
        "from IPython.display import Latex"
      ],
      "metadata": {
        "id": "8U1JNhEmEkzG",
        "outputId": "9dc2773e-3a4d-4132-c637-d40623a62a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-024163930d81>:3: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('png', 'pdf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic=True\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "YTO1Pk4x24PP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "nXiYSSxO707X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##English-French dataset"
      ],
      "metadata": {
        "id": "ycbbAdR0fjAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s quickly load small English-French dataset\n",
        "all_data = []\n",
        "response = urlopen(\"https://download.pytorch.org/tutorial/data.zip\")\n",
        "zipfile = ZipFile(BytesIO(response.read()))\n",
        "\n",
        "for line in zipfile.open(\"data/eng-fra.txt\").readlines():\n",
        "  line = line.decode(\"utf-8\").lower()   # lower case only please\n",
        "  line = re.sub(r\"[-.!?]+\", r\" \", line) # no puntuation\n",
        "  source_lang, target_lang = line.split(\"\\t\")[0: 2]\n",
        "  all_data.append((source_lang.strip(), target_lang.strip())) # (english, french)"
      ],
      "metadata": {
        "id": "_chm5G2Z3Eqs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(all_data[i])"
      ],
      "metadata": {
        "id": "NymODd7xqoW7",
        "outputId": "5992a809-2c4b-4995-84b9-d277067bb4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('go', 'va')\n",
            "('run', 'cours')\n",
            "('run', 'courez')\n",
            "('wow', 'ça alors')\n",
            "('fire', 'au feu')\n",
            "('help', \"à l'aide\")\n",
            "('jump', 'saute')\n",
            "('stop', 'ça suffit')\n",
            "('stop', 'stop')\n",
            "('stop', 'arrête toi')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data[:10]"
      ],
      "metadata": {
        "id": "e-hdDWUGq9tb",
        "outputId": "cf376f06-1455-4d1a-94df-e75867ca6b79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go', 'va'),\n",
              " ('run', 'cours'),\n",
              " ('run', 'courez'),\n",
              " ('wow', 'ça alors'),\n",
              " ('fire', 'au feu'),\n",
              " ('help', \"à l'aide\"),\n",
              " ('jump', 'saute'),\n",
              " ('stop', 'ça suffit'),\n",
              " ('stop', 'stop'),\n",
              " ('stop', 'arrête toi')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make training faster, let’s limit ourselves to sentences that contain six or fewer words.\n",
        "short_subset = []\n",
        "MAX_LEN = 6\n",
        "\n",
        "for (s, t) in all_data:\n",
        "  if max(len(s.split(\" \")), len(t.split(\" \"))) <= MAX_LEN:\n",
        "    short_subset.append((s, t))\n",
        "print(f\"Using {len(short_subset)} / {len(all_data)}\")"
      ],
      "metadata": {
        "id": "jtko00QyrbC3",
        "outputId": "21d04d6b-3ddd-4440-9dc9-92e0841b8826",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 66251 / 135842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's defines SOS, EOS, and padding markers and creates a dictionary word2indx to create the mapping \n",
        "# and an inverted dictionary indx2word so that we can look at our results more easily when we are done\n",
        "SOS_token = \"<SOS>\"  # START_OF_SENTANCE_TOKEN\n",
        "EOS_token = \"<EOS>\"  # END_OF_SENTANCE_TOKEN\n",
        "PAD_token = \"_PADDING_\"\n",
        "\n",
        "word2index = {PAD_token: 0, SOS_token: 1, EOS_token: 2}\n",
        "for s, t in short_subset:\n",
        "  for sentance in (s, t):\n",
        "    for word in sentance.split(\" \"):\n",
        "      if word not in word2index:\n",
        "        word2index[word] = len(word2index)\n",
        "print(f\"Size of Vocab: {len(word2index)}\")\n",
        "\n",
        "# build the inverted dict for looking at the outputs later\n",
        "index2word = {}\n",
        "for word, index in word2index.items():\n",
        "  index2word[index] = word"
      ],
      "metadata": {
        "id": "DG4yIqcWsrU8",
        "outputId": "7b537735-c026-41b7-ded2-5eafcedf63b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Vocab: 24577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Takes a dataset with tuples of strings (x, y) and converts them to tuples of int64 tensors. \n",
        "  This makes it easy to encode Seq2Seq problems.\n",
        "  \n",
        "  Strings in the input and output targets will be broken up by spaces\n",
        "  \"\"\"\n",
        "  def __init__(self, lang_pairs, word2index):\n",
        "    \"\"\"\n",
        "    lang_pairs: a List[Tuple[String,String]] containing the source,target pairs for a Seq2Seq problem. \n",
        "    word2indx: a Map[String,Int] that converts each word in an input string into a unique ID.\n",
        "    \"\"\"\n",
        "    self.lang_pairs = lang_pairs \n",
        "    self.word2index = word2index \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lang_pairs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x, y = self.lang_pairs[idx]\n",
        "    x = SOS_token + \" \" + x + \" \" + EOS_token\n",
        "    y = y + \" \" + EOS_token\n",
        "    \n",
        "    # convert to lists of integers\n",
        "    x = [self.word2index[w] for w in x.split(\" \")]\n",
        "    y = [self.word2index[w] for w in y.split(\" \")]\n",
        "\n",
        "    x = torch.tensor(x, dtype=torch.int64)\n",
        "    y = torch.tensor(y, dtype=torch.int64)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rsFqIimT7L9B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_dataset = TranslationDataset(short_subset, word2index)"
      ],
      "metadata": {
        "id": "AXROIGZbLf3A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define a collate_fn function to create one larger batch from the inputs that are different lengths\n",
        "def pad_batch(batch):\n",
        "  \"\"\"Pad items in the batch to the length of the longest item in the batch\"\"\"\n",
        "  \"\"\"\n",
        "  We actually have two different maxiumum lengths! \n",
        "  The max length of the input sequences, and the max length of the output sequences. \n",
        "  So we will determine each seperatly, and only pad the inputs/outputs by the exact amount we need\n",
        "  \"\"\"\n",
        "  max_x = max([i[0].size(0) for i in batch])\n",
        "  max_y = max([i[1].size(0) for i in batch])\n",
        "\n",
        "  PAD = word2index[PAD_token]\n",
        "\n",
        "  # We will use the F.pad function to pad each tensor to the right\n",
        "  X = [F.pad(i[0], (0, max_x[0].size(0)), value=PAD) for i in batch]\n",
        "  Y = [F.pad(i[1], (0, max_y[1].size(0)), value=PAD) for i in batch]\n",
        "\n",
        "  X, Y = torch.stack(X), torch.stack(Y)\n",
        "  return (X, Y), Y"
      ],
      "metadata": {
        "id": "5wYj9qN5YPpC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Want a consistent dataset split\n",
        "set_seed(42)\n",
        "\n",
        "B = 128\n",
        "epochs = 10\n",
        "\n",
        "train_size = round(len(big_dataset) * 0.9)\n",
        "test_size = len(big_dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(big_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, collate_fn=pad_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=B, collate_fn=pad_batch)"
      ],
      "metadata": {
        "id": "SosIhpxes8xQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inputs to Seq2Seq"
      ],
      "metadata": {
        "id": "fEYFfHdcgo78"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJyuOrxggp-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}