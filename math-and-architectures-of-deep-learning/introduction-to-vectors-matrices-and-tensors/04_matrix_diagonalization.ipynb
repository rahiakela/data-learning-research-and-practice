{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP6bt9NbQmcmVpsMor0tU1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/math-and-architectures-of-deep-learning/introduction-to-vectors-matrices-and-tensors/04_matrix_diagonalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrix Diagonalization"
      ],
      "metadata": {
        "id": "ltkjGN9-EmW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a $n \\times n$ matrix $A_{n, n}$ with $n$ linearly independent eigenvectors.\n",
        "\n",
        "Let $S_{n,n}$ be a matrix with these eigenvectors as its columns.\n",
        "\n",
        "$$Ae_1 = \\lambda_1e_1$$\n",
        "$$Ae_2 = \\lambda_2e_2$$\n",
        "$$... = ...$$\n",
        "$$Ae_n = \\lambda_ne_n$$\n",
        "\n",
        "And,\n",
        "\n",
        "$$\n",
        "S = \n",
        "\\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then,\n",
        "\n",
        "$$\n",
        "AS = \n",
        "A \\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        Ae_1 & Ae_2 & ... & Ae_n \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1e_1 & \\lambda_2e_2 & ... & \\lambda_ne_n \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}\n",
        "=S\\Sigma\n",
        "$$\n",
        "\n",
        "Where,\n",
        "\n",
        "$$\n",
        "\\Sigma = \\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "is a diagonal matrix with the eigenvalues of $A$ on the diagonal and 0 everywhere else.\n",
        "\n",
        "Thus, we have\n",
        "\n",
        "$$AS = S\\Sigma$$\n",
        "\n",
        "Which lead to,\n",
        "\n",
        "$$A=S\\Sigma S^{-1}$$\n",
        "\n",
        "and,\n",
        "\n",
        "$$\\Sigma = S^{-1}AS$$\n",
        "\n",
        "If $A$ is symmetric, then its eigenvectors are orthogonal. Then\n",
        "\n",
        "$$S^TS = SS^T = I$$\n",
        "\n",
        "So, \n",
        "\n",
        "$$S^{-1}=S^T$$\n",
        "\n",
        "and we get the diagonalization of $A$.\n",
        "\n",
        "$$\n",
        "A = S \\Sigma S^T\n",
        "$$"
      ],
      "metadata": {
        "id": "sti_nU3kR-fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "n7gMjVAehJdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.linalg as LA\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from math import cos, sin, radians\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D "
      ],
      "metadata": {
        "id": "JsRgdABiFg4F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "fGUvrmiKFj8v",
        "outputId": "f46edd1b-7cbe-472c-a79d-fcebc8560357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe2b9a61a50>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numpy matrix diagonalization"
      ],
      "metadata": {
        "id": "Eh1WCaxPhVtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us reconsider our rotation matrix."
      ],
      "metadata": {
        "id": "Gtqw_3h5EnCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diagonalise(matrix):\n",
        "  \"\"\"\n",
        "  np.linalg.eig(matrix) returns the eigenvalues of matrix in an array (first return value) \n",
        "  and the eigvectors as a matrix (each column is an eigenvector)\n",
        "  \"\"\"\n",
        "  try:\n",
        "    l, e = torch.linalg.eig(matrix)\n",
        "    # make a diagonal matrix from the eigenvalues\n",
        "    sigma = torch.diag(l)\n",
        "    # return the three factor matrices\n",
        "    return e, torch.diag(l), torch.linalg.inv(e)\n",
        "  except RuntimeError:\n",
        "    print(\"Cannot diagonalise matrix!\")"
      ],
      "metadata": {
        "id": "2F-RWUsoFoqT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([\n",
        "  [0.7071, 0.7071, 0],\n",
        "  [-0.7071, 0.7071, 0],\n",
        "  [0, 0, 1]\n",
        "])\n",
        "print(f\"A: \\n{A}\")\n",
        "\n",
        "S, sigma, S_inv = diagonalise(A)\n",
        "\n",
        "# Let us reconstruct the original matriox from its factors\n",
        "A1 = torch.matmul(S, torch.matmul(sigma, S_inv))\n",
        "print(f\"\\nS =\\n{S}\")\n",
        "print(f\"\\nsigma =\\n{sigma}\")\n",
        "print(f\"\\nS_inv =\\n{S_inv}\")\n",
        "print(f\"\\nS sigmaa S_inv =\\n{A1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrnIgp6bUijG",
        "outputId": "12acd592-c0f6-4b20-bc6e-9ff2f9c9bb06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: \n",
            "tensor([[ 0.7071,  0.7071,  0.0000],\n",
            "        [-0.7071,  0.7071,  0.0000],\n",
            "        [ 0.0000,  0.0000,  1.0000]])\n",
            "\n",
            "S =\n",
            "tensor([[0.7071+0.0000j, 0.7071-0.0000j, 0.0000+0.0000j],\n",
            "        [0.0000+0.7071j, 0.0000-0.7071j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.0000-0.0000j, 1.0000+0.0000j]])\n",
            "\n",
            "sigma =\n",
            "tensor([[0.7071+0.7071j, 0.0000+0.0000j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.7071-0.7071j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.0000+0.0000j, 1.0000+0.0000j]])\n",
            "\n",
            "S_inv =\n",
            "tensor([[0.7071+0.0000j, 0.0000-0.7071j, 0.0000-0.0000j],\n",
            "        [0.7071+0.0000j, 0.0000+0.7071j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.0000+0.0000j, 1.0000+0.0000j]])\n",
            "\n",
            "S sigmaa S_inv =\n",
            "tensor([[ 0.7071+5.9605e-08j,  0.7071+5.9605e-08j,  0.0000+0.0000e+00j],\n",
            "        [-0.7071+5.9605e-08j,  0.7071-5.9605e-08j,  0.0000+0.0000e+00j],\n",
            "        [ 0.0000+0.0000e+00j,  0.0000+0.0000e+00j,  1.0000+0.0000e+00j]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assert that the original matrix is the same as the reconstruction from the diagonal decomposition factors."
      ],
      "metadata": {
        "id": "IEjGY7Ys3pMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(A, A1.real)"
      ],
      "metadata": {
        "id": "mCdRZmN03r7h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Systems with Diagonalization"
      ],
      "metadata": {
        "id": "gofaydHqnZUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general,\n",
        "matrix inversion (i.e., computation of $A^{-1}$ is a very complex process which is numerically\n",
        "unstable. \n",
        "\n",
        "Hence, solving $Ax = b$ via $x = A^{-1}b$ is to be avoided when possible.\n",
        "\n",
        "In the particular case of a square symmetric matrix with n distinct eigenvalues, diagonalization can come to the rescue. \n",
        "\n",
        "We can solve in multiple steps: \n",
        "\n",
        "We first diagonalize $A$\n",
        "\n",
        "$$\n",
        "A = S\\Lambda S^T\n",
        "$$\n",
        "\n",
        "Then,\n",
        "\n",
        "$$Ax=b$$\n",
        "\n",
        "can be written as:\n",
        "\n",
        "$$A = S\\Lambda S^Tx=b$$\n",
        "\n",
        "where $S$ is the matrix with eigenvectors of $A$ as its columns:\n",
        "\n",
        "$$\n",
        "S = \n",
        "\\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Since $A$ is symmetric, these eigenvectors are orthogonal. Hence $S^TS=SS^T=I$.\n",
        "\n",
        "The solution can be obtained in a series of very simple steps as shown below:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "YaJbqdMQ4IL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first solve,\n",
        "\n",
        "$$Sy_1=b$$\n",
        "\n",
        "as\n",
        "\n",
        "$$y1=S^Tb$$\n",
        "\n",
        "Notice that both transpose and matrix vector multiplications are simple and numerically\n",
        "stable operations unlike matrix inversion. \n",
        "\n",
        "Then we get,\n",
        "\n",
        "$$\\Lambda (S^Tx) = y_1$$\n",
        "\n",
        "Now solve,\n",
        "\n",
        "$$\\Lambda y_2 = y_1$$\n",
        "\n",
        "as\n",
        "\n",
        "$$y_2 = \\Lambda^{-1} y_1$$\n",
        "\n",
        "Note that since $\\Lambda$ is a diagonal matrix, inverting it is trivial,\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}^{-1}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        \\frac{1}{\\lambda_1} & 0 & ... & 0 \\\\\n",
        "        0 & \\frac{1}{\\lambda_2} & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\frac{1}{\\lambda_n} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "As final step, solve,\n",
        "\n",
        "$$S^Tx=y_2$$\n",
        "\n",
        "as\n",
        "\n",
        "$$x=Sy_2$$\n",
        "\n",
        "Thus we have obtained $x$ without a single complex or unstable step.\n",
        "\n",
        "Now, let us try solving the following set of equations:\n",
        "\n",
        "\\begin{align*}\n",
        "    x + 2y + z    &= 8 \\\\\n",
        "    2x + 2y + 3z &= 15 \\\\\n",
        "    x + 3y + 3z  &= 16\n",
        "\\end{align*}\n",
        "\n",
        "This can be written using matrices and vectors as\n",
        "\\begin{equation*}\n",
        "A\\vec{x} = \\vec{b}\n",
        "\\end{equation*}\n",
        "where $A=\n",
        "\\begin{bmatrix}\n",
        "        1 & 2 & 1 \\\\\n",
        "        2 & 2 & 3 \\\\\n",
        "        1 & 3 & 3 \n",
        "\\end{bmatrix}\n",
        "\\;\\;\\;\\;\\;\\;\n",
        "\\vec{x} = \n",
        "\\begin{bmatrix}\n",
        "        x \\\\\n",
        "        y \\\\\n",
        "        z \n",
        "\\end{bmatrix}\n",
        "\\;\\;\\;\\;\\;\\;\n",
        "\\vec{b} = \n",
        "\\begin{bmatrix}\n",
        "        8 \\\\\n",
        "        15 \\\\\n",
        "        16\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "Note that $A$ is a symmetric matrix. It has orthogonal eigenvectros.\n",
        "<br> The matrix with eigenvectors of $A$ in columns is orthogonal.\n",
        "<br> Its transpose and inverse are same."
      ],
      "metadata": {
        "id": "zGFYPWOxxvAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "  [1, 2, 1],\n",
        "  [2, 2, 3],\n",
        "  [1, 3, 3]\n",
        "])\n",
        "\n",
        "# since it is symmetric matrix\n",
        "assert np.all(A == A.T)\n",
        "\n",
        "b = np.array([8, 15, 16])\n",
        "\n",
        "# One way to solve for this is to compute matrix inverse i.e x = A_inv b\n",
        "x_0 = np.matmul(np.linalg.inv(A), b)\n",
        "print(f\"Solution using inverse: {x_0}\")\n",
        "\n",
        "# Matrix inversion is a complex process that can be\n",
        "# numerically unstable. If possible we use diagonalisation.\n",
        "w, S = np.linalg.eig(A)\n",
        "\n",
        "# We know that A = S sigma S_inv, so S sigma S_inv x = b\n",
        "# sigma S_inv x =  S_inv b  ==> sigma S_inv x = S_t b\n",
        "S_inv_b = np.matmul(S.T, b)\n",
        "\n",
        "# => S_inv x = sigma_inv(S_t b)\n",
        "# Since inversion of the diagonal matrix is just division of all elements by 1, we can compute sigma_inv as follows\n",
        "sigma_inv = np.diag(1/w)\n",
        "sigma_inv_S_inv_b = np.matmul(sigma_inv, S_inv_b)\n",
        "\n",
        "# => x = S (sigma_inv(S_t b))\n",
        "x_1 = np.matmul(S, sigma_inv_S_inv_b)\n",
        "print(f\"Solution using diagonalisation: {x_1}\")\n",
        "\n",
        "assert np.allclose(x_0, x_1)"
      ],
      "metadata": {
        "id": "fLUm7vSznchk",
        "outputId": "76ecfa79-06a7-4554-a678-fe46f134feac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using inverse: [1. 2. 3.]\n",
            "Solution using diagonalisation: [1. 2. 3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrix powers using diagonalization"
      ],
      "metadata": {
        "id": "OFUGDF_9ffn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matrix $R$ is orthogonal if and only if it its transpose is also its inverse, i.e., $R^TR = RR^T =  I$.\n",
        "\n",
        "All rotations matrices are orthogonal matrices. All orthogonal matrices represent some rotation.\n",
        "\n",
        "For instance:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "        cos\\theta & -sin\\theta \\\\\n",
        "        sin\\theta & cos\\theta \\\\\n",
        "\\end{bmatrix}^T \n",
        "\\begin{bmatrix}\n",
        "        cos\\theta & -sin\\theta \\\\\n",
        "        sin\\theta & cos\\theta \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        cos\\theta & sin\\theta \\\\\n",
        "        -sin\\theta & cos\\theta \\\\\n",
        "\\end{bmatrix}^T \n",
        "\\begin{bmatrix}\n",
        "        cos\\theta & -sin\\theta \\\\\n",
        "        sin\\theta & cos\\theta \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        cos^2\\theta + sin^2\\theta & 0 \\\\\n",
        "        0 & cos^2\\theta + sin^2\\theta \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        1 & 0 \\\\\n",
        "        0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "=I\n",
        "$$\n",
        "\n",
        "**Orthogonality implies rotation is length preserving**\n",
        "\n",
        "Lengths (magnitudes) of the 2 vectors $x, y$ are equal, since it is easy to see that:\n",
        "\n",
        "$$\n",
        "||y|| = y^Ty = (Rx)^T(Rx) = x^TR^TRx = x^TIx = x^Tx = ||x||\n",
        "$$\n",
        "\n",
        "Since we know that $(AB)^T = B^TA^T$"
      ],
      "metadata": {
        "id": "V-Rw04WAfgaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R_30 = rotation_matrix_2d(30)\n",
        "print(f\"Matrix to rotate in-plane by 30 degrees about origin:\\n{R_30}\")\n",
        "\n",
        "# Inverse of rotation matrix is same as the transpose, this is orthogonality.\n",
        "assert torch.allclose(torch.linalg.inv(R_30), R_30.T)\n",
        "\n",
        "# Equivalently, if we multiply a rotation matrix and its transpose, we get the identity matrix.\n",
        "assert torch.allclose(torch.matmul(R_30, R_30.T), torch.eye(2))"
      ],
      "metadata": {
        "id": "_xiEpiY9lp5q",
        "outputId": "beed0f1a-4612-40ce-8f47-d032652e0bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix to rotate in-plane by 30 degrees about origin:\n",
            "tensor([[ 0.8660, -0.5000],\n",
            "        [ 0.5000,  0.8660]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us take a random point (4, 0)\n",
        "u = torch.tensor([[4], [0]], dtype=torch.float)\n",
        "\n",
        "# Rotate it by 30 degrees\n",
        "v = torch.matmul(R_30, u)\n",
        "print(f\"Original vector u:\\n{u}\")\n",
        "print(f\"Rotated Vector v:\\n{v}\")\n",
        "\n",
        "print(f\"Length of u: {torch.linalg.norm(u)}\")\n",
        "print(f\"Length of v: {torch.linalg.norm(v)}\")\n",
        "\n",
        "# We assert that rotation is length preserving\n",
        "assert torch.linalg.norm(u) == torch.linalg.norm(v)"
      ],
      "metadata": {
        "id": "YM-JxRl8nFMC",
        "outputId": "901575fa-d33d-41b2-c563-87a5f36d3b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vector u:\n",
            "tensor([[4.],\n",
            "        [0.]])\n",
            "Rotated Vector v:\n",
            "tensor([[3.4641],\n",
            "        [2.0000]])\n",
            "Length of u: 4.0\n",
            "Length of v: 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Negating the angle of rotation is equivalent to inverting the rotation matrix, which is equivalent to transposing the rotation matrix**\n",
        "\n",
        "For instance, consider in plane rotation. Say a point $x$ is rotated about the origin to vector $y$ via matrix $R$. Thus\n",
        "\n",
        "$$\n",
        "R = \n",
        "\\begin{bmatrix}\n",
        "        cos\\theta & -sin\\theta \\\\\n",
        "        sin\\theta & cos\\theta \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "y = Rx\n",
        "$$\n",
        "\n",
        "Now, we can go back from $y$ to $x$ by rotating by $-\\theta$.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "        cos(-\\theta) & -sin(-\\theta) \\\\\n",
        "        sin(-\\theta) & cos(-\\theta) \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        cos\\theta & sin\\theta \\\\\n",
        "        -sin\\theta & cos\\theta \\\\\n",
        "\\end{bmatrix}\n",
        "= R^T\n",
        "$$\n",
        "\n",
        "In other words, $R^T$ inverts the rotation, i.e., rotates by the negative angle.\n",
        "\n",
        "Let us now negate the rotation i.e rotate the point back by -30 degrees.\n",
        "\n"
      ],
      "metadata": {
        "id": "gTlVoA1_n988"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R_neg30 = rotation_matrix_2d(-30)\n",
        "print(f\"Matrix to rotate in-plane by -30 degrees about origin:\\n{R_neg30}\")\n",
        "\n",
        "# Rotation by negative angle is equivalent to inverse rotation\n",
        "w = torch.matmul(R_neg30, v)\n",
        "print(f\"Re-Rotated Vector w:\\n{w}\")\n",
        "\n",
        "# We assert that this vector is the same as the original vector u\n",
        "assert torch.all(w == u)\n",
        "\n",
        "# We also assert that R_neg30 is the transpose and the inverse of R_30\n",
        "assert torch.allclose(R_30, R_neg30.T)\n",
        "# Matrix that rotates by an angle is inverse of the matrix that rotates by negative of the same angle\n",
        "assert torch.allclose(torch.matmul(R_30, R_neg30), torch.eye(2))"
      ],
      "metadata": {
        "id": "eqWJV85iQuq6",
        "outputId": "9296c9a9-933c-4c90-bbcd-d1a1186ea0a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix to rotate in-plane by -30 degrees about origin:\n",
            "tensor([[ 0.8660,  0.5000],\n",
            "        [-0.5000,  0.8660]])\n",
            "Re-Rotated Vector w:\n",
            "tensor([[4.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eigenvalues and eigenvectors of the rotation matrix: How to find the axis of rotation**\n",
        "\n",
        "Let $\\lambda,  e$ be an eigenvalue, eigenvector pair of a rotation matrix $R$. Then,\n",
        "\n",
        "$$Re = \\lambda e$$\n",
        "\n",
        "After Transposing both sides,\n",
        "\n",
        "$$(Re)^T = (\\lambda e)^T$$\n",
        "$$e^TR^T = \\lambda e^T$$\n",
        "\n",
        "Multiplying left and right sides, respectively, with equivalent entities $Re$ and $\\lambda e$, we get,\n",
        "\n",
        "$$e^TR^T(Re) = \\lambda e^T(\\lambda e)$$\n",
        "$$e^T(R^TR)e = \\lambda^2 e^Te$$\n",
        "$$e^T(I)e = \\lambda^2 e^Te$$\n",
        "$$e^Te = \\lambda^2 e^Te$$\n",
        "$$ \\lambda^2 = 1$$\n",
        "$$ \\lambda = 1$$\n",
        "\n",
        "(the negative solution $\\lambda = -1$ corresponds to reflection).\n",
        "\n",
        "Thus, all rotation matrices\n",
        "will have 1 as one of its eigenvalues. The corresponding eigenvector $e$ satisfies $Re = e$.\n",
        "\n",
        "This is the axis of rotation - the set of points that stay where they were post rotation.\n",
        "\n",
        "Let us now compute the eigen values and eigen vectors of the rotation matrix.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8rm89BrsTm_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix for rotation by 45 degrees about origin\n",
        "R = torch.tensor([\n",
        "  [0.7071, 0.7071, 0], \n",
        "  [-0.7071, 0.7071, 0],\n",
        "  [0, 0, 1]\n",
        "])\n",
        "\n",
        "# As seen in the previous section, A is a rotation matrix around the Z axis\n",
        "l, e = LA.eig(R)"
      ],
      "metadata": {
        "id": "xfAF8N6PYxg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that all rotation matrices will have 1 as one of its eigenvalues.\n",
        "\n",
        "The eigen vector corresponding to that value is the axis of rotation."
      ],
      "metadata": {
        "id": "NkZkQ6DIZuuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.where returns the indices where the specified condition is satisfied\n",
        "axis_of_rotation = e[:, torch.where(l == 1.0)]\n",
        "\n",
        "# torch.squeeze is used to remove dimensions of size 1\n",
        "axis_of_rotation = torch.squeeze(axis_of_rotation)\n",
        "print(f\"Axis of rotation is: {axis_of_rotation}\")\n",
        "\n",
        "# axis of rotation is the Z-axis\n",
        "assert np.allclose(axis_of_rotation, np.array([0, 0, 1]))\n",
        "\n",
        "# Let us take a random point on the axis of rotation\n",
        "p = torch.randint(0, 10, (1, )) * axis_of_rotation  \n",
        "print(f\"Point of axis of rotation: {p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULz86MfPZzKo",
        "outputId": "22732c1b-1860-4722-e6b8-b5f3c8f34971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Axis of rotation is: tensor([0.+0.j, 0.+0.j, 1.+0.j])\n",
            "Point of axis of rotation: tensor([0.+0.j, 0.+0.j, 7.+0.j])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point on the axis of rotation remain unchanged even after rotation. \n",
        "\n",
        "Thus vector p and its transform $Rp$ are close."
      ],
      "metadata": {
        "id": "-sWWTgQPake7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(torch.matmul(R, p.real), p.real)"
      ],
      "metadata": {
        "id": "wTQ6_f62arOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}