{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3Ye6NQ8JovKZO18hx9FJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/math-and-architectures-of-deep-learning/introduction-to-vectors-matrices-and-tensors/04_matrix_diagonalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrix Diagonalization"
      ],
      "metadata": {
        "id": "ltkjGN9-EmW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a $n \\times n$ matrix $A_{n, n}$ with $n$ linearly independent eigenvectors.\n",
        "\n",
        "Let $S_{n,n}$ be a matrix with these eigenvectors as its columns.\n",
        "\n",
        "$$Ae_1 = \\lambda_1e_1$$\n",
        "$$Ae_2 = \\lambda_2e_2$$\n",
        "$$... = ...$$\n",
        "$$Ae_n = \\lambda_ne_n$$\n",
        "\n",
        "And,\n",
        "\n",
        "$$\n",
        "S = \n",
        "\\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then,\n",
        "\n",
        "$$\n",
        "AS = \n",
        "A \\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        Ae_1 & Ae_2 & ... & Ae_n \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1e_1 & \\lambda_2e_2 & ... & \\lambda_ne_n \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}\n",
        "=S\\Sigma\n",
        "$$\n",
        "\n",
        "Where,\n",
        "\n",
        "$$\n",
        "\\Sigma = \\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "is a diagonal matrix with the eigenvalues of $A$ on the diagonal and 0 everywhere else.\n",
        "\n",
        "Thus, we have\n",
        "\n",
        "$$AS = S\\Sigma$$\n",
        "\n",
        "Which lead to,\n",
        "\n",
        "$$A=S\\Sigma S^{-1}$$\n",
        "\n",
        "and,\n",
        "\n",
        "$$\\Sigma = S^{-1}AS$$\n",
        "\n",
        "If $A$ is symmetric, then its eigenvectors are orthogonal. Then\n",
        "\n",
        "$$S^TS = SS^T = I$$\n",
        "\n",
        "So, \n",
        "\n",
        "$$S^{-1}=S^T$$\n",
        "\n",
        "and we get the diagonalization of $A$.\n",
        "\n",
        "$$\n",
        "A = S \\Sigma S^T\n",
        "$$"
      ],
      "metadata": {
        "id": "sti_nU3kR-fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "n7gMjVAehJdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.linalg as LA\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from math import cos, sin, radians\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D "
      ],
      "metadata": {
        "id": "JsRgdABiFg4F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "fGUvrmiKFj8v",
        "outputId": "b4cfe2e3-6637-4a34-b6ab-f424624053d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f140900b890>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numpy matrix diagonalization"
      ],
      "metadata": {
        "id": "Eh1WCaxPhVtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us reconsider our rotation matrix."
      ],
      "metadata": {
        "id": "Gtqw_3h5EnCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diagonalise(matrix):\n",
        "  \"\"\"\n",
        "  np.linalg.eig(matrix) returns the eigenvalues of matrix in an array (first return value) \n",
        "  and the eigvectors as a matrix (each column is an eigenvector)\n",
        "  \"\"\"\n",
        "  try:\n",
        "    l, e = torch.linalg.eig(matrix)\n",
        "    # make a diagonal matrix from the eigenvalues\n",
        "    sigma = torch.diag(l)\n",
        "    # return the three factor matrices\n",
        "    return e, torch.diag(l), torch.linalg.inv(e)\n",
        "  except RuntimeError:\n",
        "    print(\"Cannot diagonalise matrix!\")"
      ],
      "metadata": {
        "id": "2F-RWUsoFoqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([\n",
        "  [0.7071, 0.7071, 0],\n",
        "  [-0.7071, 0.7071, 0],\n",
        "  [0, 0, 1]\n",
        "])\n",
        "print(f\"A: \\n{A}\")\n",
        "\n",
        "S, sigma, S_inv = diagonalise(A)\n",
        "\n",
        "# Let us reconstruct the original matriox from its factors\n",
        "A1 = torch.matmul(S, torch.matmul(sigma, S_inv))\n",
        "print(f\"\\nS =\\n{S}\")\n",
        "print(f\"\\nsigma =\\n{sigma}\")\n",
        "print(f\"\\nS_inv =\\n{S_inv}\")\n",
        "print(f\"\\nS sigmaa S_inv =\\n{A1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrnIgp6bUijG",
        "outputId": "12acd592-c0f6-4b20-bc6e-9ff2f9c9bb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: \n",
            "tensor([[ 0.7071,  0.7071,  0.0000],\n",
            "        [-0.7071,  0.7071,  0.0000],\n",
            "        [ 0.0000,  0.0000,  1.0000]])\n",
            "\n",
            "S =\n",
            "tensor([[0.7071+0.0000j, 0.7071-0.0000j, 0.0000+0.0000j],\n",
            "        [0.0000+0.7071j, 0.0000-0.7071j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.0000-0.0000j, 1.0000+0.0000j]])\n",
            "\n",
            "sigma =\n",
            "tensor([[0.7071+0.7071j, 0.0000+0.0000j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.7071-0.7071j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.0000+0.0000j, 1.0000+0.0000j]])\n",
            "\n",
            "S_inv =\n",
            "tensor([[0.7071+0.0000j, 0.0000-0.7071j, 0.0000-0.0000j],\n",
            "        [0.7071+0.0000j, 0.0000+0.7071j, 0.0000+0.0000j],\n",
            "        [0.0000+0.0000j, 0.0000+0.0000j, 1.0000+0.0000j]])\n",
            "\n",
            "S sigmaa S_inv =\n",
            "tensor([[ 0.7071+5.9605e-08j,  0.7071+5.9605e-08j,  0.0000+0.0000e+00j],\n",
            "        [-0.7071+5.9605e-08j,  0.7071-5.9605e-08j,  0.0000+0.0000e+00j],\n",
            "        [ 0.0000+0.0000e+00j,  0.0000+0.0000e+00j,  1.0000+0.0000e+00j]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assert that the original matrix is the same as the reconstruction from the diagonal decomposition factors."
      ],
      "metadata": {
        "id": "IEjGY7Ys3pMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(A, A1.real)"
      ],
      "metadata": {
        "id": "mCdRZmN03r7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Systems with Diagonalization"
      ],
      "metadata": {
        "id": "gofaydHqnZUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general,\n",
        "matrix inversion (i.e., computation of $A^{-1}$ is a very complex process which is numerically\n",
        "unstable. \n",
        "\n",
        "Hence, solving $Ax = b$ via $x = A^{-1}b$ is to be avoided when possible.\n",
        "\n",
        "In the particular case of a square symmetric matrix with n distinct eigenvalues, diagonalization can come to the rescue. \n",
        "\n",
        "We can solve in multiple steps: \n",
        "\n",
        "We first diagonalize $A$\n",
        "\n",
        "$$\n",
        "A = S\\Lambda S^T\n",
        "$$\n",
        "\n",
        "Then,\n",
        "\n",
        "$$Ax=b$$\n",
        "\n",
        "can be written as:\n",
        "\n",
        "$$A = S\\Lambda S^Tx=b$$\n",
        "\n",
        "where $S$ is the matrix with eigenvectors of $A$ as its columns:\n",
        "\n",
        "$$\n",
        "S = \n",
        "\\begin{bmatrix}\n",
        "        e_1 & e_2 & ... & e_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Since $A$ is symmetric, these eigenvectors are orthogonal. Hence $S^TS=SS^T=I$.\n",
        "\n",
        "The solution can be obtained in a series of very simple steps as shown below:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "YaJbqdMQ4IL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first solve,\n",
        "\n",
        "$$Sy_1=b$$\n",
        "\n",
        "as\n",
        "\n",
        "$$y1=S^Tb$$\n",
        "\n",
        "Notice that both transpose and matrix vector multiplications are simple and numerically\n",
        "stable operations unlike matrix inversion. \n",
        "\n",
        "Then we get,\n",
        "\n",
        "$$\\Lambda (S^Tx) = y_1$$\n",
        "\n",
        "Now solve,\n",
        "\n",
        "$$\\Lambda y_2 = y_1$$\n",
        "\n",
        "as\n",
        "\n",
        "$$y_2 = \\Lambda^{-1} y_1$$\n",
        "\n",
        "Note that since $\\Lambda$ is a diagonal matrix, inverting it is trivial,\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}^{-1}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "        \\frac{1}{\\lambda_1} & 0 & ... & 0 \\\\\n",
        "        0 & \\frac{1}{\\lambda_2} & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\frac{1}{\\lambda_n} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "As final step, solve,\n",
        "\n",
        "$$S^Tx=y_2$$\n",
        "\n",
        "as\n",
        "\n",
        "$$x=Sy_2$$\n",
        "\n",
        "Thus we have obtained $x$ without a single complex or unstable step.\n",
        "\n",
        "Now, let us try solving the following set of equations:\n",
        "\n",
        "\\begin{align*}\n",
        "    x + 2y + z    &= 8 \\\\\n",
        "    2x + 2y + 3z &= 15 \\\\\n",
        "    x + 3y + 3z  &= 16\n",
        "\\end{align*}\n",
        "\n",
        "This can be written using matrices and vectors as\n",
        "\\begin{equation*}\n",
        "A\\vec{x} = \\vec{b}\n",
        "\\end{equation*}\n",
        "where $A=\n",
        "\\begin{bmatrix}\n",
        "        1 & 2 & 1 \\\\\n",
        "        2 & 2 & 3 \\\\\n",
        "        1 & 3 & 3 \n",
        "\\end{bmatrix}\n",
        "\\;\\;\\;\\;\\;\\;\n",
        "\\vec{x} = \n",
        "\\begin{bmatrix}\n",
        "        x \\\\\n",
        "        y \\\\\n",
        "        z \n",
        "\\end{bmatrix}\n",
        "\\;\\;\\;\\;\\;\\;\n",
        "\\vec{b} = \n",
        "\\begin{bmatrix}\n",
        "        8 \\\\\n",
        "        15 \\\\\n",
        "        16\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "Note that $A$ is a symmetric matrix. It has orthogonal eigenvectros.\n",
        "<br> The matrix with eigenvectors of $A$ in columns is orthogonal.\n",
        "<br> Its transpose and inverse are same."
      ],
      "metadata": {
        "id": "zGFYPWOxxvAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "  [1, 2, 1],\n",
        "  [2, 2, 3],\n",
        "  [1, 3, 3]\n",
        "])\n",
        "\n",
        "# since it is symmetric matrix\n",
        "assert np.all(A == A.T)\n",
        "\n",
        "b = np.array([8, 15, 16])\n",
        "\n",
        "# One way to solve for this is to compute matrix inverse i.e x = A_inv b\n",
        "x_0 = np.matmul(np.linalg.inv(A), b)\n",
        "print(f\"Solution using inverse: {x_0}\")\n",
        "\n",
        "# Matrix inversion is a complex process that can be\n",
        "# numerically unstable. If possible we use diagonalisation.\n",
        "w, S = np.linalg.eig(A)\n",
        "\n",
        "# We know that A = S sigma S_inv, so S sigma S_inv x = b\n",
        "# sigma S_inv x =  S_inv b  ==> sigma S_inv x = S_t b\n",
        "S_inv_b = np.matmul(S.T, b)\n",
        "\n",
        "# => S_inv x = sigma_inv(S_t b)\n",
        "# Since inversion of the diagonal matrix is just division of all elements by 1, we can compute sigma_inv as follows\n",
        "sigma_inv = np.diag(1/w)\n",
        "sigma_inv_S_inv_b = np.matmul(sigma_inv, S_inv_b)\n",
        "\n",
        "# => x = S (sigma_inv(S_t b))\n",
        "x_1 = np.matmul(S, sigma_inv_S_inv_b)\n",
        "print(f\"Solution using diagonalisation: {x_1}\")\n",
        "\n",
        "assert np.allclose(x_0, x_1)"
      ],
      "metadata": {
        "id": "fLUm7vSznchk",
        "outputId": "76ecfa79-06a7-4554-a678-fe46f134feac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using inverse: [1. 2. 3.]\n",
            "Solution using diagonalisation: [1. 2. 3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrix powers using diagonalization"
      ],
      "metadata": {
        "id": "OFUGDF_9ffn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If matrix $A$ can be diagonalized then,\n",
        "\n",
        "$$A = S\\Lambda S^{-1}$$\n",
        "\n",
        "$$\n",
        "A^2 = S\\Lambda S^{-1} S\\Lambda S^{-1}\n",
        "=\n",
        "A = S\\Lambda I \\Lambda S^{-1}=A = S\\Lambda^2 S^{-1}\n",
        "$$\n",
        "\n",
        "$$A^n = ... = ... = S\\Lambda^n S^{-1}$$\n",
        "\n",
        "For a diagonal matrix:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1 & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2 & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The $n^{th}$ power is simply,\n",
        "\n",
        "$$\n",
        "\\Lambda^n= \n",
        "\\begin{bmatrix}\n",
        "        \\lambda_1^n & 0 & ... & 0 \\\\\n",
        "        0 & \\lambda_2^n & ... & 0 \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        ... & ... & ... \\\\\n",
        "        0 & 0 & ... & \\lambda_n^n \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "If one needs to compute various powers of a `m x m` matrix A at various times, one\n",
        "should pre-compute the matrix S and compute any power with only $O(m)$ operations - compared to $(nm^3)$ operations necessary for naive computations."
      ],
      "metadata": {
        "id": "V-Rw04WAfgaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([\n",
        "  [1, 2, 1],\n",
        "  [2, 2, 3],\n",
        "  [1, 3, 3]\n",
        "], dtype=torch.float)\n",
        "\n",
        "def brute_force_matrix_power(matrix, y):\n",
        "  \"\"\"Returns matrix raised to the power of y\"\"\"\n",
        "  # Assert that it is a square matrix\n",
        "  assert len(matrix.shape) == 2 and matrix.shape[0] == matrix.shape[1]\n",
        "  assert type(y) == int\n",
        "\n",
        "  if y == 0:\n",
        "    # Return identity matrix \n",
        "    return torch.eye(matrix.shape[0])\n",
        "  output_matrix = torch.clone(matrix)\n",
        "\n",
        "  for i in range(y - 1):\n",
        "    output_matrix = torch.matmul(matrix, output_matrix)\n",
        "  return output_matrix"
      ],
      "metadata": {
        "id": "_xiEpiY9lp5q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_power_using_diag(matrix, y):\n",
        "  \"\"\"Returns matrix raised to the power of y\"\"\"\n",
        "  # Assert that it is a square matrix\n",
        "  assert len(matrix.shape) == 2 and matrix.shape[0] == matrix.shape[1]\n",
        "  assert type(y) == int\n",
        "\n",
        "  if y == 0:\n",
        "    # Return identity matrix \n",
        "    return torch.eye(matrix.shape[0])\n",
        "\n",
        "  w, v = torch.linalg.eig(matrix)\n",
        "  # Compute w^y\n",
        "  w_y = w ** float(y)\n",
        "\n",
        "  return torch.matmul(v, torch.matmul(torch.diag(w_y), torch.linalg.inv(v)))"
      ],
      "metadata": {
        "id": "nyKI5F_J_6WX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One way to compute A**2 would be to multiply A by A\n",
        "# This is an O(nm^3) operation\n",
        "A_2_brute_force = brute_force_matrix_power(A, 2)\n",
        "\n",
        "# We can instead diagonalise the matrix and compute A ** 2\n",
        "# This reduces the complexity to O(m)\n",
        "A_2_diag = matrix_power_using_diag(A, 2)\n",
        "\n",
        "print(f\"A:\\n{A}\")\n",
        "print(f\"A**2 computed via brute force:\\n{A_2_brute_force}\")\n",
        "print(f\"A**2 computed via diagonalisation:\\n{A_2_diag}\")\n",
        "\n",
        "# We assert that A**2 computed via brute force and diagonalisation is identical\n",
        "assert torch.allclose(A_2_brute_force, A_2_diag.real)"
      ],
      "metadata": {
        "id": "YM-JxRl8nFMC",
        "outputId": "e350c9c9-149b-4c1a-ec69-c2204687b97d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            "tensor([[1., 2., 1.],\n",
            "        [2., 2., 3.],\n",
            "        [1., 3., 3.]])\n",
            "A**2 computed via brute force:\n",
            "tensor([[ 6.,  9., 10.],\n",
            "        [ 9., 17., 17.],\n",
            "        [10., 17., 19.]])\n",
            "A**2 computed via diagonalisation:\n",
            "tensor([[ 6.0000+0.j,  9.0000+0.j, 10.0000+0.j],\n",
            "        [ 9.0000+0.j, 17.0000+0.j, 17.0000+0.j],\n",
            "        [10.0000+0.j, 17.0000+0.j, 19.0000+0.j]])\n"
          ]
        }
      ]
    }
  ]
}