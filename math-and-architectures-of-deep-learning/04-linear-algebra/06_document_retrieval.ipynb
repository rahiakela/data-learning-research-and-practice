{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNq/fEQPzX87KgdLdWF73Vu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/math-and-architectures-of-deep-learning/04-linear-algebra/06_document_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Document Retrieval"
      ],
      "metadata": {
        "id": "TqGFkXWmLEas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose, we have a set of documents ${d_0,...,d_6}$.\n",
        "\n",
        "Now given an incoming query\n",
        "phrase, we have to retrieve documents that match the query phrase. We will use bag\n",
        "of words model - i.e., our matching approach does not pay attention to where a word\n",
        "appears in a document, it simply pays attention to how many times a word appears in\n",
        "a document.\n",
        "\n",
        "Our documents are:\n",
        "\n",
        "* $d_0$:Roses are lovely. Nobody hates roses.\n",
        "* $d_1$:**Gun violence** has reached an epidemic proportion in America.\n",
        "* $d_2$:The issue of **gun violence** is really over-hyped. One can find many instances of **violence** where no **guns** were involved.\n",
        "* $d_3$:**Guns** are for violence prone people. **Violence** begets **guns**. **Guns** beget **violence**.\n",
        "* $d_4$:I like **guns** but I hate **violence**. I have never been involved in **violence**. But I own many **guns**. **Gun violence** is incomprehensible to me. I do believe **gun** owners are the most anti **violence** people on the planet. He who never uses a **gun** will be prone to senseless **violence**.\n",
        "* $d_5$:**Guns** were used in a armed robbery in San Francisco last night.\n",
        "* $d_6$:Acts of **violence** usually involve a weapon.\n",
        "\n",
        "Let us look at a toy dataset. Rows correspond to documents. Columns correspond to terms. Each cell\n",
        "contains the term frequency. The terms “Gun” and “Violence” occur equal number of times in most documents, indicating clear correlation.\n",
        "\n",
        "\\begin{vmatrix}\n",
        "  & violence & gun & america & roses \\\\\n",
        "  d_{0} & 0 & 0 & 0 & 2 \\\\\n",
        "  d_{1} & 1 & 1 & 1 & 0 \\\\\n",
        "  d_{2} & 2 & 2 & 0 & 0 \\\\\n",
        "  d_{3} & 3 & 3 & 0 & 0 \\\\\n",
        "  d_{4} & 5 & 5 & 0 & 0 \\\\\n",
        "  d_{5} & 0 & 1 & 0 & 0 \\\\\n",
        "  d_{6} & 1 & 0 & 0 & 0 \\\\\n",
        "\\end{vmatrix}\n",
        "\n",
        "Cosine similarity between document vectors is often used to measure similarity between two documents. Cosine Similarity only considers direct overlap of terms. \n",
        "\n",
        "The terms \"Gun\" and \"violence\" have clear correlation (they appear together in many other documents, so documents containing \"Gun\" should be similar to documents containing \"violence\"). \n",
        "\n",
        "Cosine Similarity will not see that. LSA wiil. In LSA terms often occuring together become part of the same topic. \n",
        "\n",
        "Documents are projected into topic space - e.g., \"Gun-violence\" is a topic - where indirect similarities are visible."
      ],
      "metadata": {
        "id": "LZE15D3xLGTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "2blbil6DYAlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ZqS2RT5VYEKh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return torch.dot(vec_1, vec_2) / (torch.linalg.norm(vec_1) * torch.linalg.norm(vec_2))"
      ],
      "metadata": {
        "id": "bU_qcIDDiB_W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVD and LSA"
      ],
      "metadata": {
        "id": "nKcEnHjqd9V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # consider only 4 terms for simplicity\n",
        "terms = [\"violence\", \"gun\", \"america\", \"roses\"]\n",
        "\n",
        "# pre-computed document term matrix: Each row describes a document and Each column contains TF scores for one term. IDF is ignored for simplicity\n",
        "doc_term_matrix = torch.tensor([\n",
        "  [0, 0, 0, 2], \n",
        "  [1, 1, 1, 0], \n",
        "  [2, 2, 0, 0], \n",
        "  [3, 3, 0, 0], \n",
        "  [5, 5, 0, 0], \n",
        "  [0, 1, 0, 0], \n",
        "  [1, 0, 0, 0]\n",
        "]).float()"
      ],
      "metadata": {
        "id": "S_kcGIfBYKJ2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform SVD on the doc-term matrix. The columns of\n",
        "resulting matrix `V` correspond to topics. \n",
        "\n",
        "These are eigenvectors of $X^TX$, i.e., principal vectors of the\n",
        "doc-term matrix. \n",
        "\n",
        "Thus, a topic corresponds to the direction of maximum variance in doc feature\n",
        "space."
      ],
      "metadata": {
        "id": "LjOcwaCZg9Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us perform SVD\n",
        "U, S, V_t = torch.linalg.svd(doc_term_matrix)\n",
        "print(f\"Principal Values: {S[0]:.2f} {S[1]:.2f} {S[2]:.2f} {S[3]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-eV429ZemgB",
        "outputId": "07ef7fc7-eda2-4d4f-96d2-3fc01b2d31c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Values: 8.89 2.00 1.00 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns of V are the topic vectors. Each topic vector can be seen as a weighted sum of the terms in our vocabulary."
      ],
      "metadata": {
        "id": "i2Qi0ZV5f1NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = V_t.T"
      ],
      "metadata": {
        "id": "AsLlZSVEf3b8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`S` indicates the diagonal matrix of principal values. These\n",
        "signify topic weights ( importance). \n",
        "\n",
        "We choose a cut-off and discard all topics below that weight - dimensionality reduction.\n",
        "\n",
        "Let us reduce this to a lower rank representation.\n",
        "\n",
        "There is a big  drop in principal value from `S[0]` to `S[1]`.Hence, we choose to cutoff all principal vectors beyong `V[0]`.\n",
        "\n",
        "We will retain only the first column of `V`, the principal axis. "
      ],
      "metadata": {
        "id": "vJ8IzRkffarl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank = 1\n",
        "U = U[:, :rank]\n",
        "V = V[:, :rank]"
      ],
      "metadata": {
        "id": "_HhN93KKfmAO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have reduced the dimensionality to only contain one topic, let let us look at the weighted contributions of terms to this topic.\n",
        "\n",
        "Note that both violence and gun have every high affinity, and contribute equally to this topic."
      ],
      "metadata": {
        "id": "PDLd6fzIgCt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term_topic_affinity = list(zip(terms, V[:, 0]))\n",
        "print(term_topic_affinity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RV-T1kGgGBh",
        "outputId": "10073a93-c440-4e5b-de11-28112c1c2a78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('violence', tensor(-0.7070)), ('gun', tensor(-0.7070)), ('america', tensor(-0.0181)), ('roses', tensor(1.1381e-09))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us consider 2 documents $d_5$ and $d_6$. \n",
        "\n",
        "Note that the similarity between the two documents is 0 even though intuitively they are similar."
      ],
      "metadata": {
        "id": "8aOKAKULiOzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d5_d6_similarity = cosine_similarity(doc_term_matrix[5], doc_term_matrix[6])\n",
        "assert d5_d6_similarity == 0\n",
        "print(f\"Cosine similarity between document 5 and document 6 in original space is {d5_d6_similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjT3HbRriJbY",
        "outputId": "003c74c8-affb-42fa-f3fd-bc5c2332efce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between document 5 and document 6 in original space is 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us instead look at the document representation in the topic space.\n",
        "\n",
        "We notice in this new space, documents 5 and 6 are close."
      ],
      "metadata": {
        "id": "lqi6smw7iuns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_topic_matrix = torch.matmul(doc_term_matrix, V)\n",
        "d5_d6_similarity = cosine_similarity(doc_topic_matrix[5], doc_topic_matrix[6])\n",
        "print(f\"LSA topic based Cosine similarity between document 5 and document 6 is {d5_d6_similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRv3h_kYixNp",
        "outputId": "31bfddfa-46f5-4c37-b761-63c6308fb91b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSA topic based Cosine similarity between document 5 and document 6 is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cosine similarity between document vectors does not look at such secondary evidence.\n",
        "\n",
        "This is the blind spot that LSA tries to overcome.\n",
        "\n",
        "Words are known by the company they keep. This means, if terms appear together in many\n",
        "documents, they are likely to share some semantic similarity. \n",
        "\n",
        "For instance, guns and violence\n",
        "in the above examples. Such terms should be grouped together into a common\n",
        "pool of semantically similar terms. We will call this pool a topic. \n",
        "\n",
        "Document similarity\n",
        "should be measured in terms of common topics rather than common terms. \n",
        "\n",
        "Thus, we\n",
        "have expanded the notion of shared terms between documents to shared topics between\n",
        "documents."
      ],
      "metadata": {
        "id": "jyhPSxkMnYfG"
      }
    }
  ]
}