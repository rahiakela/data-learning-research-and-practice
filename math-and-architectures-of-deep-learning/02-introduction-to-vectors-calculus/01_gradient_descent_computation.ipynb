{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOCFo8nUFImonyPcevaWm0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/math-and-architectures-of-deep-learning/02-introduction-to-vectors-calculus/01_gradient_descent_computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Descent Computation"
      ],
      "metadata": {
        "id": "j_6fwQtSMFfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, we identify the input and output variables\n",
        "pertaining to the problem at hand and cast the problem as\n",
        "generating outputs from input variables. All the inputs are\n",
        "represented together by the vector $\\vec{x}$. Sometimes there\n",
        "are multiple outputs, sometimes single output. Accordingly,\n",
        "we have an output vector $\\vec{y}$ or output scalar $y$.\n",
        "Let us denote the function that generates the output from input\n",
        " vector as $f$, i.e., $y = f\\left(\\vec{x}\\right)$.\n",
        "\n",
        "In real life problems, we do not know $f$. The crux of machine\n",
        "learning is to estimate $f$ from a set of observed inputs\n",
        "$\\vec{x}_{i}$ and their corresponding outputs $y_{i}$.\n",
        "Each observation can be depicted as a pair $\\langle\\vec{x}_{i}, y_{i}\\rangle$.\n",
        "We model the unknown function $f$ with a known function $\\phi$.\n",
        "$\\phi$ is a parameterized function. Alhtough the nature of $\\phi$\n",
        "is known, its parameter values are unknown. These parameter values\n",
        " are \"learnt\" via training. This means, we estimate the parameter\n",
        "values such that the overall error on the observations is minimized.\n",
        "\n",
        "If $\\vec{w}, b$ denotes the current set of parameters (weights, bias), then the model will\n",
        "output $\\phi\\left(\\vec{x}_{i}, \\vec{w}, b\\right)$ on the observed input $\\vec{x}_{i}$.\n",
        "Thus the error on this $i^{th}$ observation is $e_{i}^{2}=\\left(\\phi\\left(\\vec{x}_{i}, \\vec{w}, b\\right) - y_{i}\\right)^{2}$.\n",
        "We can batch up several observations and add up the error into a batch error\n",
        "$L = \\sum_{i=0}^{i=N}\\left(e^{\\left(i\\right)}\\right)^{2}$.\n",
        "\n",
        "The error is a function of the parameter set $\\vec{w}$.\n",
        "The question is: how do we adjust $\\vec{w}$ so that the error $e_{i}^{2}$ decreases.\n",
        "We know a function's value changes most when we move along the direction of\n",
        "of the gradient of the parameters. Hence, we adjust the parameters\n",
        "$\\vec{w}, b$ as\n",
        "$\\begin{bmatrix}\n",
        "\\vec{w}\\\\b\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "\\vec{w}\\\\b\n",
        "\\end{bmatrix} - \\mu \\nabla_{\\vec{w}, b}L\\left(\\vec{w}, b\\right)$.\n",
        "Each adjustment reduces the error. Starting from a random set of parameter values\n",
        "doing this \"sufficiently\" large number of times yields the desired model.\n",
        "\n",
        "A simple and popular model $\\phi$ is the linear function (predicted value is\n",
        "dot product between input and parameters plus bias):\n",
        "$\\tilde{y}_{i} = \\phi\\left(\\vec{x}_{i}, \\vec{w}, b\\right) = \\vec{w}^{T}\\vec{x} + b\n",
        "= \\sum_{j}w_{j}x_{j} + b$.\n",
        "In the example below, this is the model architecture used.\n",
        "\n",
        "Thus \n",
        "\\begin{align*}\n",
        "L &= \\sum_{i=0}^{i=N}\\left(e^{\\left(i\\right)}\\right)^{2}\\\\\n",
        "  &= \\sum_{i=0}^{i=N}\\left(\\vec{w}^{T}\\vec{x} + b - y_{i}\\right)^{2}\\\\\n",
        "\\nabla_{\\vec{w}, b}L &\\propto \\sum_{i=0}^{i=N}\\left(\\vec{w}^{T}\\vec{x}_{i} + b - y_{i}\\right)\\vec{x}_{i} \\\\\n",
        "                     &\\propto \\sum_{i=0}^{i=N}\\left(\\tilde{y}_{i} - y_{i}\\right)\\vec{x}_{i}\n",
        "\\end{align*}\n",
        "Our initial implementation will simply mimic this formula.\n",
        "For more complicated models $\\phi$ (with millions of parameters and non-linearities)\n",
        "we cannot obtain closed form gradients like this.\n",
        "\n",
        "The next example, based on NumPy and PyTorch, relies on PyTorch's\n",
        "autograd (automatic gradient computation) which does not have this limitation."
      ],
      "metadata": {
        "id": "e3XIplkKMLBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "eZPtNLyZM0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pCNUjg1tM2Gy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(params, learning_rate):\n",
        "  \"\"\"\n",
        "  Update the current weight and bias values\n",
        "  from gradient values.\n",
        "  \"\"\"\n",
        "  # Don't track gradients while updating params\n",
        "  with torch.no_grad():\n",
        "      for i, p in enumerate(params):\n",
        "          params[i] = p - learning_rate * p.grad\n",
        "          \n",
        "  # Restore tracking of gradient for all params\n",
        "  for i in range(len(params)):\n",
        "      params[i].requires_grad = True\n",
        "\n",
        "\n",
        "def draw_line(m, c, min_x=0, max_x=10, color='magenta', label=None):\n",
        "  \"\"\"\n",
        "  Plots y = mx + c from interval (min_x to max_x)\n",
        "  \"\"\"\n",
        "  # linspace creates an array of equally spaced\n",
        "  # values between the specified min and max in \n",
        "  # specified number of steps.\n",
        "  x = np.linspace(min_x, max_x, 100)\n",
        "  y = m*x + c\n",
        "  \n",
        "  plt.plot(x, y, color=color, \n",
        "            label='y=%0.2fx+%0.2f'%(m, c)\\\n",
        "                if not label else label)\n",
        "    \n",
        "def draw_parabola(w0, w1, w2,  min_x=0, max_x=10, color='magenta', label=None):\n",
        "  \"\"\"\n",
        "  Plots y = w0 + w1*x + w2*x^2 from interval\n",
        "  (min_x to max_x)\n",
        "  \"\"\"\n",
        "  x = np.linspace(min_x, max_x, 100)\n",
        "  y = w0 + w1*x +  w2* (x**2)\n",
        "  plt.plot(x, y, color=color, \n",
        "            label='y=%0.2f+ %0.2fx + %0.2fx^2'\n",
        "            %(w0, w1, w2) if not label else label)\n",
        "    \n",
        "def draw_subplot(pos, step, true_draw_func, true_draw_params, pred_draw_func, pred_draw_params):\n",
        "  \"\"\"\n",
        "  Plots the curves corresponding to a specified pair of functions.\n",
        "  We use it to plot\n",
        "  (i) the true function (used to generate the observations\n",
        "      that we are trying to predict with a trained mode) vis a vis\n",
        "  (ii) the model function (used to makes the predictions)\n",
        "        When the predictor is good, the two plots should more or less coincide.\n",
        "  Thus this is used to visualize the goodness of the current approximation.\n",
        "  \"\"\"\n",
        "  plt.subplot(2, 2, pos)\n",
        "  plt.title('Step %d'%(step))\n",
        "  true_draw_func(**true_draw_params)\n",
        "  pred_draw_func(**pred_draw_params)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend(loc='upper left')"
      ],
      "metadata": {
        "id": "Q6ID6ngHNBaV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent (manually) via NumPy"
      ],
      "metadata": {
        "id": "Tv__HWosNiuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# generate random input values\n",
        "N = 100\n",
        "x = 10 * np.random.randn(N)\n",
        "\n",
        "# Compute function outputs\n",
        "y = 1.5 * x + 2.73\n",
        "\n",
        "# Add random noise to get the observed value of y\n",
        "y_obs = y + (0.5 * np.random.randn(N))\n",
        "\n",
        "# Random initialization of model parameters\n",
        "w = np.random.randn(1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "# Training: repeatative adjustment of parameters via gradient.\n",
        "num_steps = 4000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "for step in range(num_steps):\n",
        "  # our model - initialized with arbitrary parameter values\n",
        "  y_pred = w * x + b\n",
        "  mean_squared_error = np.mean(y_pred - y_obs) ** 2\n",
        "\n",
        "  # Gradient of error computation using calculus formulas.\n",
        "  w_grad = np.mean(2 * ((y_pred - y_obs) * x))\n",
        "  b_grad = np.mean(2 * (y_pred - y_obs))\n",
        "  w = w - learning_rate * w_grad\n",
        "  b = b - learning_rate * b_grad\n",
        "\n",
        "  # periodically print diagnostic messages\n",
        "  if step % 400 == 0:\n",
        "    print(f\"Step {step}: w = {w} b = {b} \\nMSE Error = {mean_squared_error}\")\n",
        "    print(f\"Gradient of w: {w_grad} \\nGradient of b: {b_grad}\")\n",
        "\n",
        "print(\"True function: y = 1.5*x + 2.73\")\n",
        "print(f\"Learnt function: y_pred = {w[0]}*x + {b[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xvv120SNmMU",
        "outputId": "19cab019-eca0-4b63-f794-b0e6af5c97af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: w = [0.01667435] b = [-0.23112257] \n",
            "MSE Error = 17.042912303509105\n",
            "Gradient of w: -385.85618670584734 \n",
            "Gradient of b: -8.256612453908469\n",
            "Step 400: w = [1.5137106] b = [1.42144298] \n",
            "MSE Error = 1.8064489836696587\n",
            "Gradient of w: 0.015926565641006717 \n",
            "Gradient of b: -2.6880840639159027\n",
            "Step 800: w = [1.50932594] b = [2.16148491] \n",
            "MSE Error = 0.36620643215003873\n",
            "Gradient of w: 0.007170876936491713 \n",
            "Gradient of b: -1.2102998506982288\n",
            "Step 1200: w = [1.50735176] b = [2.49468604] \n",
            "MSE Error = 0.07423799518303086\n",
            "Gradient of w: 0.003228660666584702 \n",
            "Gradient of b: -0.5449330057283404\n",
            "Step 1600: w = [1.5064629] b = [2.64470861] \n",
            "MSE Error = 0.015049653542234021\n",
            "Gradient of w: 0.0014536924552342434 \n",
            "Gradient of b: -0.24535405879857802\n",
            "Step 2000: w = [1.50606269] b = [2.71225571] \n",
            "MSE Error = 0.003050891543917218\n",
            "Gradient of w: 0.0006545196205436543 \n",
            "Gradient of b: -0.11046975231106872\n",
            "Step 2400: w = [1.5058825] b = [2.74266854] \n",
            "MSE Error = 0.0006184819595096562\n",
            "Gradient of w: 0.0002946950244739899 \n",
            "Gradient of b: -0.04973859505493319\n",
            "Step 2800: w = [1.50580137] b = [2.75636181] \n",
            "MSE Error = 0.0001253797222001173\n",
            "Gradient of w: 0.00013268533857496934 \n",
            "Gradient of b: -0.02239461740687858\n",
            "Step 3200: w = [1.50576484] b = [2.76252715] \n",
            "MSE Error = 2.541719204140248e-05\n",
            "Gradient of w: 5.97410801282372e-05 \n",
            "Gradient of b: -0.010083093184415678\n",
            "Step 3600: w = [1.50574839] b = [2.76530307] \n",
            "MSE Error = 5.152616706539319e-06\n",
            "Gradient of w: 2.6898199093077225e-05 \n",
            "Gradient of b: -0.004539875199403314\n",
            "True function: y = 1.5*x + 2.73\n",
            "Learnt function: y_pred = 1.5057409986183152*x + 2.7665508770678433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent (manually) via PyTorch"
      ],
      "metadata": {
        "id": "1OTWG8w2QwHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# generate random input values\n",
        "N = 100\n",
        "x = 10 * torch.randn(N)\n",
        "\n",
        "# Compute function outputs\n",
        "y = 1.5 * x + 2.73\n",
        "\n",
        "# Add random noise to get the observed value of y\n",
        "y_obs = y + (0.5 * torch.randn(N))\n",
        "\n",
        "# Random initialization of model parameters\n",
        "w = torch.randn(1)\n",
        "b = torch.randn(1)\n",
        "\n",
        "# Training: repeatative adjustment of parameters via gradient.\n",
        "num_steps = 4000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "for step in range(num_steps):\n",
        "  # our model - initialized with arbitrary parameter values\n",
        "  y_pred = w * x + b\n",
        "  mean_squared_error = torch.mean(y_pred - y_obs) ** 2\n",
        "\n",
        "  # Gradient of error computation using calculus formulas.\n",
        "  w_grad = torch.mean(2 * ((y_pred - y_obs) * x))\n",
        "  b_grad = torch.mean(2 * (y_pred - y_obs))\n",
        "  w = w - learning_rate * w_grad\n",
        "  b = b - learning_rate * b_grad\n",
        "\n",
        "  # periodically print diagnostic messages\n",
        "  if step % 400 == 0:\n",
        "    print(f\"Step {step}: w = {w} b = {b} \\nMSE Error = {mean_squared_error}\")\n",
        "    print(f\"Gradient of w: {w_grad} \\nGradient of b: {b_grad}\")\n",
        "\n",
        "print(\"True function: y = 1.5*x + 2.73\")\n",
        "print(f\"Learnt function: y_pred = {w[0]}*x + {b[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0auxLj-QyLm",
        "outputId": "9a4d0032-df87-41ed-e071-45d35dbd33fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: w = tensor([0.6108]) b = tensor([-0.2075]) \n",
            "MSE Error = 13.136869430541992\n",
            "Gradient of w: -217.66844177246094 \n",
            "Gradient of b: -7.248963832855225\n",
            "Step 400: w = tensor([1.5089]) b = tensor([1.4195]) \n",
            "MSE Error = 1.7582067251205444\n",
            "Gradient of w: 0.016568755730986595 \n",
            "Gradient of b: -2.6519477367401123\n",
            "Step 800: w = tensor([1.5043]) b = tensor([2.1497]) \n",
            "MSE Error = 0.3565356135368347\n",
            "Gradient of w: 0.007476482540369034 \n",
            "Gradient of b: -1.1942120790481567\n",
            "Step 1200: w = tensor([1.5023]) b = tensor([2.4785]) \n",
            "MSE Error = 0.07230029255151749\n",
            "Gradient of w: 0.0033882809802889824 \n",
            "Gradient of b: -0.5377742648124695\n",
            "Step 1600: w = tensor([1.5013]) b = tensor([2.6265]) \n",
            "MSE Error = 0.014661334455013275\n",
            "Gradient of w: 0.0015091609675437212 \n",
            "Gradient of b: -0.24216799437999725\n",
            "Step 2000: w = tensor([1.5009]) b = tensor([2.6932]) \n",
            "MSE Error = 0.0029730559326708317\n",
            "Gradient of w: 0.0006666564731858671 \n",
            "Gradient of b: -0.10905147343873978\n",
            "Step 2400: w = tensor([1.5007]) b = tensor([2.7232]) \n",
            "MSE Error = 0.0006029126816429198\n",
            "Gradient of w: 0.00030168532975949347 \n",
            "Gradient of b: -0.049108561128377914\n",
            "Step 2800: w = tensor([1.5007]) b = tensor([2.7368]) \n",
            "MSE Error = 0.00012226247054059058\n",
            "Gradient of w: 0.00017433166794944555 \n",
            "Gradient of b: -0.022114472463726997\n",
            "Step 3200: w = tensor([1.5006]) b = tensor([2.7428]) \n",
            "MSE Error = 2.4792365366010927e-05\n",
            "Gradient of w: 7.095337059581652e-05 \n",
            "Gradient of b: -0.009958386421203613\n",
            "Step 3600: w = tensor([1.5006]) b = tensor([2.7456]) \n",
            "MSE Error = 5.0281782932870556e-06\n",
            "Gradient of w: 5.92327123740688e-05 \n",
            "Gradient of b: -0.004484720062464476\n",
            "True function: y = 1.5*x + 2.73\n",
            "Learnt function: y_pred = 1.5005970001220703*x + 2.746823787689209\n"
          ]
        }
      ]
    }
  ]
}