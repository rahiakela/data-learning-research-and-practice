{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNP5Pvr8F18um+nK7ydFGiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-research-and-practice/blob/main/math-and-architectures-of-deep-learning/02-introduction-to-vectors-calculus/03_cat_brain_linear_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cat brain linear model"
      ],
      "metadata": {
        "id": "j_6fwQtSMFfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, we identify the input and output variables\n",
        "pertaining to the problem at hand and cast the problem as\n",
        "generating outputs from input variables. All the inputs are\n",
        "represented together by the vector $\\vec{x}$. Sometimes there\n",
        "are multiple outputs, sometimes single output. Accordingly,\n",
        "we have an output vector $\\vec{y}$ or output scalar $y$.\n",
        "Let us denote the function that generates the output from input\n",
        " vector as $f$, i.e., $y = f\\left(\\vec{x}\\right)$.\n",
        "\n",
        "In real life problems, we do not know $f$. The crux of machine\n",
        "learning is to estimate $f$ from a set of observed inputs\n",
        "$\\vec{x}_{i}$ and their corresponding outputs $y_{i}$.\n",
        "Each observation can be depicted as a pair $\\langle\\vec{x}_{i}, y_{i}\\rangle$.\n",
        "We model the unknown function $f$ with a known function $\\phi$.\n",
        "$\\phi$ is a parameterized function. Alhtough the nature of $\\phi$\n",
        "is known, its parameter values are unknown. These parameter values\n",
        " are \"learnt\" via training. This means, we estimate the parameter\n",
        "values such that the overall error on the observations is minimized.\n",
        "\n",
        "If $\\vec{w}, b$ denotes the current set of parameters (weights, bias), then the model will\n",
        "output $\\phi\\left(\\vec{x}_{i}, \\vec{w}, b\\right)$ on the observed input $\\vec{x}_{i}$.\n",
        "Thus the error on this $i^{th}$ observation is $e_{i}^{2}=\\left(\\phi\\left(\\vec{x}_{i}, \\vec{w}, b\\right) - y_{i}\\right)^{2}$.\n",
        "We can batch up several observations and add up the error into a batch error\n",
        "$L = \\sum_{i=0}^{i=N}\\left(e^{\\left(i\\right)}\\right)^{2}$.\n",
        "\n",
        "The error is a function of the parameter set $\\vec{w}$.\n",
        "The question is: how do we adjust $\\vec{w}$ so that the error $e_{i}^{2}$ decreases.\n",
        "We know a function's value changes most when we move along the direction of\n",
        "of the gradient of the parameters. Hence, we adjust the parameters\n",
        "$\\vec{w}, b$ as\n",
        "$\\begin{bmatrix}\n",
        "\\vec{w}\\\\b\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "\\vec{w}\\\\b\n",
        "\\end{bmatrix} - \\mu \\nabla_{\\vec{w}, b}L\\left(\\vec{w}, b\\right)$.\n",
        "Each adjustment reduces the error. Starting from a random set of parameter values\n",
        "doing this \"sufficiently\" large number of times yields the desired model.\n",
        "\n",
        "A simple and popular model $\\phi$ is the linear function (predicted value is\n",
        "dot product between input and parameters plus bias):\n",
        "$\\tilde{y}_{i} = \\phi\\left(\\vec{x}_{i}, \\vec{w}, b\\right) = \\vec{w}^{T}\\vec{x} + b\n",
        "= \\sum_{j}w_{j}x_{j} + b$.\n",
        "In the example below, this is the model architecture used.\n",
        "\n",
        "Thus \n",
        "\\begin{align*}\n",
        "L &= \\sum_{i=0}^{i=N}\\left(e^{\\left(i\\right)}\\right)^{2}\\\\\n",
        "  &= \\sum_{i=0}^{i=N}\\left(\\vec{w}^{T}\\vec{x} + b - y_{i}\\right)^{2}\\\\\n",
        "\\nabla_{\\vec{w}, b}L &\\propto \\sum_{i=0}^{i=N}\\left(\\vec{w}^{T}\\vec{x}_{i} + b - y_{i}\\right)\\vec{x}_{i} \\\\\n",
        "                     &\\propto \\sum_{i=0}^{i=N}\\left(\\tilde{y}_{i} - y_{i}\\right)\\vec{x}_{i}\n",
        "\\end{align*}\n",
        "Our initial implementation will simply mimic this formula.\n",
        "For more complicated models $\\phi$ (with millions of parameters and non-linearities)\n",
        "we cannot obtain closed form gradients like this.\n",
        "\n",
        "The next example, based on NumPy and PyTorch, relies on PyTorch's\n",
        "autograd (automatic gradient computation) which does not have this limitation."
      ],
      "metadata": {
        "id": "e3XIplkKMLBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "eZPtNLyZM0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pCNUjg1tM2Gy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Model"
      ],
      "metadata": {
        "id": "sW3vJJzYdykM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's solve the cat-brain problem directly via pseudo-inverse.\n",
        "\n",
        "As expected, the model parameters\n",
        "will be converge to a solution close to that obtained by the pseudo-inverse technique."
      ],
      "metadata": {
        "id": "YuY5Vvwkdyt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "X = torch.tensor([\n",
        "  [0.11, 0.09], [0.01, 0.02], [0.98, 0.91],\n",
        "  [0.12, 0.21], [0.98, 0.99], [0.85, 0.87],\n",
        "  [0.03, 0.14], [0.55, 0.45], [0.49, 0.51], \n",
        "  [0.99, 0.01], [0.02, 0.89], [0.31, 0.47],\n",
        "  [0.55, 0.29], [0.87, 0.76], [0.63, 0.24]\n",
        "], dtype=torch.float)\n",
        "\n",
        "# add bias column\n",
        "X = torch.column_stack((X, torch.ones(15)))\n",
        "\n",
        "y = torch.tensor([\n",
        "  -0.8, -0.97, 0.89, -0.67, 0.97, 0.72,\n",
        "  -0.83, 0.00, 0.00, 0.00, -0.09, -0.22, \n",
        "  -0.16, 0.63, 0.37\n",
        "], dtype=torch.float)\n",
        "\n",
        "# Let us compute solution using pseudo inverse\n",
        "solution_pseudo = torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(X.T, X)), X.T), y)\n",
        "print(f\"Solution via pseudo inverse: {solution_pseudo}\")\n",
        "\n",
        "y = y.reshape((-1, 1))"
      ],
      "metadata": {
        "id": "taWoHRJIe3h8",
        "outputId": "2fd9d5a6-a88e-4029-d621-c61cdad65fd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution via pseudo inverse: tensor([ 1.0766,  0.8976, -0.9582])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_features):\n",
        "    super(LinearModel, self).__init__()\n",
        "    self.w = torch.nn.Parameter(torch.randn(num_features, 1))\n",
        "\n",
        "  def forward(self, X):\n",
        "    y_pred = torch.mm(X, self.w)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "FvhHCj8YGWaP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_unknowns = 3\n",
        "model = LinearModel(num_features=num_unknowns)\n",
        "\n",
        "# Let us use  Pytorch MSE loss function\n",
        "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "# Train model iteratively\n",
        "num_steps = 1000\n",
        "\n",
        "for step in range(num_steps):\n",
        "  # linear model\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # calculate loss\n",
        "  loss = loss_fn(y_pred, y)\n",
        "\n",
        "  # Periodically plot the true function and current approximation to check how we are doing\n",
        "  if step % 100 == 0:\n",
        "    print(f\"Loss at step {step, loss}\")\n",
        "\n",
        "  # zero out all partial derivatives\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Compute partial derivatives via AutoGrad\n",
        "  loss.backward()\n",
        "\n",
        "  # Update parameters from gradient computed in the backward() step\n",
        "  optimizer.step()\n",
        "\n",
        "solution_gd = torch.squeeze(model.w.data)\n",
        "print(f\"\\n\\nThe solution via gradient descent is {solution_gd}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG_PQTgvGW7k",
        "outputId": "dad3f75c-81c6-4971-a7ee-4305be3952ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step (0, tensor(6.6479, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (100, tensor(0.2207, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (200, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (300, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (400, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (500, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (600, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (700, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (800, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "Loss at step (900, tensor(0.2172, grad_fn=<MseLossBackward0>))\n",
            "\n",
            "\n",
            "The solution via gradient descent is tensor([ 1.0766,  0.8976, -0.9582])\n"
          ]
        }
      ]
    }
  ]
}